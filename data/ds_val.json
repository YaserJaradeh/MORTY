{"summary": "has research problem: Classification of Production Technology for Books using Convolutional Neural Networks/Classification of Images based on the cropped images extracted from the main images", "text": "#Properties\nhas research problem\n#Text\nCultural research is dedicated to understanding the processes of knowledge dissemination and the social and technological practices in the book industry. Research on children books in the 19 th century can be supported by computer systems. Specifically, the advances in digital image processing seem to offer great opportunities for analyzing and quantifying the visual components in the books. The production technology for illustrations in books in the 19 th century was characterized by a shift from wood or copper engraving to lithography. We report classification experiments which intend to classify images based on the production technology. For a classification task that is also difficult for humans, the classification quality reaches only around 70%.. We analyze some further error sources and identify reasons for the low performance. Fig. 1: Example of an illustration from the collectionFor humans, the identification of the printing technology used for historic books is also not a trivial task. Only experts in the area are able to classify images for this goal. They sometimes need magnifying glasses and the task is often easier when the original paper version is available. For the digital version, this classification is typically even more difficult.There are also many different forms of lithography (e.g. chromolithography). However, for the purposes of our study the identification of lithography in general is sufficient. State of the ArtTo the best of the authors' knowledge, this is the first attempt to classify the printing technology using images from books. Other previous work is focused on identifying the positions of text and image blocks within a page. The HBA data challenge for old books intends to improve algorithms for this task . These books are much older than the Hobrecker collection and contain also manuscripts. Most of the research in image processing is currently being carried out for photographs. These collections vary greatly from the non-realistic drawings and illustrations which can be found in children books. It is unclear how CNN and other models optimized for photographs perform for these images.Also in the analysis of art, there is little work on mining approaches. Few researchers have processed large amounts of images in this field. One experiment by Salah & Elgammel is dedicated to classify the painter of artistic work. Such work is highly dependent on the type of paintings in the collection .A recent project is focusing on graphic novels. Current state of the art CNNs are applied to tasks like author identification with very good success . In addition, the processing is aimed at measuring the drawing style of a graphic novel in order to find similar books. However, the production technology is not relevant for these modern books.The work that seems to be most similar to the classification of historical printing technologies is research in the area of texture recognition. Some authors manage to classify material from visual data. The materials are different types of plastic. The algorithm learns the typical texture structure of each material based on magnified data. For a material classification task, narrow structures for a CNN were used . There has also been work towards applying deep networks and transfer learning for material classification. Cimpoi et al. conducted material classification with deeper structure and transfer learning . Some authors achieved very successful results showing an accuracy of 99 to 99.9 percent with specially constructed dataset i.e. CU-ReT . Data Collection and ClassificationOne of the first goals for the research of images in historical children books lies within the production technologies. As a classification problem with few classes, it seems like a challenge which could be solved with current technology. Our primary assumption is, that differences in printing technology can be observed only at the detail level. Therefore, resizing of the images is being avoided. Much rather, the decision was made to extract small parts of the images in full resolution in order to retain the minute details of the printing. It is much less relevant to size images and make sure all objects are fully contained in an image because the problem seems not related to object identification.Consequently, the few available training images were cropped into small parts without re-sizing them which also leads to more training examples. Data CollectionThe data collection is based on the Hobrecker collection. Karl Hobrecker was a collector of children books. His books are now archived in the library of the Technical University of Braunschweig. A subset has been digitized and is available online .The collection is of great interest for cultural research. It contains different types of children books mainly from the 19 th century: e.g. alphabetization books, biographies, natural history descriptions as well as adventure and travel stories. The scanned Hobrecker collection of children's books available for digital analysis contains around 350 books. There is no knowledge about the distribution of printing technologies within the entire set. It would require too much human effort to manually classify all books.Out of these, 32 books were carefully labeled by experts and are used for the classification tasks. The experts classified the books into either wood engraving or lithography. Fig. 2: Examples cropping using slicing and a contour methodThe images in the scanned books were separated from the text by using image preprocessing techniques as described by Ban . To minimize the pixel information loss on images, all the images are cropped into the size of 128 * 128 pixels. In order to avoid an imbalance problem for classification, the number of crops are set equally by uniformly sampling from each book. An example for a crop is shown in figure 2.  shows some general statistics of the 32 books sorted by the two printing technologies.  .The major difference between the dataset used in material classification and the Hobrecker dataset is that the Hobrecker dataset contains a lot more noisy features due to the nature of scanned images from old books which are often compromised by age. The dataset used for material classification is created for the main purpose of material analysis, whereas the material for the books is always paper. However the quality and type differ greatly.Two methods are used for the task. One is using the pre-trained model. We applied the Inception Network as suggested by Szegedy , for feature extraction and use these features to feed fully connected neural networks (FCN) as well as Support Vector Machine (SVM) for classification. The second method uses a slim CNN architecture, similar to the one used by Ban , and trains the model with a randomized weight initialization.. Several modifications are made on top of this model. Firstly, the input size is increased from 64 *64 to 128 * 128. Secondly, two kinds of deeper networks with more convolution layers and pooling layers are tested. The architectures are shown in .These architectures were used for the following motivation. The size the of the features space to be learned is unclear. So we initially selected two different filter sizes to account for that.For the smaller filter size, two different layer structures were applied. The first one uses one pooling layer for each convolution layer. The second one uses fewer pooling layer. 4Results and analysis ResultsUnlike the results of CUReT , transfer learning results turned out to be poor of maximum of 58 % of accuracy. This accuracy is achieved when using FCN of two hidden layers with 512 nodes respectively using the features passed through by the pre-trained Inception model. Replacing FCN with SVM decreased the performance leading to 47% for linear kernel and 52% for non-linear kernel.The results using the model from Ban  showed 47% of accuracy and a similar model adopted from Mehri et al.  showed 51% of accuracy at the most. The results of the small-filters less-pooling architecture achieved 61% accuracy and Big-Filters showed the best performance of 63% classification accuracy. Small-Filters-Balanced-Pooling showed the lowest performance of 48% among the architectures which were tested. An overview of the results can be seen in . Some examples for misclassified iages are hown in . OutlookFor optimizing the identification of the printing type, more data is obtained. Labelled images from the Pictura Paedagogica Online (PPO)  are obtained and will be processed.Currently, experiments with object recognition on the collection are being carried out with Yolo  which is a pre-trained model based on photographs. It is interesting to check whether it also works for non-realistic images as we find them in historic books. First results indicate that the performance is very different for individual classes. The object recognition with a satisfying quality would allow the humanities scholars to trace the frequency of objects and the introduction of new knowledge to children's books.\n###\n"}
{"text": "#Properties\nHas value, Global Mean Sea level Rise Projection, has lower limit for likely range, has upper limit for likely range, has  start of period, has end of period, climate scenario, has research problem, has unit\n#Text\nSea level rise poses a significant threat to coastal communities, infrastructure, and ecosystems. Sea level rise is not uniform globally but is affected by a range of regional factors. In this study, we calculate regional projections of 21st century sea level rise in northern Europe, focusing on the British Isles, the Baltic Sea, and the North Sea. The input to the regional sea level projection is a probabilistic projection of the major components of the global sea level budget. Local sea level rise is partly compensated by vertical land movement from glacial isostatic adjustment. We explore the uncertainties beyond the likely range provided by the IPCC, including the risk and potential rate of marine ice sheet collapse. Our median 21st century relative sea level rise projection is 0.8 m near London and Hamburg, with a relative sea level drop of 0.1 m in the Bay of Bothnia (near Oulu, Finland). Considerable uncertainties remain in both the sea level budget and in the regional expression of sea level rise. The greatest uncertainties are associated with Antarctic ice loss, and uncertainties are skewed towards higher values, with the 95th percentile being characterized by an additional 0.9 m sea level rise above median projections. MATERIALS AND METHODSThe conventional approach to project sea level rise is to simulate the individual major sea level components of the global sea level budget: ocean steric expansion (T ), melting/dynamics of glaciers (GIC), ice loss from the Greenland and Antarctic ice sheets (GrIS and AIS), and changes in land water storage (LW) and then sum them up . We write this as:(1) For regional sea level, each contributor to GMSL rise will have a distinct spatial 'fingerprint' (i.e. characteristic pattern). It is therefore necessary to estimate how the sea level budget evolves into the future when we aim to project regional sea level rise.All masses, such as the ice contained in the Greenland ice sheet, gravitationally attract the oceans around them. Projected mass loss of the glaciers and ice sheets will reduce this gravitational pull, as well as cause an immediate elastic rebound of the solid Earth . Additionally, the mass redistribution will perturb the Earth's rotation ). These effects combine and produce a new so-called static equilibrium (SE) in the sea level configuration. This equilibrium is only quasistatic as it will vary over time as mass is added or removed from the ocean. The net SE response from melting ice is that the sea level contribution will not be distributed evenly on Earth but will rather be characterised by a specific spatial fingerprint. E.g. ice loss from Greenland may result in a lowering of sea level until Ireland and Norway, primarily due to the reduced gravitational pull in the Supplement at www.int-res.com/articles/ suppl/c064p015_supp.pdf). The sea level fingerprints of mass redistribution applied in the present study have all been calculated by solving the sea-level equation , following a pseudospectral approach , including the effects of migrating coastlines and of changes in the Earth's rotation . The elastic response of the solid Earth has been computed for a radially stratified and compressible Earth based on the Preliminary Reference Earth Model (PREM) .Steric expansion of the oceans will be largest in the open ocean where the water column is deepest . This non-uniform expansion leads to a differential increase in the steric sea surface heights (SSH), which will drive a redistribution of ocean mass GMSL GIC GrIS AIS LW T = + + + + from the deep ocean interior to shallower regions . Changes in ocean circulation and in the hydrological cycle induce additional changes. We combine the global average steric response (T ) with the dynamic sea level (DSL) response . The mass redistribution towards shelf areas induce increased gravitational attraction as well as increased loading on the sea floor, and is thus associated with a shelf mass loading (SML) sea level fingerprint . We express the relative sea level (RSL) at every location (x) from the 5 major contributions to GMSL (T, GIC, AIS, GrIS, and LW), the dynamic ocean response (DSL), their corresponding spatial fingerprints (F SML , F GIC , F LW , F GrIS , and F AIS ), and the glacial isostatic adjustment (GIA) as follows:( We express the fingerprints from terrestrial ice melt and land water as spatial multipliers to their global average contribution to sea level rise. The SML fingerprint was calculated from projected ocean bottom pressure change from NorESM1-M, but expressed as a multiplier to the local change in the ocean response following . This is an approximation that only holds if the mass redistribution used to calculate F SML is consistent with T + DSL.We use a Monte Carlo procedure to draw samples from the uncertainty distributions of all terms in Eq. (2) simultaneously to obtain an uncertainty distribution for RSL. We emulate the AR5 projection uncertainties using a multivariate normal distri bution with a covariance structure designed to be consistent with the AR5 likely range, combined with independent uniform distributions for LW and rapid ice sheet dynamics (see the Supplement and a). DataAs input to our sea level projection, we primarily rely on the IPCC AR5 process-based projections of the sea level budget ). The AR5 did not provide estimates beyond the likely range (i.e. the 66% uncertainty interval), primarily because of limitations in ice sheet modelling . It was concluded that an instability of marine-based sectors of the ice sheets could lead to sea level rise beyond the projected likely range, but that this scenario was unlikely . To capture the uncertainty distribution beyond the likely range, we use the expert elicitation for the ice sheet contribution (see the . This elicitation captures the current degree of uncertainty within the community of ice-sheet experts and includes the perceived risk and potential rate from marine ice sheet instabilities. Post-AR5 studies indicate that Pine Island Glacier in Antarctica is already engaged in an unstable retreat , a situation that is projected to extend to the neighbouring Thwaites glacier ,\n###\n", "summary": " Has value: 0.8\n, Global Mean Sea level Rise Projection: Global Mean Sea Level Rise Projections\n, has lower limit for likely range: 0.58\n, has upper limit for likely range: 1.20\n, has  start of period: 2000\n, has end of period: 2100\n, climate scenario: RCP8.5\n, has research problem: Global Mean Sea Level Rise Projections\n, has unit: m\n###"}
{"text": "#Properties\nHas value, Method, Time period, has beginning, has end, Lower confidence limit, Upper confidence limit, Located in, Basic reproduction number, Confidence interval (95%), has research problem, location, Approaches, same as, description\n#Text\nOn December 31, 2019, the World Health Organization was notified about a cluster of pneumonia of unknown aetiology in the city of Wuhan, China. Chinese authorities later identified a new coronavirus (2019-nCoV) as the causative agent of the outbreak. As of January 23, 2020, 655 cases have been confirmed in China and several other countries. Understanding the transmission characteristics and the potential for sustained human-to-human transmission of 2019-nCoV is critically important for coordinating current screening and containment strategies, and determining whether the outbreak constitutes a public health emergency of international concern (PHEIC). We performed stochastic simulations of early outbreak trajectories that are consistent with the epidemiological findings to date. We found the basic reproduction number, R 0 , to be around 2.2 (90% high density interval 1.4-3.8), indicating the potential for sustained human-to-human transmission. Transmission characteristics appear to be of a similar magnitude to severe acute respiratory syndrome-related coronavirus (SARS-CoV) and the 1918 pandemic influenza. These findings underline the importance of heightened screening, surveillance and control efforts, particularly at airports and other travel hubs, in order to prevent further international spread of 2019-nCoV. MethodsWe performed stochastic simulations of the first few generations of human-to-human transmission of 2019-nCoV.Simulations were initialized with one index case. For each primary case, we generated secondary cases according to a negative-binomial offspring distribution with mean R 0 and dispersion k . The dispersion parameter k can be interpreted as a measure of the probability of superspreading events (the lower the value of k, the higher the probability of superspreading). The generation time interval D was assumed to be gamma-distributed with a shape parameter of 2, and a mean that varied between 7 and 14 days. We explored a wide range of parameter combinations ) and ran 1,000 stochastic simulations for each individual combination. This corresponds to a total of 3.52 million one-index-case simulations that were run on UBELIX ( the high performance computing cluster at the University of Bern.In a second step, we accounted for the uncertainty regarding the number of index cases n and the date T of the initial zoonotic animal-to-human transmissions at the wet market in Wuhan. An epidemic with several index cases can be considered as the sum of several independent epidemics with one index case each. We sampled (with replacement) n of the one-index-case epidemics, sampled a date of onset for each index case, and summed the epidemic curves together. The sampling of the date of onset was done uniformly from a two-week interval around November 27, 2019, in coherence with early phylogenetic analyses of 11 2019-nCoV genomes . This step was repeated 4,800 times for each combination of R 0 (22 points) and k (20 points) for a total of 2,112,000 full epidemics simulated that included the uncertainty on D, n and T . Finally, we calculated the proportion of stochastic simulations that reached a total number of infected cases within the interval  by January 18, 2020, as estimated by Imai and colleagues . In a process related to Approximate Bayesian Computation (ABC), the parameter value combinations that led to simulations within that interval were treated as approximations to the posterior distributions of the parameters with uniform prior distributions. Model simulations and analyses were performed in the R software for statistical computing . Code files are available on ResultsIn order to reach between 1,000 and 9,700 infected cases by January 18, 2020, the early human-to-human transmission of 2019-nCoV are characterized by values of R 0 around around 2.2 (90% high density interval 1.4-3.8) (figure 1). The observed data at this point is compatible with a large range of values for the dispersion parameter k (median 0.54, 90% high density interval 0.014-6.95). However, our simulations suggest that very low values of k, corresponding to a large probability of superspreading events, are less likely. These estimates incorporate the uncertainty on the current total epidemic size (as of January 23, 2020) and on the date and scale of the initial zoonotic event (figure 2).Comparison with other emerging viruses in the past allows to put in perspective the available information regarding the transmission patterns of 2019-nCoV. Our estimates of R 0 and k are more similar to previous estimates focusing on early human-to-human transmission of SARS-CoV in Beijing and Singapore  than of MERS-CoV  (figure 3). Our estimates for 2019-nCoV are also in line with those of 1918 Influenza .\n###\n", "summary": " Has value: 2.2\n, Method: Stochastic simulations of early outbreak trajectories\n, Time period: Time interval\n, has beginning: 2020-01-18\n, has end: 2020-01-08\n, Basic reproduction number: Basic reproduction number estimate value specification\n, has research problem: Determination of the COVID-19 basic reproduction number\n, location: China and overseas\n, Approaches: Stochastic simulations of early outbreak trajectories were performed that are consistent with the epidemiological findings to date\n, same as: https://en.wikipedia.org/wiki/COVID-19_pandemic\n, description: This research problem aims at determining the basic reproduction rate of COVID-19.\n###"}
{"summary": "Pages: 193--210\nCustom3: Computer Science - Cryptography and Security\nCustom1: Cloud servers offer data outsourcing facility to their clients. A client outsources her data without having any copy at her end. Therefore, she needs a guarantee that her data are not modified by the server which may be malicious. Data auditing is performed on the outsourced data to resolve this issue. Moreover, the client may want all her data to be stored untampered. In this chapter, we describe proofs of retrievability (POR) that convince the client about the integrity of all her data.\nAnnote: ZSCC: 0000000\nYear: 2015\nJournal: arXiv:1711.06039 [cs]\nNote: Comment: A version has been published as a book chapter in Guide to Security Assurance for Cloud Computing (Springer International Publishing Switzerland 2015)\nIdentifier: senguptaCloudDataAuditing2015\nBibliographyType: 7", "text": "#Properties\nPages, Custom3, Custom1, Annote, Year, Journal, Note, Identifier, BibliographyType\n#Text\nCloud servers offer data outsourcing facility to their clients. A client outsources her data without having any copy at her end. Therefore, she needs a guarantee that her data are not modified by the server which may be malicious. Data auditing is performed on the outsourced data to resolve this issue. Moreover, the client may want all her data to be stored untampered. In this chapter, we describe proofs of retrievability (POR) that convince the client about the integrity of all her data. PreliminariesIn this section, we briefly discuss about some backgrounds needed for understanding the following sections. The detailed discussions can be found in . NotationWe take \u03bb as the security parameter. An algorithm A(1 \u03bb ) is called a probabilistic polynomial-time algorithm when its running time is polynomial in \u03bb and its output y is a random variable which depends on the internal coin tosses of A. We write y \u2190 A(\u2022) or y \u2190 A(\u2022, . . . , \u2022) depending upon whether A takes one input or more inputs, respectively. Moreover, if A is given access to an oracle O, we write y \u2190 A O (\u2022, . . . , \u2022). In this case, the Turing machine A has an additional query tape where A places its query x and calls another Turing machine O. Then, O is invoked with the input x, and the output is written on the same query tape . An element a chosen uniformly at random from a set S is denoted as a R \u2190 \u2212 S. A function f : N \u2192 R is called negligible in \u03bb if for all positive integers c and for all sufficiently large \u03bb, we have f (\u03bb) < 1 \u03bb c . We call a problem \"hard\" to denote that no polynomial-time algorithms exist for solving the problem. Message authentication codes are used as a digest to authenticate the message. MACs are defined in symmetric setting, that is, the sender and the receiver need to share a secret key. In the generation phase, the sender calculates the MAC for the message using the secret key and sends the message along with the MAC. In the verification phase the receiver verifies, using the same key, whether the MAC is computed on the given message using the same secret key. Due to the first property mentioned above, it is hard to modify a message m keeping the value of f k (m) unchanged.Message authentication codes are used hugely for authentication purposes. There are several constructions for MACs. Some of the constructions are based on pseudorandom functions  (e.g., XOR MAC , CMAC [48]), and some of them are based on cryptographic hash functions (e.g., HMAC [49]). Bilinear MapsLet G 1 , G 2 and G T be multiplicative cyclic groups of prime order p. Let g 1 and g 2 be generators of the groups G 1 and G 2 , respectively. A bilinear map ] is a function e :2. e is non-degenerate, that is, e(g 1 , g 2 ) = 1. Furthermore, properties 1 and 2 imply that 3. for all). If G 1 = G 2 = G, the bilinear map is symmetric; otherwise, asymmetric. Unless otherwise mentioned, we consider bilinear maps which are symmetric and efficiently computable. Let BLSetup(1 \u03bb ) be an algorithm which outputs (p, g, G, G T , e), the parameters of a bilinear map, where g is a generator of G. Digital SignatureDiffie and Hellman introduce the public-key cryptography and the notion of digital signatures in their seminal paper \"New Directions in Cryptography\" . Rivest, Shamir and Adleman  propose the first digital signature scheme based on the RSA assumption. Several signature schemes are available in the literature. Several signature schemes are found in the literature .We define a digital signature scheme as proposed by Goldwasser et al. . A digital signature scheme consists of the following polynomial-time algorithms: a key generation algorithm KeyGen, a signing algorithm Sign and a verification algorithm Verify. KeyGen takes as input the security parameter \u03bb and outputs a pair of keys (pk, sk), where sk is the secret key and pk is the corresponding public key. Algorithm Sign takes a message m from the message space M and the secret key sk as input and outputs a signature \u03c3. Algorithm Verify takes as input the public key pk, a message m and a signature \u03c3, and outputs accept or reject depending upon whether the signature is valid or not. Any of these algorithms can be probabilistic in nature. A digital signature scheme has the following properties.1. Correctness: Algorithm Verify always accepts a signature generated by an honest signer, that is,Pr[Verify(pk, m, Sign(sk, m)) = accept] = 1.2. Security: Let Sign sk (\u2022) be the signing oracle and A be any probabilistic polynomialtime adversary with an oracle access to Sign sk (\u2022). The adversary A makes polynomial number of sign queries to Sign sk (\u2022) for different messages and gets back the signatures on those messages. The signature scheme is secure if A cannot produce, except with some probability negligible in \u03bb, a valid signature on a message not queried previously, that is, for any probabilistic polynomial-time adversary A Sign sk (\u2022) , the following probabilityis negligible in \u03bb, where Q is the set of sign queries made by A to Sign sk (\u2022).As a concrete example, we mention the algorithms of the BLS signature proposed by Boneh, Lynn and Shacham . Let the algorithm BLSetup(1 \u03bb ) output (p, g, G, G T , e) as the parameters of a bilinear map, where G and G T are multiplicative cyclic groups of prime order p, g is a generator of G and e : G \u00d7 G \u2192 G T (see Section 2.3). KeyGen chooses sk R \u2190 \u2212 Z p as the secret key, and the public key is set to be pk = g sk . The algorithm Sign uses a full-domain hash H : {0, 1} * \u2192 G, and it generates a signature \u03c3 = H(m) sk on a message m \u2208 {0, 1} * . Given a message-signature pair (m, \u03c3), the algorithm Verify checks e(\u03c3, g) ? = e(H(m), pk). Verify outputs accept if and only if the equality holds. Erasure CodeAn (n, f, d) \u03a3 erasure code is a forward error-correcting code  that consists of an encoding algorithm Enc: \u03a3 f \u2192 \u03a3 n (encodes a message consisting of f symbols into a longer codeword consisting of n symbols) and a decoding algorithm Dec: \u03a3 n \u2192 \u03a3 f (decodes a codeword to a message), where \u03a3 is a finite alphabet and d is the minimum distance (Hamming distance between any two codewords is at least d) of the code. The quantity f n is called the rate of the code. An (n, f, d) \u03a3 erasure code can tolerate up to d \u2212 1 erasures. If d = n \u2212 f + 1, we call the code a maximum distance separable (MDS) code. For an MDS code, the original message can be reconstructed from any f out of n symbols of the codeword . Reed-Solomon codes  and their extensions are examples of non-trivial MDS codes. We give a simple example, from , of a Reed-Solomon code below.Let us consider the finite field F This code can correct a single erasure (d \u2212 1 = 1). For example, 1 * \u03b3 (' * ' denotes the erasure) can be decoded uniquely to 10\u03b3. In other words, a partially erased codeword can be reconstructed from the other two symbols available. Oblivious RAMGoldreich and Ostrovsky introduce the notion of oblivious RAM (ORAM) . In a RAM (Random Access Memory) model, there is a CPU and a memory module. Anyone can intercept the communications between the CPU and the memory module, and observe the memory-access patterns. Oblivious RAM (ORAM) is a probabilistic RAM where the access-pattern is independent of the address input to the memory module. ORAM involves a hierarchical data structure which allows hiding memory-access patterns. This data structure consists of hash tables of different lengths at different levels. The number of levels is O(log n). An element of a hash table contains an (address, value) pair. When an address is searched for a read operation, the address is first hashed and the hash value is matched with the hash table at the top level. If a match is not found, the address is hashed again and matched with the hash table in the next level, and so on. If a match is found, random locations are searched in the hash tables in the subsequent levels. This is continued until the last level. If an address is found more than once, ORAM returns the most updated value residing at the topmost level. For a write operation, the new value is inserted into the hash table of the top level. As each addresssearch is associated with hash tables in every level of the hierarchical data structure, an adversary cannot gain any knowledge about the pattern of the search. For the same reason, ORAM takes time polynomial in log n for each read or write operation as all the hash tables need to be consulted to hide the actual access-pattern. There is a \"rebuild\" phase which is executed periodically to rebuild the levels (due to too many insertions). Recent works on ORAM include . Proofs of RetrievabilityA client uploads a file to the cloud server. However, the client needs a guarantee that all her data are stored in the server untampered. Proofs-of-retrievability (POR) schemes make the client be assured that her data are stored intact in the server. Juels and Kaliski introduce proofs of retrievability for static data . Static data mostly include archival data which the client does not modify after she uploads the file to the server. However, some of the POR schemes deal with dynamic data where the client modifies her data. We provide a brief idea about the building blocks of POR schemes. We discuss them in detail in Section 3 and Section 4.In the setup phase, the client preprocesses her file F 0 . The preprocessing step involves encoding the file F 0 with an erasure code to form another file F . Then, an authenticator is attached to each of the blocks of F (for checking the integrity of the blocks later). Finally, the client uploads F along with the authenticators to the server. We consider the file F as a collection of n blocks or segments where each block is an element of Z p . The client can read data from the file she has outsourced. She performs audits to check the integrity of her data. An audit comprises of two algorithms for proof-generation and proof-verification. During an audit, the client generates a random challenge and sends it to the server which acts as a prover. Upon receiving the challenge, the server responds to the client with a proof. The client then verifies the integrity of the data by checking the validity of the proof. If the proof is valid, the verification algorithm outputs 1; otherwise, it outputs 0. For dynamic POR schemes, the client can issue write operations along with read operations. The basic operations are illustrated in .POR schemes satisfy two properties: correctness and soundness. The correctness property demands that the proof generated by an honest server always makes the verification algorithm output 1. The soundness property of POR schemes is formalized by the existence of an extractor algorithm that extracts F after interacting with a malicious server which passes an audit (that is, the verification algorithm outputs 1) with any probability non-negligible in the security parameter \u03bb.There are two types of POR schemes: privately verifiable and publicly verifiable schemes. In private-verification schemes, only the client can perform audits as the ver- ification of a proof requires some secret information. On the other hand, in publicly verifiable schemes, anyone can verify the proof supplied by the server. In privacy preserving auditing, the verifier (any verifier other than the client) cannot gain any knowledge about the data outsourced to the server . Proofs of Retrievability for Static DataIn the static setting, the client does not modify her data once they are outsourced to the cloud server. We discuss two POR schemes for static data below. However, there are other POR schemes related to static data. We mention some of them in Section 5. POR Scheme by Juels and KaliskiJuels and Kaliski  propose the first POR scheme for static data. A similar scheme for online memory checking is given by Naor and Rothblum . Though the basic idea is the same for both of the schemes, the first one uses a sentinels (random strings that are independent of the file's content) and the latter scheme uses MACs for authentication. Here, we describe the MAC-based solution and make a brief note about the sentinelbased solution.The client selects k R \u2190 \u2212 K as her secret key, where K is the key space for a MAC. Let the client have a file F 0 with f blocks or segments which she wants to upload to the cloud server. The client encodes F 0 with an erasure code to form a file F with n segments. Let each segment of the file F be an element of Z p , that is, F [i] \u2208 Z p for all 1 i n. The client computes \u03c3 i = MAC k (i||F [i]) for all 1 i n and uploads the file F along with the tags {\u03c3 i } 1 i n to the server.During an audit, the client generates a random challenge Q = {i} and sends it to the server which acts as a prover. Upon receiving Q, the prover responds to the client with {(F [i], \u03c3 i )} i\u2208Q . The verification algorithm, for each i \u2208 Q, checks ifand outputs 1 if the equality holds for each i \u2208 Q; it outputs 0, otherwise. If the MAC scheme is secure, then the server cannot produce a valid MAC on a message of its choice without knowing the secret key k. Now, the client can get a fraction (say, \u03c1) of the data blocks of F by interacting with a server which passes an audit with some probability, non-negligible in the security parameter \u03bb. Since the initial file F 0 has been encoded to form F , all the blocks of F 0 can be retrieved from \u03c1-fraction of blocks of F .The scheme mentioned above is privately verifiable as only the client (having the knowledge of the secret key k) can verify the integrity of her data. However, this scheme can be turned into a publicly verifiable scheme if MACs are replaced by digital signatures.In the original scheme proposed by Juels and Kaliski , the blocks of the encoded file F are encrypted and a large number of random elements (sentinels) are inserted in random locations of F . The server cannot distinguish between the encrypted blocks of F and the sentinels. During an audit, the verifier (only the client can be the verifier) checks the authenticity of several sentinels at different positions. If the server modifies a considerable fraction of the blocks, a similar fraction of sentinels are modified as well (as the sentinels are inserted in random locations of F ). The server cannot selectively delete non-sentinel blocks as it cannot distinguish them from sentinels. Thus, with high probability, the server cannot pass the audit. On the other hand, once the client challenges for some sentinel-locations, they are revealed to the server. Therefore, the future challenges must not include these locations. This makes the number of audits that can be performed in this scheme bounded. POR Schemes by Shacham and WatersShacham and Waters propose two short and efficient homomorphic authenticators in their POR schemes for static data . The first one, based on pseudorandom functions, provides a POR scheme which is privately verifiable (that is, only the client can verify a proof) and secure in the standard model 1 ; the second one, based on BLS signatures (see Section 2.4), gives a POR scheme which is publicly verifiable (that is, anyone can verify a proof) and secure in the random oracle model 2 .As mentioned by Shacham and Waters, Reed-Solomon codes are necessary against adversarial erasures where the server can delete blocks selectively. One drawback of these codes is the complexity of encoding and decoding is O(n 2 ), where n is the number of blocks of the file uploaded to the server. We can employ codes with linear decoding time instead of Reed-Solomon codes. However, these codes are secure against random erasures only. Shacham and Waters discuss a solution to this problem strictly for the privately verifiable scheme. We briefly describe the schemes below.  Standard model is a model of computation where the security of a cryptographic scheme is derived from some complexity assumptions (for example, hardness of factoring large integers [70], or hardness of finding discrete logarithm of an element of a finite group .)  Random oracle model is a model of computation where the security of a cryptographic scheme is proven assuming a cryptographic hash function used in the scheme as a truly random function.POR Scheme with Private Verification The client chooses (\u03b1, k) as her secret key,where \u03b1 R \u2190 \u2212 Z p and k R \u2190 \u2212 K (K is the key space for a pseudorandom function). Let h : K \u00d7 {0, 1} * \u2192 Z p be a pseudorandom function . Let the client have a file F 0 with f blocks or segments which she wants to upload to the cloud server. The client encodes F 0 with an erasure code to form a file F with n segments. Let each segment of the file F be an element of Z p , that is, F [i] \u2208 Z p for all 1 i n. The client computes \u03c3 i = h k (i) + \u03b1F [i] (mod p) for all 1 i n and uploads the file F along with the tags {\u03c3 i } 1 i n to the server.During an audit, the client generates a random query Q = {(i, \u03bd i )} and sends it to the server which acts as a prover. Upon receiving Q, the prover computes \u03c3 =The prover responds to the client with (\u03c3, \u00b5). Then the client verifies the integrity of her data by checking the verification equationand outputs 1 or 0 depending on whether the equality holds or not. As discussed in Section 2.7, a POR scheme is correct if the verifier always outputs 1 when the proof is supplied by an honest server. The correctness of the scheme can be proved as below.Correctness:In this privately verifiable scheme, only the client can perform the verification as the verification algorithm requires the knowledge of the secret key (\u03b1, k).POR Scheme with Public Verification Let there be an algorithm BLSetup(1 \u03bb ) that outputs (p, g, G, G T , e) as the parameters of a bilinear map, where G and G T are multiplicative cyclic groups of prime order p = \u0398(\u03bb), g is a generator of G and e : G \u00d7 G \u2192 G T (see Section 2.3). The client chooses x R \u2190 \u2212 Z p as her secret key.The public key of the client is v = g x . Let \u03b1 R \u2190 \u2212 G be another generator of G and H : {0, 1} * \u2192 G be the BLS hash (see Section 2.4). Let the client have a file F 0 with f blocks or segments which she wants to upload to the cloud server. The client encodes F 0 with an erasure code to form a file F with n segments. Let each segment of the file F be an element of Z p , that is, F [i] \u2208 Z p for all 1 i n. The client computesx for all 1 i n and uploads the file F along with the tags {\u03c3 i } 1 i n to the server.During an audit, the verifier generates a random query Q = {(i, \u03bd i )} and sends it to the server which acts as a prover. Upon receiving Q, the prover computes \u03c3 = and outputs 1 or 0 depending on whether the equality holds or not. The correctness of the scheme can be proved as below.Correctness:In this publicly verifiable scheme, the verifier does not need the secret key x to verify the response from the prover; knowledge of the public key pk would suffice for that purpose. Due to this reason, any third party auditor (TPA) can perform audits on behalf of the client (owner of the data). In privacy preserving auditing, there is an additional requirement that the TPA should not learn the data on which the audits are being performed. For example, Wang et al. use the publicly verifiable scheme of Shacham and Waters, and they achieve privacy preserving auditing using a technique called random masking . Proofs of Retrievability for Dynamic DataIn the previous section, we have described some POR schemes for static data which the clients do not modify once they are uploaded in the cloud server. A natural question comes if any POR schemes are available for dynamic data where the clients modify their outsourced data \"efficiently\". In this section, we discuss about the difficulties of modification of the uploaded data. Then, we will mention two POR schemes for dynamic data.To maintain the retrievability of the whole file, erasure coding has been employed on the file. The blocks of the file are encoded in such a way that the file can be retrieved from a fraction of blocks of the encoded file. The content of each block is now distributed in other O(n) blocks. Therefore, to actually delete a block the server has to delete all the related blocks. This restricts the server from deleting or modifying a block maliciously and still passing the verification with non-negligible probability in \u03bb. However, this advantage comes with some drawbacks. If the client wants to update a single block, she has to update all the related blocks as well. This makes the update process inefficient as n can be very large.Cash et al.  discuss about two failed attempts to provide a solution of the problem mentioned above. In the first case, a possible solution might be to encode the file locally. Now, each codeword consists of a small number of blocks. Therefore, an update of a single block requires an update of a few blocks within that particular codeword. However, a malicious server can gain the knowledge of this small set of blocks (within a codeword) whenever the client updates a single block. Thus, the server can delete this small set of blocks without being noticed during an audit. In the second attempt, after encoding the file locally, all of the n blocks are permuted in a pseudorandom fashion. Apparently, the server cannot get any information about the blocks in a codeword. However, during an update the server can identify the related blocks in a codeword. Therefore, the server can again delete these blocks and pass the verification during an audit.Due to the issues discussed above, only a few POR schemes for dynamic data are available in the literature. Now, we briefly mention two of these schemes below. The first scheme  exploits oblivious RAM for hiding data-access patterns. The second scheme  uses an incremental code to reduce the amortized cost for updates. POR Scheme by Cash, K\u00fcp\u00e7\u00fc and WichsCash et al.  propose a POR scheme for dynamic data using ORAM (see Section 2.6). They proceed as the first attempt mentioned in Section 4. That is, the data is divided into several chunks where each chunk contains a few blocks in it. Then, the blocks in each chunk are encoded \"locally\" using an erasure code to form a codeword. Thus, an update on a single block requires updating only related blocks of that particular codeword. This makes the update process much more efficient than that when all the blocks of the data are encoded to form a single large codeword. However, this solution comes with a drawback that the malicious server can now identify all the related blocks and delete these blocks selectively. As the number of blocks in a codeword is small, the server has a considerable chance to get through an audit.Cash et al. introduce ORAM as a solution for the problem mentioned above, still keeping the update-complexity low. Small chunks are encoded to form small codewords to make the updates efficient. However, the challenge is to hide the access-patterns from the server so that the server cannot identify the blocks in a codeword. ORAM lets the client read from the outsourced data in a pseudorandom fashion (using ORAM-Read protocol). It also provides a privacy-preserving way to write the blocks of a codeword (using ORAM-Write protocol). We give a high-level overview of the scheme as follows.In the setup phase, data blocks are divided into chunks and chunks are encoded to form codewords. In this phase, the ORAM protocol is initiated as well. For a read operation, the exact location of the block is found from the chunk-address (add ch) and the offset (add off ), and this address is fed into ORAM-Read. For a write operation, add ch and add off are calculated first. Then, the codeword corresponding to add ch is obtained (using ORAM-Read) and decoded. The exact block is located (using add off ) and modified accordingly. The new chunk is now encoded again and updated in the server using ORAM-Write. To run the audit protocol, a set of random locations are read using ORAM-Read and their authenticity is checked. The verifier outputs 1 if and only if the data-integrity is preserved. POR Scheme by Shi, Stefanov and PapamanthouThe privacy of the access-patterns is achieved by the scheme proposed by Cash et al. . However, Shi et al.  argue that a POR scheme need not hide the accesspatterns and it must satisfy only two properties: authenticated storage (the client needs an assurance about the authenticity of her data) and retrievability (the client can extract her data at any point of time). Shi et al. propose another dynamic POR scheme where the scheme satisfies these two properties, and it is more efficient (in terms of the computation-cost or communication bandwidth required for the basic operations) than the scheme by Cash et al. as the additional cost for hiding access-patterns is now eliminated. Here, we describe the basic construction of the scheme briefly.The main challenge is to reduce the write cost since an update in a single block is followed by updates on other O(n) blocks. In this scheme, the encoded copy is not updated for every write operation. Instead, it is updated (or rebuilt) only when sufficient updates are done on the data file. Thus, the amortized cost for writes is reduced dramatically. However, between two such rebuilds this encoded copy stores stale data. Therefore, they introduce a temporary hierarchical log structure which stores values for these intermediate writes. During an audit, O(\u03bb) random locations of the encoded data file as well as the hierarchical log structure are checked for authenticity. The scheme involves three data structures: an uncoded buffer U which is updated updated after every write and reads are performed on this buffer only, an encoded (using an erasure code) buffer C which is updated after every n writes, and an encoded log structure H which accommodates every update between two rebuilds of C.The buffer U contains an up-to-date copy of the data file. Reads and writes are performed directly on the required locations of U. Merkle hash tree [71,45] is used for U to check the authenticity of the read block. Reads and writes are never performed directly on the buffer C. After n write operations, the buffer U is encoded using an erasure code (see Section 2.5) to form a new copy of C, and the existing copy of C is replaced by this new one. The log structure H consists of log n + 1 levels and stores the intermediate updates temporarily. The l-th level consists of an encoded copy of 2 l blocks using a (2 l+1 , 2 l , 2 l )-erasure code for each 0 l log n + 1. When a block is updated it is written in the topmost level (l = 0). If the top l levels are already full, a rebuild is performed to accommodate all the blocks up to l-th level as well as the new block in the (l + 1)-th level and to make all the levels up to l-th level empty. Shi et al. employs a fast incrementally constructible code based on Fast Fourier Transform . Using this code, the rebuild cost of l-th level takes O(\u03b2 \u2022 2 l ) time, where \u03b2 is the block size. The l-th level is rebuild after 2 l writes. Therefore, the amortized cost for rebuilding is O(\u03b2 log n) per write operation. This improves the earlier scheme of Cash et al.  which requires O(\u03b2\u03bb(log n) 2 ). Each rebuild of C is followed by making H empty. To perform an audit, the verifier chooses O(\u03bb) random locations of the encoded buffer C and O(\u03bb) random locations of each full level of the hierarchical log structure H, and check for authenticity. The verification algorithm outputs 1 if all the blocks are authentic; it outputs 0, otherwise.Shi et al.  improve this basic construction by using homomorphic checksums. In this improved construction, the cost of communication bandwidth and the cost of client computation are further reduced to \u03b2 + O(\u03bb log n). However, the server computation remains the same, that is, O(\u03b2 log n). ConclusionIn this chapter we have given a brief overview of proofs-of-retrievability (POR), and we have discussed some POR schemes. There are several POR schemes in the literature which we have not covered in this chapter. Interested readers may take a look at these works .\n###\n"}
{"summary": "has research problem: data-driven predictive control using input-output data\nRobustness analysis: false\nHas example: yes - quadcoptor\nTheoretical guarantees: false\ndescription: The predictive model in the predictive control problem is now replaced by a set of input/output data matrices", "text": "#Properties\nhas research problem, Robustness analysis, Has example, Theoretical guarantees, description\n#Text\nWe consider the problem of optimal trajectory tracking for unknown systems. A novel data-enabled predictive control (DeePC) algorithm is presented that computes optimal and safe control policies using real-time feedback driving the unknown system along a desired trajectory while satisfying system constraints. Using a finite number of data samples from the unknown system, our proposed algorithm uses a behavioural systems theory approach to learn a non-parametric system model used to predict future trajectories. The DeePC algorithm is shown to be equivalent to the classical and widely adopted Model Predictive Control (MPC) algorithm in the case of deterministic linear time-invariant systems. In the case of nonlinear stochastic systems, we propose regularizations to the DeePC algorithm. Simulations are provided to illustrate performance and compare the algorithm with other methods. II. PROBLEM STATEMENTConsider the discrete-time system given bywhere A \u2208 R n\u00d7n , B \u2208 R n\u00d7m , C \u2208 R p\u00d7n , D \u2208 R p\u00d7m , andare respectively the state, control input, and output of the system at time t \u2208 Z \u22650 . Given a desired reference trajectory r = (r 0 , r 1 , . . . ) \u2208 (R p ) Z \u22650 , input constraint set U \u2286 R m , output constraint set Y \u2286 R p , we wish to apply control inputs such that the system output tracks the reference trajectory r while satisfying constraints and optimizing a cost function. Tracking of the trivial trajectory r = (0, 0, . . . ) is simply regulation.In the case when the model for the system is known, i.e., matrices A, B, C and D are known, the problem can be approached using MPC (see Section III). This paper focuses on the above trajectory tracking problem in the case when the model for system (1) is unknown, but input/output data samples are available. III. MPC: A BRIEF OVERVIEWWe outline the well-known receding-horizon model predictive control and estimation algorithm for trajectory tracking when the model of system (1) is known (see, e.g., ). Consider the following optimization problem:are the decision variables, and r t+k \u2208 R p is the desired reference at time t+k, where t \u2208 Z \u22650 is the time at which the optimization problem should be solved. The norm u k R denotes the quadratic form u T k Ru k (similarly for \u2022 Q ), where R \u2208 R m\u00d7m is the control cost matrix and Q \u2208 R p\u00d7p is the output cost matrix. The estimated state at time t is denoted byx(t) and the predicted state and output at time t + k are denoted by x k and y k , respectively. If the entire state is measured then x(t) = x(t). When the state measurement is not available, but system (1) is observable, an observer is typically used to estimate the state based on knowledge of the system (1) and the measured output y . One may also combine the control and estimation into a single min-max optimization problem .The classical MPC algorithm involves solving optimization problem (2) in a receding horizon manner. Algorithm 1 MPCInput: (A, B, C, D), reference trajectory r, past input/output data (u, y), constraint sets U and Y, and performance matrices Q and R 1) Generate state estimatex(t) using past input/output data. 2) Solve (2) for Note that choosing s > 0 reduces the number of computations, and in some cases may improve performance .Under standard assumptions, it can be shown that the MPC algorithm is recursively feasible and stabilizing .One crucial ingredient for MPC is an accurate model of the system; this is needed both to formulate problem  and, in cases where the state is not measured exactly, to generate the initial state estimatesx(t). The need for a model is traditionally addressed through system identification , where observations of the system are collected offline before the online control operation begins and are used to estimate a model of the form (1) that matches the observed data in an appropriate sense. For complex systems, this can be a cumbersome and expensive process - . For this reason, we propose a Data-enabled Predictive Control (DeePC) algorithm which learns the behaviour of the system and does not require an explicit model, system identification, or state estimation (see Algorithm 2). IV. PRELIMINARIES A. Non-parametric system representationBehavioural system theory is a natural way of viewing a dynamical system when one is not concerned with a particular system representation, but rather the subspace of the signal space in which trajectories of the system live. This is in contrast with classical system theory, where a particular parametric system representation (such as the state-space model (1)) is used to describe the input/output behaviour, and properties of the system are derived by studying the chosen system representation. Following , we define a dynamical system and its properties in terms of its behaviour.Definition 4.1:where Z \u22650 is the discrete-time axis, W is a signal space, and B \u2286 W Z \u22650 is the behaviour. Definition 4.2:is linear if W is a vector space and B is a linear subspace of Wis the forward time shift defined by (\u03c3w)(t) = w(t + 1) and \u03c3B = {\u03c3w | w \u2208 B}. (iii) (Z \u22650 , W, B) is complete if B is closed in the topology of pointwise convergence.Note that if a dynamical system satisfies (i)-(ii) then (iii) is equivalent to finite dimensionality of W (see .1]). We denote the class of systemsWith slight abuse of notation and terminology, we denote a dynamical system in L m+p only by its behaviour B. Next, we define the setto a window of length T . Without loss of generality, we assume that B can be written as the product space of two sub-behaviours B u and B y , where B u = (R m ) Z \u22650 and B y \u2286 (R p ) Z \u22650 are the spaces of input and output signals, respectively (see ), that is, any trajectory w \u2208 B can be written as w = col(u, y), where col(u, y) := (u T , y T ) T . We now present two concepts: controllability, and persistency of excitation. Definition 4.3: A system B \u2208 L m+p is controllable if for every T \u2208 Z >0 , w 1 \u2208 B T , w 2 \u2208 B there exists w \u2208 B and T \u2032 \u2208 Z >0 such that w t = w 1 t for 1 \u2264 t \u2264 T and w t = w 2 t\u2212T \u2212T \u2032 for t > T + T \u2032 . In other words, a behavioural system is controllable if any two trajectories can be patched together in finite time.Definition 4.4:is of full row rank.The term persistently exciting describes an input signal sufficiently rich and long as to excite the system yielding an output sequence that is representative for the system's behaviour. B. Parametric system representationThere are several equivalent ways of representing a behavioural system B \u2208 L m+p , including the classical input/output/state representation (1) denoted byThe input/output/state representation of smallest order (i.e., smallest state dimension) is called a minimal representation, and we denote its order by n(B). Another important property of a system B \u2208 L m+p is the lag defined by the smallest integer \u2113 \u2208 Z >0 such that the observability matrix O \u2113 (A, C) := col C, CA, . . . , CA \u2113\u22121 has rank n(B). We denote the lag by \u2113(B) (see  for equivalent input/output/state representation free definitions of lag). The lower triangular Toeplitz matrix consisting of A, B, C, D is denoted byWe can now present a uniqueness result. Lemma 4.1:In other words, given a sufficiently long window of initial system data col(u ini , y ini ), the state to which the system is driven by the sequence of inputs u ini is unique. Furthermore, if the matrices A, B, C, D are known, the state x ini can be computed. We now present a result known in behavioural systems theory as the Fundamental Lemma . Note that the number of data points T must be at least (m + 1)(t + n(B)) \u2212 1 in order to satisfy the persistency of excitation condition. Lemma 4.2 replaces the need for a model or system identification process and allows for any trajectory of a controllable LTI system to be constructed using a finite number of data samples generated by a sufficiently rich (in particular, persistently exciting) input signal. In a sense, the Hankel matrix is itself a nonparametric predictive model based on raw data. It allows one to implicitly estimate the state of an LTI system, predict its future behaviour, and design optimal feedforward control inputs . Lemma 4.1 and Lemma 4.2 can be used together in a predictive control algorithm similar to Algorithm 1 by replacing the state observer (respectively, the system model) with the data col(u ini , y ini ) (respectively, H t (w)). V. DEEPC: A DATA-ENABLED PREDICTIVE CONTROL ALGORITHM A. Data collectionWe begin by assuming that the data is generated by an unknown controllable LTI system B \u2208 L m+p with a minimal input/output/state representation Bbe a sequence of T inputs applied to B, andthe corresponding outputs. Furthermore, assume u d is persistently exciting of order T ini +N +n(B). The superscript d is used to indicate that these are sequences of data samples collected during an offline procedure from the unknown system. Note that the data col(u d , y d ) \u2208 B T can be equivalently thought of as coming from the minimimal input/output/state representation B(A, B, C, D). Next, we partition the input/output data into two parts which we call past data and future data. More formally, definewhere U p consists of the first T ini block rows of H Tini+N (u d ) and U f consists of the last N block rows of H Tini+N (u d ) (similarly for Y p and Y f ). In the sequel, past data denoted by the subscript p will be used to estimate the initial condition of the underlying state, whereas the future data denoted by the subscript f will be used to predict the future trajectories. B. State estimation and trajectory predictionBy Lemma 4.2, we can construct any T ini + N length trajectory of B Tini+N using the data collected. Indeed, a trajectory col(u ini , u, y ini , y) belongs to B Tini+N if and only if there exists g \u2208If T ini \u2265 \u2113(B), then Lemma 4.1 implies that there exists a unique x ini \u2208 R n(B) such that the output y is uniquely determined by . Intuitively, the trajectory col(u ini , y ini ) fixes the underlying initial state x ini from which the trajectory col(u, y) evolves. Note, however, that (5) does not require the input/output/state representation of the system to be known. The state x ini is only \"fixed\" implicitly by col(u ini , y ini ).As first shown in , this allows one to predict future trajectories based on a given initial trajectory col(u ini , y ini ) \u2208 B Tini , and the precollected data in U p , U f , Y p , and Y f . Indeed, given an initial trajectory col(u ini , y ini ) \u2208 B Tini of length T ini \u2265 \u2113(B) and a sequence of future inputs u \u2208 R N m , the first three block equations of (5) can be solved for g. The sequence of future outputs are then given by y = Y f g. Furthermore, by Lemma 4.1 the vector y computed contains the unique sequence of outputs corresponding to the inputs u.Vice versa, given a desired reference output y an associated feedforward control input can be calculated . C. DeePC algorithmGiven a time horizon N \u2208 Z >0 , a reference trajectory r = (r 0 , r 1 , . . . ) \u2208 (R p ) Z \u22650 , past input/output data col(u ini , y ini ) \u2208 B Tini , input constraint set U \u2286 R m , output constraint set Y \u2286 R p , output cost matrix Q \u2208 R p\u00d7p , and control cost matrix R \u2208 R m\u00d7m , we formulate the following optimization problem:Note here, that u and y are not independent decision variables of the optimization problem. Rather they are described completely by the fixed data matrices U f and Y f and the decision variable g. A comparison of the two optimal control problems (2) and (6) yields only a single (though key) difference; the model and the state estimate in (2) are replaced completely with input/output data samples in . We now present the DeePC algorithm. Algorithm 2 DeePCInput: col(u d , y d ) \u2208 B T , reference trajectory r \u2208 R N p , past input/output data col(u ini , y ini ) \u2208 B Tini , constraint sets U and Y, and performance matrices Q and R 1) Solve (6) for g \u22c6 .2) Compute the optimal input sequence u \u22c6 = U f g \u22c6 .3) Apply input (u(t), . . . , u(t + s)) = (u \u22c6 0 , . . . , u \u22c6 s ) for some s \u2264 N \u2212 1. 4) Set t to t + s and update u ini and y ini to the T ini most recent input/output measurements. 5) Return to 1. D. Equivalence of DeePC and MPCIt can be shown that Algorithm 1 and Algorithm 2 yield equivalent closed-loop trajectories under certain assumptions. We first show the equivalence of the feasible sets of (2) and .Theorem 5.1: (Feasible Set Equivalence): Consider a controllable LTI system B \u2208 L m+p with minimal input/ouput/state representation B  given as in . Consider the MPC and DeePC optimization problems  and . Let T ini \u2265 \u2113(B) and col(u ini , y ini ) \u2208 B Tini be the most recent input/output measurements from system . Assume that the data col(Then there exists a state estimatex(t) in  such that the feasible sets of (2) and (6) are equal.Proof: We first look at the feasible set of (6). Since u d is persistently exciting of order . Hence, the feasible set of (6) is equal to {(u, y) \u2208 U N \u00d7 Y N | col(u ini , u, y ini , y) \u2208 B Tini+N }, where U N is the cartesian product of U with itself N -times (similarly for Y N ). Since the system B yields an equivalent representation given by B , then by Lemma 4.1 the feasible set of (6) can be written as the set of pairswhere x ini is uniquely determined from col(u ini , y ini ). We now look at the feasible set of (2). By rewriting the constraints in (2) we obtainwherex(t) is the estimation of the state x(t) at time t. Setting the state estimatex(t) = x ini yields equal feasible sets since the state-space coordinates of the B(A, B, C, D) and the system in (2) are identical.Note that the state estimatex(t) = x ini is a natural choice when full-state measurements are available or when input/output measurements are deterministic.Corollary 5.1: (Equivalent Closed Loop Behaviour): Consider the MPC Algorithm 1 and the DeePC Algorithm 2 with Q 0, R \u227b 0, and U, Y convex and non-empty. Under the assumptions of Theorem 5.1, Algorithm 1 and Algorithm 2 result in equivalent closed-loop behaviour, i.e., the optimal control sequence u \u22c6 and corresponding system output y \u22c6 at every iteration is identical.Proof: Since R \u227b 0, the cost function in (2) is strictly convex in the decision variable u. Thus, since the constraints are convex and non-empty, a solution (u \u22c6 MPC , x \u22c6 MPC , y \u22c6 MPC ) to (2) exists, and u \u22c6 MPC is unique (see, e.g., ). Similarly, the cost function in (6) is strictly convex in the decision variable u and the constraints are convex and non-empty. Hence, a solution (g \u22c6 DeePC , u \u22c6 DeePC , y \u22c6 DeePC ) to (6) exists, and u \u22c6 DeePC is unique. Since the cost function in (2) and (6) coincide and the feasible sets of (2) and (6) are equal (by Theorem 5.1), then u \u22c6 MPC = u \u22c6 DeePC . Applying control inputs (u(t), . . . , u(t + s)) = (u \u22c6 0 , . . . , u \u22c6 s ) for some s \u2264 N \u2212 1 to system (1) yields corresponding output sequence (y(t), . . . , y(t + s) = (y \u22c6 0 , . . . , y \u22c6 s ). Updating col(u ini , y ini ) to the most recent input/output measurements and setting the state estimate in Algorithm 1 to x ini as in Theorem 5.1 yields equal feasible sets. Repeating the above argument, both algorithms compute an identical optimal control sequence. This argument can be repeated for all iterations of the algorithms proving the result.One notable feature of the DeePC algorithm presented in Algorithm 2 is its simplicity when compared to reinforcement learning approaches , and other related modelbased schemes. The DeePC algorithm achieves system ID, state estimation, and trajectory prediction with one linear equation resulting in a quadratic program with T \u2212T ini \u2212N +1 number of decision variables where T \u2208 Z >0 is the amount of data collected. In order to satisfy the persistency of excitation assumption in Theorem 5.1, one must collect a minimum of (m + 1)(T ini + N + n(B)) \u2212 1 data samples implying that the number of decision variables in (6) is at least m(T ini + N ) + (m + 1)n(B).Note that the persistency of excitation assumption assumes knowledge of n(B) and \u2113(B), which are properties that are a priori unknown. We know that \u2113(B) \u2264 n(B) by the Cayley-Hamilton theorem. Hence an upper bound for n(B) is sufficient. In practice, one would simply collect a sufficiently large amount of data to exceed the necessary amount for persistency of excitation. If, however, n(B) is underestimated and an insufficient amount of data is collected, the matrix col(U p , Y p , U f , Y f ) will represent a reduced order linear system with an approximate input/output behaviour. The precise implications of this model reduction need to be investigated in future work. VI. BEYOND DETERMINISTIC LTI SYSTEMSIn this section we provide preliminary results which shows the promising extension of the DeePC algorithm beyond deterministic LTI systems. We offer insightful algorithmic extensions by means of salient regularizations, show their utility through a numerical study, and provide plausible reasoning for the regularizations. A. Regularized DeePC AlgorithmConsider now the nonlinear discrete-time system given bywhere \u03b7(t) \u2208 R p is white measurement noise, and f : R n \u00d7 R m \u2192 R n and h : R n \u00d7 R m \u00d7 R p \u2192 R p are not necessarily linear. One may also consider a system affected by process noise. However, we focus on systems only affected by measurement noise in order to isolate its effect on the DeePC algorithm. To apply the DeePC algorithm to system (7), we propose three regularizations to the optimal control problem . In particular, we introduce the following regularized optimization problem:where \u03c3 y \u2208 R Tinip is an auxiliary slack variable, \u03bb y , \u03bb g \u2208 R >0 are regularization parameters, andis a low-rank matrix approximation of col(U p , Y p , U f , Y f ). Let us explain these three regularizations. We offer convincing numerical evidence in Section VI-B. Slack variable: When the output measurements are corrupted by noise, the constraint equation in (6) may become inconsistent. Hence, in (8) we include the slack variable \u03c3 y in the constraint to ensure feasibility of the constraint at all times. We penalize the slack variable with a weighted one-norm penalty function. Choosing \u03bb y sufficiently large gives the desired property that \u03c3 y = 0 only if the constraint is infeasible (see, e.g., ), that is, only if the data is inconsistent.One-norm regularization on g: The cost includes a onenorm penalty on g. We conjecture that this regularization is related to distributionally robust optimization problems, in which similar regularization terms arise , .Low-rank approximation: By performing a low-rank matrix approximation (e.g., via singular value decomposition (SVD) and truncation ), we take into account only the most dominant sub-behaviour (corresponding to the largest singular values), resulting in a data matrix describing the behaviour of the closest deterministic LTI system (where \"closest\" is measured with the Frobenius norm in the SVD case). In the case of noisy measurements, the SVD filters the noise. In the case of nonlinear dynamics (which can be lifted to infinite-dimensional linear dynamics with a nonlinear output map , ), the SVD results in a matrix describing an approximate LTI model, i.e., the most relevant basis functions of the infinite-dimensional lift whose dimension can be chosen by adjusting the SVD cutoff. Note that after performing the low-rank approximation, the DeePC algorithm does not require that the matrix col( U p , Y p , U f , Y f ) have a Hankel structure. This is in contrast to subspace ID techniques, in which low-rank approximations must be carefully modified in order to preserve the Hankel structure of the data matrix resulting in higher algorithmic and computational complexity . B. Aerial Robotics Case StudyWe illustrate the performance of the regularized DeePC algorithm, i.e., Algorithm 2 with (8), by simulating it on a high-fidelity nonlinear quadcopter model , and compare the performance to system identification (ID) followed by MPC using the identified model. The states of the quadcopter model are given by the 3 spatial coordinates (x, y, z) and their velocities, and the 3 angular coordinates (\u03b1, \u03b2, \u03b3) and their velocities, i.e., the state is (x, y, z,\u1e8b,\u1e8f,\u017c, \u03b1, \u03b2, \u03b3,\u03b1,\u03b2,\u03b3). The inputs are given by the thrusts from the 4 rotors, (u 1 , u 2 , u 3 , u 4 ). We assume full state measurement to facilitate the comparison to standard MPC. Data was collected from the nonlinear model subject to additive white measurement noise. We collected 214 input/output measurements with a sample time of 0.1 seconds. Drawing the input sequence from a uniformly distributed random variable ensured that the data was persistently exciting. For the model-based MPC, the data was used to identify the system parameters through the least square prediction error method with an assumed state dimension of 12. The following parameters were chosen for the optimization problems  and :The thrust from each rotor was constrained between 0 and 1, and the (x, y, z) coordinates were constrained between \u22123 and 3.We simulated the system ID followed by the MPC algorithm and the regularized DeePC algorithm on the nonlinear and stochastic quadcopter model in which the quadcopter was commanded to follow a series of figure-eight trajectories for a duration of 60 seconds. We observe that DeePC performs better than sequential ID and MPC in terms of reference tracking and constraint satisfaction; see  for an illustration.Another simulation was performed in which the quadcopter was commanded to perform a simple step trajectory in the (x, y, z) coordinates with the same constraints listed above. The duration of constraint violations and the cost were measured. This was repeated 30 times with different data sets for constructing col(U p , Y p , U f , Y f ), and different random seeds for the measurement noise. The results are displayed in  and show that DeePC consistently outperforms identification-based MPC in terms of cost and constraint satisfaction. While these observations should be made cautiously, an intuitive explanation for the superior performance of DeePC is that (8) simultaneously optimizes for the best system model, state estimation, and control policy, whereas conventional MPC requires fixing a system model and performs these tasks independently.To study the effect of the regularizations on the performance of the DeePC algorithm, we performed a sensitivity analysis on the regularization parameters \u03bb y and \u03bb g . We did not perform any low-rank approximation to the data. The quadcopter was commanded to follow the same step trajectory as in the previous simulation. The duration of constraint violations and cost were measured. This was repeated 8 times with different data sets, and the duration of constraint violations and cost were averaged over these 8 data sets. The results in  show that the regularizations improve performance.Our preliminary simulations suggest that one-norm reg-ularization of \u03bb g is more effective and robust than a lowrank approximation of the Hankel matrix col(U p , Y p , U f , Y f ).In fact, the latter appears to be sensitive and needs to be performed on a case by case basis to avoid unstable behaviour. This will be investigated in future work. VII. CONCLUSIONWe presented a data-enabled algorithm that can be applied to unknown LTI systems and formally showed its equivalence to the classical MPC algorithm. The DeePC algorithm uses a finite data set to learn the behaviour of the unknown system and computes optimal controls using real-time feedback to drive the system along a desired trajectory while respecting system constraints. Furthermore, we simulated a regularized version of the algorithm on stochastic nonlinear quadcopter dynamics illustrating its capabilities beyond deterministic LTI systems. The performance was superior when compared to system ID followed by MPC. Ongoing and future work focuses on the robustness of the DeePC algorithm and its regularization when applied to stochastic and nonlinear dynamics.\n###\n"}
{"summary": "has benchmark: Benchmark Habitat 2020 Point Nav test-std\nhas research problem: Robot Navigation\nhas model: ego-localization\nsame as: https://en.wikipedia.org/wiki/Robot_navigation", "text": "#Properties\nhas benchmark, has research problem, has model, same as\n#Text\nRecent work has presented embodied agents that can navigate to pointgoal targets in novel indoor environments with near-perfect accuracy. However, these agents are equipped with idealized sensors for localization and take deterministic actions. This setting is practically sterile by comparison to the dirty reality of noisy sensors and actuations in the real world-wheels can slip, motion sensors have error, actuations can rebound. In this work, we take a step towards this noisy reality, developing point-goal navigation agents that rely on visual estimates of egomotion under noisy action dynamics. We find these agents outperform naive adaptions of current point-goal agents to this setting as well as those incorporating classic localization baselines. Further, our model conceptually divides learning agent dynamics or odometry (where am I?) from task-specific navigation policy (where do I want to go?). This enables a seamless adaption to changing dynamics (a different robot or floor type) by simply re-calibrating the visual odometry model-circumventing the expense of retraining of the navigation policy. Our agent was the runner-up in the PointNav track of CVPR 2020 Habitat Challenge. ApproachPreliminaries. We start by describing the Point-Goal navigation task with oracle localization and current agent architectures for the task, before describing the details of our approach.In Point-Goal navigation , an agent is spawned at a random pose in an unseen environment and asked to navigate to goal coordinates specified relative to its start location. At every step of the episode, the agent receives RGB-D observation inputs via its visual sensors and a precise estimate of its location in the trajectory (relative to the start of the episode) via an idealized GPS+Compass sensor. Using these inputs for the current step, the agent performs an action by predicting a distribution over a discrete action space -{move-forward, turn-left, turn-right, stop}.Prior work  has trained recurrent neural policies for Point-Goal navigation end-to-end using RL. At a high-level, these recurrent policies (modelled as a 2-layer LSTM) typically predict actions based on the (ResNet-50) encoded representation of the current visual observation, its previous action, the goal coordinates for the episode and the noise-free localization estimate from the GPS+Compass sensor. Note that given the information about (a) the goal coordinates and (b) its current location both with respect to the start of the episode, it is straightforward for the agent to derive a noise-free estimate of the goal coordinates relative to its current state at every point in time.Point-Goal Navigation without Oracle Localization. Our proposed approach removes the oracle localization information (GPS+Compass sensor) and instead equips agents with an odometry module that is responsible for predicting per-action egomotion estimates. Access to such an odometer allows the agent to integrate its egomotion estimates over the course of its trajectory -thereby deriving a potentially erroneous substitute of the localization information. Taking inspiration from existing literature in visual odometry, we train our odometry model to regress to the change in pose between two sequential observations. In the sections that follow, we describe the details of our visual odometry model, the dataset collection protocol that was followed to collect data for training the odometer and its integration with the agent policy.Visual Odometry Model. We design our odometry model as a CNN that takes visual observation pairs (in our case, depth maps) as input and outputs an estimate of the relative pose change between the two states. We characterize the pose change as the positional offset (\u2206x, \u2206y, \u2206z) and change in heading/yaw (\u2206\u03b8) of the second state with respect to the first (  shows the agent coordinate system). More concretely, let I t and I t+1 be the depth images corresponding to the agent states at time t and t + 1, respectively. Both frames are depth concatenated and passed through a series of 3 convolutional layers: (Conv 8\u00d78, ReLU, Conv 4\u00d74, ReLU, Conv 3\u00d73, ReLU). Subsequent fc-layers generate the flattened 512-d embedding for the depth map pair, followed by predictions for \u2206x, \u2206y, \u2206z, and \u2206\u03b8 (egomotion deltas). The egomotion CNN is trained to regress to the ground-truth egomotion deltas by minimizing the smooth-L1 loss.Egomotion Dataset. To train our odometry model, we require a dataset comprising of observation pairs and the corresponding ground-truth egomotion between the two states defined by the observation pairs. These constitute the inputs and regression targets for the odometry CNN respectively.In order to collect this dataset, we adopt the follwing protocol. We take a Point-Goal navigation agent that has been trained with oracle localization and use its policy to unroll trajectories. Then, we sample pairs of source (src) and target (tgt) agent states from within those trajectories uniformly at random. For each (src, tgt) pair of states, we record (a) the corresponding visual observations (v t and v t+1 ) and (b) the ground-truth 6-DoF camera pose from the simulator. The latter comprises of a translation vector, t \u2208 IR 3 , denoting the agent's location in world coordinates and a rotation matrix, R \u2208 SO(3), representing a transformation from agent's current state to the world. Therefore, for a given pair of (src, tgt) states, we can obtain our egomotion transformation between (src, tgt) as:In the absence of noise in agent actuations, we would, in principle, expect to obtain a deterministic mapping between agent actions and odometry readings. However, in practice, the agent occasionally suffers displacements along x while executing move-forward (see (a) for agent coordinate system). This happens due to collisions with the environment and in such cases, the agent's displacement along z i.e. its heading is also different than the standard setting of 0.25m. turn-left and turn-right always correspond to 10 \u2022 (or 0.17 radians).We sample a total of 1000 (src, tgt) pairs from each of the 72 scenes in the Gibson training split  for a total of 72k pairs. Within each scene, we balance out the sampling of the 1000 points by collecting an equal number of points from each of the 122 trajectories in that scene. Since our dataset has been collected by sampling from unrolled agent trajectories, the distribution of actions is representative of the type of actions an agent might take while navigating -58%, 21% and 21% of the dataset corresponds to move-forward, turn-left and turn-right actions respectively.Integrating Egomotion Estimation with the Agent. Having described the odometry model and the dataset that it's trained on, we now discuss how it integrates into the agent architecture. Recall (from Sec. 3) that, owing to the availability of perfect localization, the Point-Goal navigation agents from  can derive the relative goal coordinates at every state. In the absence of GPS+Compass, our proposed agent uses the odometry model to first estimate its location in the trajectory and subsequently, predicts the relative goal coordinates as follows.At every step, the egomotion predictions from the odometry CNN are first converted to a 4 \u00d7 4 transformation matrix that represents a transformation of coordinates between the agent's previous and current states in the trajectory (using the equation above). The transformation is then applied to the relative goal coordinate predictions from the previous time step to project them on to the  loc t ,loc t : oracle and estimated localization ego t : egomotion prediction (a) Agent coordinate system : (a) shows the agent coordinate system. We also show samples of data points from the egomotion dataset collected under (b) noiseless and (c) noisy actuation settings. Each data point consists of visual observations (RGB-D maps) along with ground-truth egomotion (\u2206\u03b8 represents a rotation around Y) between two states defined by the observations. (d) We show architectures for our proposed (ego-localization) and baseline agents.coordinate system defined by the agent's current state. This generates the relative goal coordinate input for the current step of the policy network.Noisy actuations. Real-world robot locomotion is far from deterministic due to the presence of actuation noise. In order to model such noisy agent actions, we leverage noise models derived from real-world benchmarking of an actual, physical robot by the authors in . Specifically, we use both linear and rotational noise models from LoCoBot . The former is a bi-variate Gaussian (with a diagonal covariance matrix) that models egomotion errors along the z and x axis whereas the latter is a uni-variate Gaussian modeling egomotion errors in the agent's heading. For any given action, we perform truncated sampling and add noise from both the linear and rotational noise models.We collect a version of the egomotion dataset under the noisy actuation setting by following the same protocol as described in Sec. 3. Under this setting, the agent suffers non-zero egomotion deltas along all degrees of freedom for all action types. Please refer to the Supplementary document for details regarding the parameters of actuation noise models and a comparison of the distributions of per-action egomotion deltas in the dataset across noiseless and noisy actuation set-ups.Training. The odometry CNN is pre-trained on the egomotion dataset and kept frozen. We train the agent's navigation policy using DD-PPO [3] -a distributed, decentralized and synchronous implementation of the Proximal Policy Optimization (PPO) algorithm.Let d t be the geodesic distance to target at time step t. Furthermore, let s denote the terminal success reward obtained at the end of a successful episode (with I success being the indicator variable denoting episode success) and \u03bb be a slack-reward to encourage efficient exploration. Then, the reward r t obtained by the agent at time t is given by: r t = s . I success success reward. We set s = 1.0, \u03bb = \u22120.01 for all our experiments. We train all our agents for a maximum of 60M frames. We initialize the weights of the visual encoder with those from an agent that has been trained (using ground-truth localization) for 2.5B frames. Check Supplement for additional training details.4 Baselines no-localization: This is a naive adaption of the Point-Goal agents from  to our setting with no localization information. The policy network for the agents in  take the previous action, visual observations, episode goal and oracle GPS+Compass as inputs at every step (Sec. 3). For adapting this model to our setting, we drop the GPS+Compass input (keeping the other 3 unchanged). We train this agent with the same reward settings and losses, as described in Sec 3 (and consistent with ). The performance of this baseline tells us how well an agent would do if it is trained for the Point-Goal navigation task using the state-of-the-art approach  but without any localization information whatsoever to guide its navigation.aux-loss (aux-loss-goal + aux-loss-ego): These agents are similar in architectural set-up to the no-localization baseline. However, they are trained with additional auxiliary losses that encourage them to predict information pertinent to their localization. At every step, the policy network is additionally trained to predict, from its hidden state, either the goal coordinates relative to its current state (aux-loss-goal) or the relative change of pose between its previous and current states (aux-lossego). Note that these baselines have access to the GPS+Compass sensor during training, just as our approach does to train our odometry module. But, they use this information indirectly as auxiliary losses rather than directly as localization information. Both our approach and these baselines do not use the oracle GPS+Compass information at test time. Comparing our approach to these baselines demonstrates the value of an explicit odometry module as opposed to relying on a monolithic neural policy agent to learn that it should infer localization-related information from auxiliary signals.dead-reckoning: The agent derives localization estimates using a static look-up table that maps actions to associated odometry readingsmove-forward: displacement of 0.25m along heading, turn-left/turn-right: 10 \u2022 on-spot rotation. A comparison with this baseline answers the question is it really necessary to learn the odometry estimates instead of naively memorizing them?classic:We also compare with a classic robotic navigation pipeline that has modular components for map creation, localizing the agent on the map, planning a path, selecting a waypoint on the planned path and moving the agent along the predicted waypoint. Such pipelines have been extensively used in robotics with several choices available for each component. Following , we leverage prior work  that proposes a complete implementation of such a pipeline that can be readily deployed in simulation.  uses ORB-SLAM2  as the agent localization module. We use the same set of hyperparameters as reported in the original work (refer to the Supplementary for more details).ego-localization: Finally, this is our approach wherein the agent uses a trained odometry model to derive localization estimates for navigation using a neural policy.As a benchmark for upper-bound performance, we also report numbers for the agent that navigates in the presence of 'oracle' localization (gt-localization) during both training and test episodes. Results and FindingsEnvironment. We use the Habitat simulator  with the Gibson dataset of 3D scans  for our experiments. We leverage the human-annotated ratings (on a scale of 1-5) provided by  for mesh reconstructions and only use high-quality environments (\u2265 4 rating) for our experiments. For a given scene, we use the dataset of Point-Goal navigation episodes from . Overall, we work with 8784 episodes across 72 scenes in train and 994 episodes across 14 scenes in val.Metrics. A navigation episode is deemed successful if the agent calls stop within a distance of 0.2m from the target location. We measure success rate as the fraction of successful episodes. Following prior work on Point-Goal navigation, we also report performance on the Success Weighted by Path Length (SPL) metric , averaged across all episodes in the validation split.Owing to their reliance on a binary indicator of episode success, both success rate and SPL do not provide any information about episodes that fail. Consider the scenario ) wherein an agent follows a path that closely resembles the shortest path between start and goal locations, but prematurely calls stop right at the 0.2m success perimeter boundary. Both success and SPL for the episode are 0. Although the agent managed to navigate reasonably well, its performance gets harshly ignored from the overall statistics. Hence, relying on success and SPL as the sole metrics (as done by prior work) often leads to an incomplete picture of the agents overall performance. We observe that this problem is even more acute in set-ups without GPS+Compass where noisy pose estimates directly affects the agent's decision to call stop at the right distance relative to the goal.To get a more holistic estimate of the agent's performance, we also report the geodesic distance to target upon episode termination (geo d T). In addition to that, we propose a new metric called Soft-SPL. The SoftSPL for an episode is defined as:where s and p are the lengths of the shortest path and the path taken by the agent. SoftSPL replaces the binary success term in SPL with a \"soft\" value that is indicates the progress made by the agent towards the goal (\"progress\" can be negative if the agent ends up farther away from the goal than where it started).We now present navigation results for our proposed model and compare with baselines in Tab. 1. How crucial is localization for PointNav?We answer this by comparing the upper-bound performance of the agent trained and evaluated with oracle localization (GT-localization) with an agent that has been trained and evaluated to navigate without any form of localization input (the no-localization baseline). We see a drastic drop in both SPL (0.866 to 0.096) and Success (0.948 to 0.099) as the localization input is taken away. This makes it seem like the baseline method is completely failing. However, we observe that these baselines are able to navigate reasonably well, they are just not reaching the exact goal location (SoftSPL of 0.726 for no-localization vs. 0.865 for gt-localization). Therefore, to get a meaningful measure of performance, we compare SoftSPL numbers, wherever appropriate in the text that follows.Next, we also compare the dead-reckoning agent with memorized odometry estimates with the nolocalization baseline. We see that the former outperforms the latter (SPL=0.096, SoftSPL=0.726 vs. SPL=0.303, SoftSPL=0.797). This shows that having some form of localization (albeit, erroneous) is essential for navigation. Our proposed agent with learnt odometry prediction) is able to further outperform dead-reckoning with 67.6% relative improvements in SPL (0.508 vs. 0.303).How privileged is an agent with access to oracle localization at test time? By comparing the gt-localization agent with our ego-localization approach, we find that our agent succeeds less often (SPL=0.508, Success=0.535 vs. SPL=0.866, Success=0.948). However, it is able to make progress towards the goal by a degree that reasonably matches that of the \"oracle\" agent (SoftSPL=0.813 v/s SoftSPL=0.865). This can be explained by the fact that our agent, with noisy estimates of its location thinks that it has reached the goal and calls stop prematurely (geo d T=0.959 v/s geo d T=0.388).How well can we do by simply memorizing odometry estimates? Recall that, in the absence of any noise in the agent's actuations, memorized egomotion estimates will be incorrect only during collisions. Our agent consistently outperforms dead-reckoning across all navigation metrics in the noiseless settings -with 2%, 67.6%, 72% and 8.4% relative improvements in SoftSPL, SPL, Success and geo d T, respectively. This points to the utility of learning egomotion from visual inputs.Transferring policies to noisy actuations. We shift our attention to the noisy actuation setting now. For this setting, we present results for the following set-ups: (a) old-policy: where we evaluate policies trained in noiseless actuations without any fine-tuning, (b) re-trained policy: where the policies are re-trained and evaluated in the noisy set-up.As one would expect, when policies trained in a noiseless actuation setting are transferred to environments with noisy actions, navigation performance suffers. For the no-localization, gt-localization and our ego-localization agents, the SoftSPLs drop from 0.726, 0.865 and 0.813 to 0.491, 0.666 and 0.280 (old-policy setting for no-localization, gt-localization and ego-localization in Tab. 1), respectively. It is interesting to note that this performance drop affects agents with imperfect localization estimates worse than those with oracle localization. This is because, for the former, noisy actuations become all the more challenging -the agent doesn't end up where it would expect its actions to lead to, and there is no corrective signal that can allow for a potential re-calibration towards the goal.  Next, we investigate ways to retrain our agents to adapt to noise in the actuations. Recall that our proposed agent offers a decoupling between learning dynamics (odometry) and navigation (policy). Taking advantage of this, we treat the policy as a \"swappable\" component that can be used with a different odometry model re-calibrated to noisy actuations. We first fine-tune the odometer on the noisy version of the egomotion dataset (Sec 3), followed by using the fine-tuned odometer with the frozen policy (the retrained-odom setting in Tab. 1). We see significant performance gains in doing so -a relative improvement of 76.43% in SoftSPL (0.280, old-policy vs. 0.494, retrained-odom).In the absence of a separation between dynamics and policy, the only way to adapt the nolocalization and gt-localization agents to the noisy setting is via an expensive re-training of the policy. Doing so leads to performance gains across both sets of agents -relative improvements of 4.88% and 0.75% in SoftSPL, respectively (old-policy v/s re-trained policy). Moreover, doing the analogous re-training of our agent's policy (with the re-trained odometer) i.e. re-trained-odom+policy, leads us to out-perform all baseline agents in noisy actuation settings as well (SoftSPL=0.576 for our proposed agent vs. 0.515, 0.407 for no-localization and dead-reckoning, respectively).Comparisons with classic approaches. In the noiseless setting, our learnt visual odometry estimates lead to better PointNav agents than using ORB-SLAM2  for localization -with 39.2%, 6.3% and 18.9% relative improvements in SoftSPL, SPL and geo d T, respectively. With noisy actuations, the baseline has a higher success rate (SPL=0.267 vs. 0.047). However, it takes much longer to reach the goal (SoftSPL=0.301 vs. 0.576). Note that the high success but poor SPL of this baseline is simply a statement about its heuristic path-planning aspect, and not about the performance of SLAM-based localization. Specifically, the baseline uses a set of hand-coded heuristics to select and move towards a waypoint on the planned-path. Every-so-often (with a probability of 0.1), the agent executes a randomly sampled action (an avenue to recover, in case the agent gets stuck). Therefore, this baseline agent does make progress towards the goal, but in a suboptimal number of steps on account of the above heuristics-based/occasionally-random sampling of actions.Habitat Challenge 2020 Our submission, comprising of a simple modification of our proposed agent, to the PointNav track of Habitat Challenge 2020, was ranked #1 on the Test-Challenge leaderboard with respect to SoftSPL (0.596) and distance to goal (1.824) and #2 with respect to SPL (0.146). Please check the Supplementary document for details regarding the challenge configuration settings, modifications to our approach and a snapshot of the leaderboard. ConclusionWe develop Point-Goal navigation agents that can operate without explicit localization information from the environment (no GPS+Compass sensor) in the presence of noisy transition dynamics. Our agent, with learnt visual odometry modules, demonstrates performance improvements over naive adaptions of existing agents to our setting as well as strong (learnt and traditional) baselines and emerges as runners-up in the Habitat Challenge 2020. We also show that such a separation between learning the dynamics of the environment (via the odometer) and learning to navigate (via the policy) allows for a straight-forward transfer of our agent from noiseless to noisy actuation settings -circumventing the time and resource intensive training of near-perfect navigation policies as in prior work . We view this as a step towards one of the grand goals of the community -making navigation agents more suitable for deployment in the real world.8 Appendix LoCoBot noise modelsFor our experiments with noisy actuations, we source noise models that were collected in the real world by measuring the accuracies of position controllers (implementations of low-level control that the robot uses to get to a desired pose) on a physical robot. The authors in  experimented with implementations of 3 different controllers on both LoCoBot and LoCoBot-Lite  -(1) DWA,  Proportional Controller and (3) ILQR (refer to  for a description of the controllers). Trials in the real world were conducted to quantify the difference in the commanded v/s achieved state of the robot by using each of the above controllers.  reports that the errors were generally lower for LocoBot and ILQR performed the best among the controllers. Hence, we source the noise models derived from the LoCoBot-ILQR trials for experiments in our paper.To re-cap, we model both translational and rotational noise in the agent's actuations. Translational noise is measured along z (the direction of agent motion) and x (the direction perpendicular to motion) on the ground plane and modelled as a bi-variate Gaussian (with a diagonal co-variance matrix).Rotational noise is measured along rotation around y and modelled as a uni-variate Gaussian.Also recall that, for any given action, we add noise to both the agent's location as well as heading (by sampling from the respective noise distributions) in order to simulate say, an agent turning a bit while attempting to move forward (or, vice-versa). We use different sets of translational and rotational noise models for (a) linear motion (i.e. move-forward) and rotational motion (i.e. turn-left and turn-right). We now present the parameters of the noise models. The mean and variance for the uni-variate rotational noise model are: \u00b5 = 0.023 and \u03c3 = 0.012. DD-PPO TrainingRecall that we use DD-PPO  to train our agent policies. Following , we force the rollout collection by all DD-PPO workers to end (and model optimization to start) after 80% (pre-emptive threshold) of the workers have finished collecting rollouts. This is done in order to avoid delays due to \"stragglers and leads to better scaling across multiple GPUs. We use PPO with Generalized Advantage Estimation . We set the discount factor \u03b3 to 0.99, the GAE parameter \u03c4 to 0.95 and the clip parameter to 0.2 (along with a linear decay for the clip parameter with the number of PPO updates). We use the Adam optimizer with an initial learning rate of 2.5e-4 and with a linear decay. We clip gradients norms to a max of 0.5 before policy updates. These hyper-parameters are consistent with the experimental settings in . Hyperparameters for the classic navigation baselineThe classic navigation baseline (Sec 4 in the main paper) builds a map of size 400 \u00d7 400 where each grid/cell in the map corresponds to 0.1m \u00d70.1m dimensions of physical space. The mapper estimates an occupancy map of the environment from depth maps and camera intrinsics. At any given step, all points in the depth map are projected into an egocentric point cloud, followed by a thresholding operation where only points within a depth range of 0.1m to 4m are retained. The point cloud is then transformed to global scene coordinates with the help of pose estimates from the ORB-SLAM2 localization module. This is followed by projecting all points that lie within a range of [0.  : Visualization of samples from the egomotion dataset collected in a noiseless actuation setting. We group the data points according to the actionmove-forward, turn-left and turn-right. For every data point, we show the RGB image frames, depth maps and the relative pose change between the source and target agent states.Given the obstacle map generated by the mapper, the SLAM-estimated pose and the target goal location, the planner finds a shortest path from the agent's location to the goal via the D* Lite algorithm . Given that planned path, the baseline selects a waypoint along the path that lies at a distance of at least 0.5m from the agent's current state. If the agent's heading is within a range of 15 \u2022 from the direction of this waypoint, it executes a move-forward action. Otherwise, it rotates (turn-left or turn-right) towards the direction of the waypoint. Every so often (with a probability 0.1), a random action (among move-forward, turn-left and turn-right) is executed.  shows some qualitative examples of data points from our egomotion dataset that has been collected in the noiseless actuation setting. For each of the 3 action classes in our dataset (move-forward, turn-left and turn-right), we show randomly sampled data points. Recall (from Sec. 3 in the main paper) that each data point in our dataset consists of RGB frames and depth maps corresponding to the source (src) and target (tgt) agent states as well as the associated ground-truth relative pose change between src and tgt. Since there's no noise in the agent actuations, the agent always turns by exactly 10 \u2022 (or, 0.17 radians) when executing turn-left or turn-right (and doesn't suffer any displacements in the ground plane while doing so). Similarly, a move-forward doesn't lead to any change in the agent's heading. The right-most example under the move-forward action presents an instance where the agent suffers a collision (with a wall) while attempting to move forward. As a result, the agent also has some non-zero displacement along the x-axis (the direction perpendicular to the agent's heading in the ground plane). Note that the displacement along the z-axis (the agent's heading) is also different from 0.25m (the default actuation specification for the move-forward action). We group the data points according to the actionmove-forward, turn-left and turn-right. Qualitative examples from the Egomotion datasetFor every data point, we show the RGB image frames, depth maps and the relative pose change between the source and target agent states.or turn-right), we record non-zero pose changes across x, z as well as \u03b8 (that deviates from the default actuation specifications for the actions).  shows a comparison of the per-action egomotion distribution between the noiseless and noisy actuation settings. Standalone evaluation of the visual odometry modelRecall that, the odometry model that our proposed agent is equipped with is pre-trained on the egomotion dataset and then kept frozen during the training/evaluation of the agent policy. In Sec. 5 of the main paper, we report results for our agent's navigation performance (using our odometer to derive localization estimates). In addition to that, in this section, we also report numbers for a standalone evaluation of our visual odometry model on the task of regressing to the ground-truth relative pose estimates between source and target agent states.We perform such an evaluation of our odometry model under two settings -novel observation pairs from previously-seen environments (val-seen) and novel observation pairs from unseen environments (val-unseen). We generate the former by sampling data points from scenes in the training split (and ensuring there is no overlap with the train set of data points) and the latter by generating data points from scenes in the val split. For both splits (val-seen and val-unseen), the data collection protocol remains the same, as described in Sec. 3 in the main paper.For the odometry model trained on noiseless agent actuations, the Smooth-L1 loss on the val-unseen and val-seen splits are 0.56e-4 and 0.46e-4, respectively. The val-unseen split provides a comparatively more challenging set-up than val-seen due to the added complexity of previously-unseen environments (in addition to evaluation of novel observation pairs) and this is reflected in the trends in the Smooth-L1 loss values. Habitat Challenge 2020We submitted a straightforward modification of our proposed (ego-localization) agent to the Point-Goal navigation (PointNav) track of Habitat Challenge 2020 (organized as part of the Embodied AI Workshop at CVPR 2020). In addition to noisy agent actuations and the absence of a GPS+Compass sensor, the configuration settings of the challenge also include the following:\u2022 visual sensing noise (noisy RGB-D observation maps)\u2022 a change in the agent's sliding dynamics. As per the simulator settings used in the experiments reported in the paper (also consistent with prior work ), the agent slides along walls and boundaries of obstacles during collisions. This design choice was inspired by game engines where such sliding behavior allows for a smoother player control. However, this behavior is not realistic a real-world robot would bump into obstacles and simply stop upon colliding. \u2022 wider turn angles (30 \u2022 vs. 10 \u2022 )\u2022 physical agent dimensions and camera configuration parameters (spatial resolution, fieldof-view and camera height) set to match the settings in LoCoBot.For the purposes of our challenge submissions, we replaced the 3-layer CNN encoder of our visual odometry model with a ResNet18 based encoder and trained the odometer on a version of the egomotion dataset collected under the challenge settings. We found that using a higher capacity ResNet18 encoder in the odometer was necessary to outperform baselines in the presence of noise in the depth maps. We also re-train our agent policy under the new challenge settings.The challenge results are shown in . Our submission (under the team name, \"ego-localization\") was ranked #1 on the Test-Challenge leaderboard with respect to SoftSPL (0.596) and distance to goal (1.824) and #2 with respect to SPL (0.146).\n###\n"}
{"summary": "Pages: 138--151\nCustom1: Cloud service providers offer various facilities to their clients. The clients with limited resources opt for some of these facilities. They can outsource their bulk data to the cloud server. The cloud server maintains these data in lieu of monetary benefits. However, a malicious cloud server might delete some of these data to save some space and offer this extra amount of storage to another client. Therefore, the client might not retrieve her file (or some portions of it) as often as needed. Proofs of retrievability (PoR) provide an assurance to the client that the server is actually storing all of her data appropriately and they can be retrieved at any point of time. In a dynamic PoR scheme, the client can update her data after she uploads them to the cloud server. Moreover, in publicly verifiable PoR schemes, the client can delegate her auditing task to some third party specialized for this purpose. In this work, we exploit the homomorphic hashing technique to design a publicly verifiable dynamic PoR scheme that is more efficient (in terms of bandwidth required between the client and the server) than the ``state-of-the-art'' publicly verifiable dynamic PoR scheme. We also analyze security and performance of our scheme./Cloud service providers offer various facilities to their clients. The clients with limited resources opt for some of these facilities. They can outsource their bulk data to the cloud server. The cloud server maintains these data in lieu of monetary benefits. However, a malicious cloud server might delete some of these data to save some space and offer this extra amount of storage to another client. Therefore, the client might not retrieve her file (or some portions of it) as often as needed. Proofs of retrievability (POR) provide an assurance to the client that the server is actually storing all of her data appropriately and they can be retrieved at any point of time. In a dynamic POR scheme, the client can update her data after she uploads them to the cloud server. Moreover, in publicly verifiable POR schemes, the client can delegate her auditing task to some third party specialized for this purpose. In this work, we exploit the homomorphic hashing technique to design a publicly verifiable dynamic POR scheme that is more efficient (in terms of bandwidth required between the client and the server) than the \"state-of-the-art\" publicly verifiable dynamic POR scheme. We also analyze security and performance of our scheme.\nAnnote: ZSCC: NoCitationData[s0]/ZSCC: NoCitationData[s0]\nYear: 2020/2018\nNumber: 1\nVolume: 8\nJournal: IEEE Transactions on Cloud Computing/arXiv:1611.03982 [cs]\nmonth: January/August\nIdentifier: senguptaEfficientProofsRetrievability2020/senguptaEfficientProofsRetrievability2018\nBibliographyType: 7/7\nCustom3: \u26d4 No DOI found,Computer Science - Cryptography and Security\nNote: Comment: A version of the paper with the same title has been published in IEEE Transactions on Cloud Computing (DOI: 10.1109/TCC.2017.2767584)", "text": "#Properties\nPages, Custom1, Annote, Year, Number, Volume, Journal, month, Identifier, BibliographyType, Custom3, Note\n#Text\nCloud service providers offer various facilities to their clients. The clients with limited resources opt for some of these facilities. They can outsource their bulk data to the cloud server. The cloud server maintains these data in lieu of monetary benefits. However, a malicious cloud server might delete some of these data to save some space and offer this extra amount of storage to another client. Therefore, the client might not retrieve her file (or some portions of it) as often as needed. Proofs of retrievability (POR) provide an assurance to the client that the server is actually storing all of her data appropriately and they can be retrieved at any point of time. In a dynamic POR scheme, the client can update her data after she uploads them to the cloud server. Moreover, in publicly verifiable POR schemes, the client can delegate her auditing task to some third party specialized for this purpose. In this work, we exploit the homomorphic hashing technique to design a publicly verifiable dynamic POR scheme that is more efficient (in terms of bandwidth required between the client and the server) than the \"state-of-the-art\" publicly verifiable dynamic POR scheme. We also analyze security and performance of our scheme. Our ContributionWe summarize our contributions in this paper as follows.-We construct a dynamic proofs-of-retrievability (POR) scheme where the client outsources her data file to the cloud server and she can update the content of the file later. Our construction is based on the homomorphic hashing technique. -Our dynamic POR scheme offers public verifiability, that is, the client can delegate the auditing task to a third party auditor who performs audits on the client's behalf. -We show that our scheme is secure in the sense that the client gets an assurance that her data file stored by the server is authentic and up-to-date, and all the data blocks can be retrieved by the client as often as needed. -We analyze the performance of our scheme and compare it with other existing dynamic POR schemes (having private or public verifiability). -Our publicly verifiable dynamic POR scheme enjoys more efficiency (in terms of communication bandwidths required for a write and an audit) than the \"state-of-the-art\" publicly verifiable dynamic POR scheme . Moreover, unlike the latter scheme, there is no need to validate or certify the public parameters in our scheme for every write operation as they are fixed since the initial setup phase.The rest of the paper is organized as follows. Section 2 describes the preliminaries and background related to our work. In Section 3, we survey the existing literature on secure cloud storage schemes. Section 4 provides a detailed construction of our publicly verifiable dynamic POR scheme. We analyze the security of our scheme in Section 5. Finally, in Section 6, we discuss the performance of our scheme and compare our scheme with other existing dynamic POR schemes based on different parameters (shown in ). We also show that our scheme is more efficient than the publicly verifiable dynamic POR scheme proposed by Shi et al. . In the concluding Section 7, we summarize the work done in this paper. NotationWe take \u03bb to be the security parameter. An algorithm denoted by A(1 \u03bb ) is a probabilistic polynomialtime algorithm when its running time is polynomial in \u03bb and its output y is a random variable which depends on the internal coin tosses of A. If A is given access to an oracle O, we denote A by A O also. An element a chosen uniformly at random from a set S is denoted as a R \u2190 \u2212 S. A function f : N \u2192 R is called negligible in \u03bb if for all positive integers c and for all sufficiently large \u03bb, we have f (\u03bb) < 1 \u03bb c . Erasure CodeA (m,\u00f1, d) \u03a3 -erasure code  is an error-correcting code  that comprises an encoding algorithm Enc: \u03a3\u00f1 \u2192 \u03a3m (encodes a message consisting of\u00f1 symbols into a longer codeword consisting ofm symbols) and a decoding algorithm Dec: \u03a3m \u2192 \u03a3\u00f1 (decodes a codeword to a message), where \u03a3 is a finite alphabet and d is the minimum distance (Hamming distance between any two codewords is at least d) of the code. The quantity\u00f1 m is called the rate of the code. A (m,\u00f1, d) \u03a3 -erasure code can tolerate up to d \u2212 1 erasures. If d =m \u2212\u00f1 + 1, we call the code a maximum distance separable (MDS) code. For an MDS code, the original message can be reconstructed from any\u00f1 out ofm symbols of the codeword. Reed-Solomon codes  and their extensions are examples of non-trivial MDS codes. Merkle Hash TreeA Merkle hash tree  is a binary tree where each leaf-node stores a data item. The label of each leafnode is the data item stored in the node itself. A collision-resistant hash function h CR is used to label the intermediate nodes of the tree. Each of the outputs of h CR on different inputs is a binary string of length O(\u03bb). The label of a intermediate node v is the output of h CR computed on the labels of the children nodes of v. A Merkle hash tree is used as a standard tool for efficient memory-checking.  shows a Merkle hash tree containing the data items {d 1 , d 2 , . . . , d 8 } stored at the leaf-nodes. Consequently, the labels of the intermediate nodes are computed using the hash function h CR . The hash value of the node A is the root-digest. The proof showing that a data item d is present in the tree consists of the data item d and the labels of the nodes along the associated path (the sequence of siblings of the node containing the data item d). For example, a proof showing that d 3 is present in the tree consists of {d 3 , (d 4 , l D , l C )}, where d 4 , l D and l C are the labels of the nodes K, D and C, respectively. Given such a proof, a verifier computes the hash value of the root. The verifier outputs accept if the computed hash value matches with the root-digest; it outputs reject, otherwise. The size of a proof is logarithmic in the number of data items stored in the leaf-nodes of the tree.Due to the collision-resistance property of h CR , it is infeasible (except with some probability negligible in the security parameter \u03bb) to add or modify a data item in the Merkle hash tree without changing its root-digest. Digital Signature SchemeDiffie and Hellman introduce the public-key cryptography and the notion of digital signatures in their seminal paper \"New Directions in Cryptography\" . Rivest et al. propose the first digital signature scheme based on the RSA assumption . Boneh et al.  introduce the first signature scheme where the signatures are short (e.g., such a signature of size 160 bits provides the security comparable to that of a 1024-bit RSA signature). The DSA (Digital Signature Algorithm)  and ECDSA (Elliptic Curve Digital Signature Algorithm)  signature schemes (variants of the ElGamal signature scheme ) are widely used in practice.A digital signature scheme consists of the following polynomial-time algorithms: a key generation algorithm KeyGen, a signing algorithm Sign and a verification algorithm Verify. KeyGen takes as input the security parameter \u03bb and outputs a pair of keys (psk, ssk), where ssk is the secret key and psk is the corresponding public verification key. Algorithm Sign takes a message m from the message space M and the secret key ssk as input and outputs a signature \u03c3. Algorithm Verify takes as input the public key psk, a message m and a signature \u03c3, and outputs accept or reject depending upon whether the signature is valid or not. Any of these algorithms can be probabilistic in nature. The correctness and security (existential unforgeability under adaptive chosen message attacks ) of a digital signature scheme are described as follows. 2. Security: Let Sign ssk (\u2022) be the signing oracle and A be any probabilistic polynomial-time adversary with an oracle access to Sign ssk (\u2022). The adversary A adaptively makes polynomial number of sign queries to Sign ssk (\u2022) for different messages and gets back the signatures on those messages. The signature scheme is secure if A cannot produce, except with some probability negligible in \u03bb, a valid signature on a message not queried previously, that is, for any probabilistic polynomial-time adversary A Sign ssk (\u2022) , the following probabilityis negligible in \u03bb, where Q s is the set of sign queries made by A to Sign ssk (\u2022). The probability is taken over the internal coin tosses of A. Discrete Logarithm AssumptionThe discrete logarithm problem  over a multiplicative group G q = g of prime order q and generated by g is defined as follows.Definition 1 (Discrete Logarithm Problem). Given y \u2208 G q , the discrete logarithm problem over G q is to compute x \u2208 Z q such that y = g x . The entities involved in a dynamic POR scheme. The client (data owner) processes the data file F to form another file F \u2032 and outsources it to the cloud storage server. She can later read or write the outsourced data. For a privately verifiable scheme, the client performs audits on her data. For a publicly verifiable scheme, she can delegate the auditing task to a third party auditor who performs audits on behalf of the client.The discrete logarithm assumption over G q says that, for any probabilistic polynomial-time adversary A(1 \u03bb ), the probabilityis negligible in \u03bb, where the probability is taken over the internal coin tosses of A and the random choice of x. Dynamic Proofs of RetrievabilityWe define a proofs-of-retrievability scheme for dynamic data as follows .Definition 2 (Dynamic POR). A dynamic POR scheme consists of the following protocols between two stateful parties: a client (data owner) and a server.-Init(1 \u03bb , n, \u03b2, F ): This protocol associates a random file-identifier fid to the data file F consisting of n data blocks each of \u03b2 bits, and it outputs the client state state C and another file F \u2032 to be stored by the server. -Read(i, F \u2032 , state C , fid): This protocol outputs the data block at the i-th location of the current state of the file or abort. -Write(i, updtype, B, F \u2032 , state C , fid): This protocol inserts the block B after the i-th block of the file or sets i-th block of the file to B or deletes the i-th block of the file (B is null in this case) based on the value of the variable updtype. It outputs updated (F \u2032 , state C ) or abort. -Audit(F \u2032 , state C , fid): This protocol checks memory contents of the current state of the data file and outputs 1 or 0.A dynamic POR scheme is privately verifiable if only the client with some secret information can perform an audit, that is, state C is secret. Otherwise, it is publicly verifiable. For a publicly verifiable dynamic POR scheme, a third party auditor (TPA) can audit the client's data on behalf of the client who delegates her auditing task to the TPA. In this case, we use the term \"verifier\" to denote an auditor who can be the TPA or the client herself.  shows the entities involved in a dynamic POR scheme. Security of a dynamic POR scheme is described in Section 5. Homomorphic Hash FunctionA homomorphic hash function h : F m \u2192 G q (for a finite field F and a multiplicative group G q of prime order q) is defined as a collision-resistant hash function satisfying the following two properties: 1) for vectors u, v \u2208 F m and scalars \u03b1, \u03b2 \u2208 F, it holds that h(\u03b1uKrohn et al.  construct a homomorphic hash function in the context of content distribution. The construction is similar to that proposed in incremental hashing schemes . Let G q be a multiplicative group of prime order q. Let m elements (generators) g 1 , g 2 , . . . , g m be selected randomly from G q . Then, the homomorphic hash of a vector u =The hash function thus constructed is homomorphic, and the collision-resistance property is derived from the discrete logarithm assumption over G q . We use this construction in our dynamic POR scheme to generate authentication tags for data blocks (see Section 4.2). Dynamic POR Scheme with Public VerifiabilityIn this section, we describe our publicly verifiable dynamic POR scheme with efficient writes and audits. Like the existing dynamic POR schemes , our construction also rely on the hierarchical structure provided by the oblivious RAM . Specifically, we follow a storage structure similar to the one proposed by Shi et al. . However, our construction is more efficient than their scheme in terms of the cost of a write operation and the cost of an audit. Our construction is based on collision-resistant homomorphic hashing technique  along with a digital signature scheme. To the best of our knowledge, the homomorphic hashing technique has not been used before in the context of POR schemes. Storage Structure for Data BlocksOur scheme relies on a storage structure similar to that proposed by Shi et al. . Let the client (data owner) associate a random file-identifier fid of \u03bb bit-size to the data file she wants to outsource to the cloud server. We assume that the data file is divided into blocks of size \u03b2 bits, and read (and write) operations are performed on these blocks. The value of \u03b2 is taken to be \u230alogp\u230b for a large primep. The way this primep is selected is discussed in Section 4.1. For static data, a standard way to guarantee retrievability of the file is to encode the file using an erasure code . The main drawback of using erasure codes in dynamic POR is that an update in a single block in a codeword (say, C) is followed by updates on other O(n) blocks in C, where n is the number of blocks being encoded to form C. The underlying idea to overcome this drawback is not to update the encoded copy (C) for every write operation (insertion, deletion or modification). Instead, it is updated (or rebuilt) only when sufficient updates are done on the data file. Thus, the amortized cost for writes is reduced dramatically. However, this encoded copy stores stale data between two such rebuilds. Therefore, a hierarchical log structure is maintained which temporarily stores values for the intermediate writes between two successive rebuilds of C. Each level of this hierarchical log is also encoded using an erasure code.We adopt the storage structure and code construction mentioned above in our scheme. However, we use collision-resistant homomorphic hashing to construct another hierarchical storage (discussed in Section 4.2) in order to reduce the cost of a write and an audit for the client. Our scheme involves the following three data structures in order to store the data blocks of the client's file:-an unencoded buffer U containing all the up-to-date data blocks of the file (that is, U is updated after every write operation is performed), -an encoded buffer C which is updated after every n writes (that is, C is rebuilt afresh by encoding the latest U after every n write operations), and -an encoded hierarchical (multiple levels of buffers) log structure H which accommodates all intermediate writes between two successive rebuilds of C (H is made empty after every n writes).We note that all of these data structures are stored on the cloud server. The server also stores two additional data structures,H andC (similar to H and C, respectively), for authentication tags described in Section 4.2. For each 0 l k, the l-th level H l = (X l , Y l ) consists of an encoded copy of 2 l data blocks using a (2 l+1 , 2 l , 2 l )-erasure code, where X l and Y l contain 2 l blocks each. The original data blocks encoded in H l arrive at time t, t + 1, . . . , t + 2 l \u2212 1 (mod n), where t is a multiple of 2 l . We describe the encoding procedure as follows. Structure ofLetp be a large prime such thatp = \u03b1\u2022(2n)+ 1 for some \u03b1 \u2208 N and the bit-size of a block \u03b2 = \u230alogp\u230b, where \u03b2 \u226b \u03bb. Letg denote a generator of Z * p . Then, \u03c9 =g \u03b1 modp is a 2n-th primitive root of unity modulop. When a block B is written to H, it is inserted in the topmost level (l = 0) if H 0 is empty. That is, X 0 is set to B. In addition, Y 0 is set to B \u2022 \u03c9 \u03c8(t) for the t-th ( mod n) write, where \u03c9 is the 2n-th primitive root of unity modulop. Here, \u03c8(\u2022) is the bit reversal function, where \u03c8(t) is the value of the binary string obtained by reversing the binary representation of t.If the top i levels H 0 , H 1 , . . . , H i\u22121 are already full, a rebuild is performed to accommodate all the blocks in these levels as well as the new block in H i (and to make all the levels up to H i\u22121 empty). Shi et al.  employ a fast incrementally constructible code based on Fast Fourier Transform (FFT) [8] 1 .  describes the algorithm for rebuild of X l that in turn uses the algorithm mix shown in . Although the algorithm deals with X l , the same algorithm can be used for rebuilding Y l if we replace the X arrays by corresponding Y arrays and the incoming block B by B \u2022 \u03c9 \u03c8(t) . We refer  for the form of the (2 l \u00d7 2 l+1 ) generator matrix G l for the l-th level code. Letx l be the vector containing 2 l data blocks most recently inserted in H (after applying a permutation). Then, the output of the algorithm mix for H l is the same as that whenx l is multiplied by G l . Any (2 l \u00d7 2 l ) submatrix of the generator matrix G l is full rank, and thus, the code achieves the maximum distance separable (MDS) property.As a concrete example, the rebuild of X 3 is demonstrated in . The other part of H 3 (that is, Y 3 ) is rebuilt in a similar fashion. We observe that, by using this code, the rebuild cost of H l is O(\u03b2 \u2022 2 l ) (i.e., linear in the length of H l ) since the algorithm mix populates H l by combining two arrays(see  and ). The l-th level is rebuilt after 2 l writes. Therefore, the amortized cost for rebuilding is O(\u03b2 log n) per write operation. Each rebuild of the buffer C (discussed in Section 4.1) is followed by making all levels of H empty.Structure of Buffer C Unlike the buffer U (and H), no read or write operations are performed directly on the buffer C. After n write operations, the buffer U is encoded using an erasure code to form a new copy of C, and the existing copy of C is replaced by this new one. The rebuild of C can be done using the same FFT-based code discussed in Section 4.1 which costs O(\u03b2n log n) both in time and bandwidth. As C is rebuilt after every n write operations, the cost incurred per write is O(\u03b2 log n). We note that C contains stale data between two successive rebuilds. However, the intermediate writes are accommodated in H with appropriate encoding. Thus, these blocks written between two successive rebuilds of C are also retrievable at any point of time.Rebuild algorithm for X l to accommodate B in H Input: Already full levels X0, X1, . . . , X l\u22121 and empty X l . Output: Empty levels X0, X1, . . . , X l\u22121 and rebuilt X l . . Rebuild algorithm for X l .Algorithm mix(A0, A1, l)\u2022 Let \u03c9 l = \u03c9 2n/2 l+1 be the 2 l+1 -th primitive root of unity modulop\u2022 Output A . Algorithm mix for two arrays A0 and A1. Storage Structure for Authentication Tags Corresponding to Data BlocksWe note that each data block in U, H and C is of size \u03b2 = \u230alogp\u230b bits for a large primep = \u03b1 \u2022 (2n) + 1 for some \u03b1 \u2208 N. Thus, the size of a data block \u03b2 \u226b \u03bb, where \u03bb is the security parameter. For example, \u03b2 is taken to be 64 KB and \u03bb is taken to be 128 bits in our scheme (see  in Section 6). In addition to the log structure H and the buffer C, two similar structuresH andC for authentication tags corresponding to the blocks in H and C (respectively) are stored on the cloud server. Thus, the server stores U, H, C,H andC. The benefits of storingH andC on the server are as follows. Let us assume that the authentication tags for data blocks have the following properties.1. The size of a tag is much less than that of a block. 2. The tags are homomorphic in the sense that, given the tags of two blocks B 1 and B 2 , the tag on any linear combination of B 1 and B 2 can be generated.We note that the fundamental operation for a write (or rebuild) on H and C is to encode data blocks, that is, to compute a linear combination of those data blocks (see Eqn. 1 and Eqn. 2 in ). Due the second property mentioned above, while the server itself performs write operations on H and C, the client (data owner) can perform similar operations onH andC. On the other hand, due to the first property, the bandwidth required between the client and the server for a write operation decreases significantly as the client now has to download much smaller tags instead of large data blocks. The cost of storage is less nowadays, whereas bandwidth is more expensive and often limited. Therefore, it is reasonable if we trade the storage off to reduce the communication bandwidth between the client and the server required for a write (or rebuild). Indeed, the authentication tags (described in the following section) we use in our dynamic POR scheme satisfy the following properties.-The size of a tag is O(\u03bb) and is independent of the size of a data block \u03b2, where \u03bb \u226a \u03b2.-The homomorphic property is achieved by using a collision-resistant homomorphic hash function. Apart from efficient write operations, the cost of an audit in our publicly verifiable dynamic POR scheme is comparable to that in the privately verifiable scheme of , and it is much less than that in the publicly verifiable scheme discussed in the same work. Generation of Authentication Tags SetupFor the data file identified by fid, the client runs an algorithm Setup(1 \u03bb ) to set parameters for generating authentication tags. The algorithm Setup selects two random primes p and q such that |q| = \u03bb q = 2\u03bb + 1, |p| = \u03bb p = O(\u03bb) and q|(p \u2212 1). Now, it divides each block B of the data file into segments of size (\u03bb q \u2212 1) bits each. This ensures that each segment is less than q and can therefore be represented as an element of Z q . Thus, m = \u2308\u03b2/(\u03bb q \u2212 1)\u2309 is the number of segments in a block, where a block is \u03b2 = \u230alogp\u230b bits long. In this setting, each block B can be represented as a vector [b 1 , b 2 , . . . , b m ] \u2208 Z m q . Let G q be a subgroup of Z * p having order q. That is, G q consists of the order q elements in Z p . Then, m random elements g 1 , g 2 , . . . , g m R \u2190 \u2212 G q are selected. Let S = (KeyGen, Sign, Verify) be a digital signature scheme (see Section 2.4) where the algorithm Sign takes messages in {0, 1} * as input and outputs signatures of size O(\u03bb) bits each. Let the pair of signing and verification keys for S be (ssk, psk). The Setup algorithm outputs the primes p and q, the secret key SK = ssk, the public parameters P K = (g 1 , g 2 , . . . , g m , fid, psk), and the descriptions of G q and S. Format of a Tag The client computes the homomorphic hash [30,5] on a blockUsing the signature scheme S, the client generates the final authentication tag for the block B aswhere addr is the physical address B is written to (at time t) and fid is the file-identifier of the data file the block B belongs to.Collision-resistance and Homomorphic Properties As shown in , given that the discrete logarithm assumption (see Section 2.5) holds in G q , it is computationally hard to find two blocks B 1 and B 2 such that B 1 = B 2 and h(B 1 ) = h(B 2 ) (collision-resistance property). Therefore, h(B) can be computed asSize of a Tag The size of an authentication tagh(B) is the sum of |h(B)| (which is \u03bb p = O(\u03bb) bits) and the size of a signature in S. If we use the standard ECDSA signature scheme  as S, then a signature is 4\u03bb bits long 2 . Thus, |h(B)| is also O(\u03bb) bits. For the values of different parameters considered in our scheme (see  in Section 6), the size of a tag is only 192 bytes which is very small compared to the size of a block (64 KB). Improvement in Cost of Tag Computationwhich requires only one exponentiation modulo p along with m multiplications and (m \u2212 1) additions modulo q. This is a huge improvement in the cost for computing an authentication tag. On the other hand, the storage overhead at the client's side is |\u0393 | which is same as the size of a single block B.Considering the fact that the client outsources millions of blocks to the cloud server, this amount of client storage is reasonable for all practical purposes.Storage Structure forH andC The storage structures forH andC are exactly the same as those for H and C, respectively, except that the authentication tags (instead of data blocks) are stored inH andC (see Section 4.1 for structures of H and C). OperationsThere are three types of operations involved in a dynamic POR scheme. The client can read, write and audit her data stored on the cloud server. The read and write operations are authenticated in that the client can verify the authenticity of these operations. We note that though the client herself performs reads and writes on her data, she can delegate the auditing task to a third party auditor (TPA) for a publicly verifiable scheme. As our scheme is publicly verifiable, we use the term verifier to denote an auditor who can be a TPA or the client herself.  gives an overview of the communication flow between the client and the server during these operations.Read Reads are performed directly from the unencoded buffer U. The authenticity and freshness of the block read can be guaranteed by using a Merkle hash tree  (or a similar data structure like rank-based authenticated skip list ) over the blocks of U. That is, the blocks of U constitute the leaf-nodes of the Merkle hash tree (see Section 2.3 for a brief description of a Merkle hash tree). The server sends the Merkle proof \u03a0 read containing the requested block and the labels of the nodes along the associated path of the tree to the client. The client maintains the up-to-date value of the root-digest of the Merkle hash tree digM HT and verifies the proof \u03a0 read with respect to this root-digest. We note that the size of the root-digest of the Merkle hash tree is O(\u03bb) bits. In the setup phase, the client sets parameters for the scheme and outsources the preprocessed file to the server. Initially, the client uploads U, C andC. The server stores them along with H andH that are initialized to be empty. Then, the client can perform reads, writes and audits on her data in an interleaved fashion. We note that, during a write, the server itself rebuilds H and C (if necessary). On the other hand, the client rebuildsH andC by downloading some of the authentication tags from them, computing the tag on the new block (using homomorphic property of tags) and sending the new tag to the server. As our scheme is publicly verifiable, audits can be performed by a third party auditor (TPA) as well.Write Let updtype denote the type of a write operation which can be insertion of a new block after the i-th block, deletion of the i-th block or modification of the i-th block. A write operation affects the buffers in the following way.-Write on U As the buffer U is unencoded, a write operation on U can be performed in a similar way as done on the data blocks in a dynamic provable data possession (PDP) scheme . We briefly describe the procedure as follows. Let digM HT be the root-digest of the current Merkle hash tree which is stored at the client's side. The client performs an authenticated read on the i-th data block of U (as described above  . Similarly, the corresponding operation for the rebuild ofH is to computeh(B) givenh(B 1 ) and h (B 2 ). For i = 1, 2, the client first downloadsh(B i ) and verifies the signature on h(B i ) by checking whetherwhere psk is the verification key for the signature scheme S. We note that addr 1 (or addr 2 ) is the physical address of the block B 1 (or B 2 ) written at time t 1 (or t 2 ), and fid is the file-identifier of the data file the block B belongs to. For any block in H and C, the time when the block was written most recently can be easily computed from the current time itself. If the verification passes, the client computes h(B) = h(B 1 ) \u03b11 \u2022 h(B 2 ) \u03b12 andh(B) subsequently. This requires two exponentiations and one multiplication modulo p along with one Sign and two Verify operations. -Write on C andC As mentioned in Section 4.1, C (C for authentication tags) is rebuilt after every n writes. The server performs a rebuild on C, whereas a rebuild onC is performed by the client. Basic operations involved in rebuilds of C andC are the same as those for rebuilds of H and H, respectively, and thus are omitted here.Audit In the challenge phase, the verifier chooses r = O(\u03bb log n) random locations {addr i } 1 i r from all levels (where O(\u03bb) random locations are selected from each level) of H and C. Then, she sends a challenge set Q = {(\u03bd i , addr i )} 1 i r to the cloud server, where \u03bd 1 , \u03bd 2 , . . . , \u03bd r R \u2190 \u2212 Z * q are random coefficients. In the response phase, the server sends to the verifier a proof containing B * = 1 i r \u03bd i B addr i and {h(B addr i )} 1 i r . Upon receiving the proof from the server, the verifier verifies each of the signatures on {h(B addri )} 1 i r . Then, she computes\u03bdi and h(B * ) using Eqn. 3. Finally, the verifier checks whetherand outputs 0 if any of the verifications fails; she outputs 1, otherwise. SecurityWe define security of a dynamic POR scheme  and show that our scheme described in Section 4 is secure according to this definition. We also show that the server cannot pass an audit without storing all data blocks properly, except with some probability negligible in \u03bb. Overview of Security of a Dynamic POR SchemeA dynamic POR scheme must satisfy the following properties . The formal security definition is given in Section 5.2. Authenticity and FreshnessThe authenticity property requires that the cloud server cannot produce valid proofs during audits without storing the corresponding blocks and their respective authentication information untampered, except with a probability negligible in \u03bb. For dynamic data, the client can modify an existing data block. However, a malicious cloud server may discard this change and keep an old copy of the block. As the old copy of the block and its corresponding tag constitute a valid pair, the client has no way to detect if the cloud server is storing the fresh (latest) copy. Thus, the client must be convinced that the server has stored the up-to-date blocks. 2. Retrievability Retrievability of data requires that, given a probabilistic polynomial-time adversary A that can respond correctly to a challenge Q with some non-negligible probability, there exists a polynomial-time extractor algorithm E that can extract all data blocks of the file (except with negligible probability) by challenging A for a polynomial (in \u03bb) number of times and verifying the responses sent by A. The algorithm E has a black-box rewinding access to A. Authenticity and freshness of data restrict the adversary A to produce valid responses (without storing the data in an authentic and up-to-date fashion) during these interactions only with some probability negligible in the security parameter \u03bb. Security ModelWe first describe the following security game between the challenger (acting as the client) and the adversary (acting as the cloud server).-The adversary selects a file F associated with a file-identifier fid to store. The challenger processes the file to form another file F \u2032 and returns F \u2032 to the adversary. The challenger stores only some metadata for verification purpose. -The adversary adaptively chooses a sequence of operations defined by {op i } 1 i q1 (q 1 is polynomial in the security parameter \u03bb), where op i is a read, a write or an audit. The challenger executes these operations on the file stored by the adversary. For each operation, the challenger verifies the response sent by the adversary and updates the metadata at her end only if the response passes the verification. -Let F * be the final state of the file after q 1 operations. The challenger has the latest metadata for the file F * . Now, she executes an audit protocol with the adversary. The challenger sends a random challenge set Q to the adversary, and the adversary returns a cryptographic proof to the challenger.The adversary wins the game if it passes the verification.Definition 3 (Security of a Dynamic POR Scheme). A dynamic POR scheme is secure if, given any probabilistic polynomial-time adversary A who can win the security game mentioned above with some non-negligible probability, there exists a polynomial-time extractor algorithm E that can extract all data blocks of the file by interacting (via challenge-response) with A polynomially many times. Security Analysis of Our SchemeWe state and prove the following theorem in order to analyze the security of our dynamic POR scheme.Theorem 1. Given that the discrete logarithm assumption holds in G q and the underlying digital signature scheme is secure, the dynamic POR scheme described in Section 4 is secure according to Definition 3.Proof. We use the following claim in order to prove Theorem 1.Claim. Given that the discrete logarithm assumption holds in G q and the underlying digital signature scheme is secure, authenticity and freshness of the challenged blocks in H and C are guaranteed.Proof. We prove the above claim for the log structure H. The proof for C follows in a similar way. In our scheme, every block B (of the file identified by fid) in H corresponds to an authentication tag h(B) = (h(B), Sign ssk (h(B), fid, addr, t)) present inH, where the signing algorithm Sign uses the secret key ssk of the client and t is the last write-time of the block B. Let B be the correct block that was actually written by the client to the address addr at time t. Suppose this block in addr is challenged during an audit. We note that the last write-time t of the block is computable from addr and the current time. So, the values of fid, addr and t are known to the challenger. Therefore, in order to break the authenticity of the scheme, the PPT adversary A has to find a block B \u2032 = B and its tagh(B \u2032 ) such that one of the following conditions holds:, -Case II: h(B \u2032 ) = h(B). Case IWe show that, if the adversary A can find a block B \u2032 = B and its authentication tagh(B \u2032 ) such that h(B \u2032 ) = h(B) andh(B \u2032 ) = (h(B \u2032 ), Sign ssk (h(B \u2032 ), fid, addr, t)), then it can break the security of the underlying signature scheme (the security of a digital signature scheme is discussed in Section 2.4).Let the adversary A be provided with a set of polynomially many authentication tags {h(B i ) = (h(B i ), Sign ssk (h(B i ), fid, addr i , t i ))} i\u2208I for {(B i , addr i , t i )} i\u2208I of A's choice (where I =  for some k polynomial in \u03bb). Let us assume that the adversary A is able to find another block B \u2032 and its tag h(B \u2032 ) = (h(B \u2032 ), Sign ssk (h(B \u2032 ), fid, addr j , t j )), such that j \u2208 I, B \u2032 = B j and h(B \u2032 ) = h(B j ). Then, we can construct another probabilistic polynomial-time (PPT) algorithm B O ssk (\u2022) that, given the public key psk and an access to the signing oracle O ssk (\u2022), executes A as a subroutine. Initially, B provides the public parameters (g 1 , g 2 , . . . , g m , fid, psk) and the description of G q to A. With the help of O ssk (\u2022), B responds to A's queries with {h(B i ) = (h(B i ), Sign ssk (h(B i ), fid, addr i , t i ))} i\u2208I . Now, if A finds another block B \u2032 and its tagh(B \u2032 ) = (h(B \u2032 ), Sign ssk (h(B \u2032 ), fid, addr j , t j )) as described above with probability \u01eb A in (polynomial) time t \u2032 A , then B also finds a forged signature Sign ssk (h(B \u2032 ), fid, addr j , t j ) (that was not queried to the signing oracle before) with probability \u01eb B = \u01eb A in time t \u2032 B \u2248 t \u2032 A . Case IIWe show that, if the adversary A can find a block B \u2032 = B and its authentication tag h(B \u2032 ) = (h(B \u2032 ), Sign ssk (h(B \u2032 ), fid, addr, t)) such that h(B \u2032 ) = h(B), then it can solve the discrete logarithm problem over G q (we refer to  for the detailed proof showing that the collision-resistance property holds for h).The idea of the proof is as follows. Let us assume that the adversary A, given the description of the multiplicative group G q = g and m random elements g 1 , g 2 , . . . , g m of G q , is able to find two blocks B, B \u2032 \u2208 Z m q such that B = B \u2032 and h(B) = h(B \u2032 ). Then, we can construct another probabilistic polynomial-time (PPT) algorithm B that, given the description of G q and y \u2208 G q , executes A as a subroutine to find a collision and uses this collision to compute x \u2208 Z q such that y = g x . In orderui if z i = 0; it sets g i = y ui if z i = 1. Then, B provides A with the description of G q = g and the elements g 1 , g 2 , . . . , g m \u2208 G q computed in the previous step. Now, suppose A finds two blocksi ) mod q and computes a \u2032 = a \u22121 mod q (a is non-zero with probability at least  2 ). Since h(B) = h(B \u2032 ), we havewheremod q is the discrete logarithm of y in G q . Thus, the algorithm B solves the discrete logarithm problem over G q with probability \u01eb B \u01ebA 2 in (polynomial) time t \u2032 B = t \u2032 A + O(m\u03bb 3 ). The overhead term O(m\u03bb 3 ) is attributed to some arithmetic operations (including m exponentiation operations) that B has to perform.Given an address addr, let B be the latest block that was actually written by the client to addr at time t. Let the challenger challenge the block in addr during an audit. In order to retain an older block B \u2032 = B (written to the same address addr at time t \u2032 < t) and still pass the audit, the adversary A has to produce its authentication tag for time t (we note that the tag for B \u2032 for time t \u2032 is available to A) such that one of the conditions mentioned above (Case I and Case II) holds. As we have seen earlier, it is computationally hard to find such a block B \u2032 , except with a probability negligible in \u03bb. Thus, the adversary must store each of the challenged blocks with its latest content to pass the audit.We define a polynomial-time extractor algorithm E that can extract all blocks from each of the levels of H and C (except with negligible probability) by interacting with an adversary A that wins the security game described in Section 5.2 with some non-negligible probability. As our dynamic POR scheme satisfies the authenticity and freshness properties mentioned above, the adversary A cannot produce a valid proof (B * = 1 i r \u03bd i B addr i , {h(B addr i )} 1 i r ) for a given challenge set Q = {(\u03bd i , addr i )} 1 i r without storing the challenged blocks and their corresponding tags properly, except with some negligible probability (see Section 4.3 and Claim 5.3). This means that if the verifier outputs 1 during the extraction phase, B * in the proof is indeed the linear combination of the untampered blocks {B addr i } 1 i r using coefficients {\u03bd i } 1 i r .Suppose that the extractor E wants to extract r blocks indexed by J. It challenges A with a challenge set Q = {(\u03bd i , addr i )} i\u2208J . If the proof is valid (that is, the verifier outputs 1), E initializes a matrix M E as [\u03bd 1i ] i\u2208J , where \u03bd 1i = \u03bd i for each i \u2208 J. The extractor challenges A for the same J but with different random coefficients. If the verifier outputs 1 and the vector of coefficients is linearly independent to the existing rows of M E , then E appends this vector to M E as a row. The extractor E runs this procedure until the matrix M E has r linearly independent rows. So, the final form of the full-rank matrix M E is [\u03bd ji ] j\u2208 ,i\u2208J . Consequently, the challenged blocks can be extracted using Gaussian elimination.Following the way mentioned above, the extractor algorithm E can interact with A (polynomially many times) in order to extract \u03c1-fraction of blocks (for some \u03c1) for each level of H and C by setting the index set J appropriately. Use of a \u03c1-rate erasure code ensures retrievability of all blocks of C (i.e., all the encoded blocks of U up to the last rebuild of C) and H (i.e., all the encoded blocks of U written after the last rebuild of C). For each l-th level of H (or C), the FFT-based code used in our scheme is a (2 l+1 , 2 l , 2 l )-erasure code; thus, \u03c1 = 1 2 . This completes the proof of Theorem 1. Probabilistic GuaranteesAs we mention in Section 4.3, each of the levels of H and the buffer C is audited with O(\u03bb) random locations. Due to the use of a (2 l+1 , 2 l , 2 l )-erasure code for each level 0 l \u230alog n\u230b, the server has to actually delete half of the blocks in a level in order to delete a single block in that level. Thus, if the server corrupts half of the blocks in any level, then it passes an audit with probability Performance AnalysisWe analyze the performance of the following types of operations (described in Section 4.3) involved in our publicly verifiable dynamic POR scheme.-Read For an authenticated read on the data block B present in U, the server sends the corresponding Merkle proof \u03a0 read which consists of the block B, the data block in the sibling leaf-node of B and the hash values along the associated path of the Merkle hash tree (see Section 2.3). Thus, a read operation takes 2\u03b2 + O(\u03bb log n) communication bandwidth between the client and the server. To reduce this cost, the client can generate authentication tags on the data blocks of U (as discussed in Section 4.2) and construct a Merkle tree over these tags instead of the data blocks. In this setting, \u03a0 read consists ofh(B), the authentication tag in its sibling leaf-node and the hash values along the associated path. This reduces the communication bandwidth between the client and the server for a read to \u03b2 + O(\u03bb log n). -Write A write operation incurs the following costs.\u2022 Write on U : A write operation on U involves an authenticated read operation followed by the verification of Eqn. 5. Thus, each write operation requires \u03b2 + O(\u03bb log n) bandwidth between the client and the server (for communicating \u03a0 read and digM HT server ).   We take \u03bb as the security parameter and n as the number of blocks (each \u03b2-bits long) of the data file to be outsourced to the server. For all of the schemes mentioned above, the storage on the server side is O(\u03b2n), where \u03b2 \u226b \u03bb. The cost of an authenticated read operation is \u03b2 + O(\u03bb log n) if a Merkle hash tree is maintained over the unencoded data blocks for checking authenticity and freshness. \u2020 \u01eb is a constant such that \u01eb > 0.-Audit For a challenge set Q containing r = O(\u03bb log n) random locations {addr i } 1 i r and random coefficients \u03bd 1 , \u03bd 2 , . . . , \u03bd r \u2208 Z * q , the server computes a proof containing B * = 1 i r \u03bd i B addr i and {h(B addr i )} 1 i r and sends the proof to the verifier. Thus, the bandwidth required for an audit is given by \u03b2 + O(\u03bb 2 log n).Comparison among Dynamic POR Schemes We compare our scheme with other existing dynamic proofs-of-retrievability (POR) schemes which is summarized in . The comparison is based on the asymptotic complexity for different parameters. Some of the figures mentioned in  are taken from .  mentions typical values of the parameters used in our scheme .From , we note that, in our publicly verifiable dynamic POR scheme, bandwidths required for a write and an audit are given by \u03b2 + O(\u03bb log n) and \u03b2 + O(\u03bb 2 log n), respectively. These figures are asymptotically the same as those in the privately verifiable scheme of . On the other hand, this is a significant improvement over the publicly verifiable scheme of  where bandwidths required for a write and an audit are \u03b2(1 + \u01eb) + O(\u03bb log n) and O(\u03b2\u03bb log n), respectively, for a constant \u01eb > 0 and \u03b2 \u226b \u03bb.Additionally, one drawback of the publicly verifiable scheme proposed by Shi et al.  is due to the fact that one or more Merkle hash trees (separate from the Merkle hash tree 3 maintained for U) are maintained to ensure the integrity of the blocks in the hierarchical log H (one for the entire log or one for each of its levels). To enable a third party auditor (TPA) to audit the data blocks residing at different levels of this log, the root-digests of these trees need to be made public. However, some of these root-digests are changed as often as new data blocks are inserted in the hierarchical log structure, thus resulting in a change in the public parameters for each write. This incurs an additional (non-trivial) overhead for validation and certification of the public parameters for every write operation. On the other hand, the public parameters in our publicly verifiable dynamic POR scheme are fixed throughout the execution of the protocols involved.Apart from the schemes listed in , we mention some POR schemes proposed recently that handle data dynamics as follows. The dynamic POR scheme proposed by Guan et al.  uses the notion of indistinguishability obfuscation (iO)  to construct a publicly verifiable POR scheme from the privately verifiable scheme of Shacham and Waters . It also handles dynamic data using a \"modified B+ Merkle tree\". However, the iO candidates available in the literature are not currently practical. Ren   propose a dynamic POR scheme where the data file is encoded using erasure coding (intraserver encoding) and network coding (inter-server encoding). The encoded blocks are then disseminated among multiple storage servers. Use of network coding reduces the communication bandwidth required for a repair in case of a node (server) failure. For the intra-server encoding, each block is divided into some sub-blocks (using an erasure code), and a \"range-based 2-3 tree\" (rb23Tree) is built upon these sub-blocks for each server. This ensures the authenticity and freshness properties of the blocks within a server. We note that each block is encoded (locally) into a few number of sub-blocks for the intra-server encoding. Therefore, an update in a block (or in any of its sub-blocks) requires updating only a few sub-blocks corresponding to that block. This makes an update in this scheme efficient. On the other hand, a malicious server needs to delete only a few sub-blocks to actually make a block unavailable. Thus, the dynamic POR scheme proposed by Ren et al.  differs from our scheme on the basis of the granularity of data the client needs. ConclusionIn this work, we have proposed a dynamic POR scheme where the client can update her data file after the initial outsourcing of the file to the cloud server and retrieve all of her data at any point of time. Our scheme is publicly verifiable, that is, anyone having the knowledge of the public parameters of the scheme can perform an audit on the client's behalf, and it offers security guarantees of a dynamic POR scheme. This scheme is more efficient (in terms of the cost of a write or an audit) than other practical and publicly verifiable dynamic POR schemes with a similar data granularity.\n###\n"}
{"text": "#Properties\nimplementation, dataset, has research problem, evaluation, Task, Test questions, Train questions, Question language, Language, On, Question analysis task, Phrase mapping task, Disambiguation task, Query construction task\n#Text\nWe present a question answering system (CASIA@V2) over Linked Data (DBpedia), which translates natural language questions into structured queries automatically. Existing systems usually adopt a pipeline framework, which contains four major steps: 1) Decomposing the question and detecting candidate phrases; 2) mapping the detected phrases into semantic items of Linked Data; 3) grouping the mapped semantic items into semantic triples; and 4) generating the rightful SPARQL query. We present a jointly learning framework using Markov Logic Network(MLN) for phrase detection, phrases mapping to semantic items and semantic items grouping. We formulate the knowledge for resolving the ambiguities in three steps of QALD as first-order logic clauses in a MLN. We evaluate our approach on QALD-4 test dataset and achieve an F-measure score of 0.36, an average precision of 0.32 and an average recall of 0.40 over 50 questions. System DescriptionThe current version of our QA system is not designed to answer the questions which contain numbers, date comparisons and aggregation operations such as group by or order by. We also do not consider the questions which contain filter condition. 1 shows the architecture of our system to translate a question into a formal SPARQL query.At first, sequence of tokens(phrase) are detected that probably indicate semantic items, such as software, developed by and California. This step detects full potentially phrases, and leaves the decision for phrases selecting in later steps.Next, the phrases are mapping to semantic items. Phrases can denote entities, classes and properties. For example, the phrase software can denote class dbo:Software and property dbo:developer, and the phrase developed by can denote entity dbr:videogamedeveloper, class dbo:BritishRoyalty and property dbo:developer. This step purely constructs a candidate space for every possible mapping, and leaves the decision for select which semantic items in the next step.Then, we should make the decisions for choosing phrases, mapping the chosen phrases to suitable semantic items and determine the relations of selected semantic item. We formulate the joint decisions as an generalized inference task. We employ rich features and constraints (including hard and soft constraints) to infer the joint decisions using a MLN.Finally, with the inference results, the last step constructs a semantic item query graph, and generates an executable SPARQL query with the question type.We will give a detailed description of each component and give a step by step explanation with the following example, shows the intermediate results of every steps:Which software has been developed by organizations founded in California, USA?. System Pipleline1) Phrase detection. Sequences of tokens (phrases) that probably indicate semantic items are detected. To this end, we do not use named entity recognizer (NER) because of its low coverage. To avoid missing useful phrases, we retain all n-grams as candidate phrases, and then use some rules to filter them. The rules include: the length of tokens span must be less than 4 (excepting all contiguous tokens are capitalizations); the POS tag of the start token must be jj, nn, rb and vb; all contiguous capitalization tokens must not be split, and so on. For instance, software, developed by, organizations, founded in and California are detected in the example.2) Mapping phrase to semantic item. After phrases are detected, each phrase may be mapped to the semantic items in KB (entities, classes and properties). For example, software is mapped to dbo:Software, dbo:developer, etc.; California is mapped to dbr:California, dbr:California (wine), etc. We use different techniques and resources to map phrases to different types of semantic items. For mapping phrases to entities, considering the entities in DBpeida are curated from Wikipeida, we employ anchors, redirections and disambiguations information from Wikipedia. For mapping phrases to classes, considering that classes have lexical variation, especially synonyms, e.g., dbo:Film could be mapped from film, movie and show, we use word2vec tool to convert every word (phrase) into a vector and compute the similarity between the phrase and the class in KB. The similarity scoring methods are introduced in Section 3.2. Then, the top-N most similar classes for each phrase are returned. For mapping phrases to properties, we employ the resources from PATTY and ReVerb . Specifically, we first compute the associations between the semantic properties in DBpedia and relation patterns in PATTY and ReVerb through instance alignments as same as . Next, if a detected phrase is matched to some relation pattern, the corresponding properties in DBpedia will be returned as the candidates. This step purely constructs a candidate space for every possible mapping, and the decision of selecting the best fitting semantic item is made in the next step.3) Feature extraction and joint inference. There are ambiguities in phrase detection and mapping-phrase-to-semantic-item. This step consists in the resolution of these ambiguities and determine the relations among the mapped semantic items. It is the core contribution of this paper, which performs disambiguation in a unified manner. First, feature extraction is performed to prepare rich features from the question and the knowledge base. Next, the disambiguation is performed in a joint fashion with a Markov Logic Network (MLN). The detailed information will be presented in the next Section. 4) SPARQL generation. Based on the inference results, we construct a query graph. The vertex contains the following information: the phrase, token span indexs of the phrase, the mapped semantic item and its type. The edge indicates the relation between two semantic items. For example, we use 1 2 to indicate that the first argument of an item matches the second argument of another item . The right bottom in shows an example of it. The relation in the query graph is paired data merely, but the SPARQL queries need the grouped triples of semantic items. Thus, we convert a query graph into multiple joined semantic triples. Three interconnected semantic items, which must ensure the middle item is a property, are converted into a semantic triple. For example, the query graph dbo:Book Feature extraction & joint inferenceIn this section, we will first briefly describe Markov Logic Networks. Then, we present the predicates(features) and the first-order logic formulas for joint inference. Markov Logic NetworksMarkov\n###\n", "summary": " implementation: CASIA\n, has research problem: Question answering systems\n, Question analysis task: Dependency parser/NE n-gram strategy/POS learned\n, Phrase mapping task: Distributional Semantics/Knowledge base labels/Redirects/Using extracted knowledge\n, Disambiguation task: Local disambiguation/MLN\n, Query construction task: Using machine learning\n###"}
{"text": "#Properties\nHas value, Time period, has beginning, has end, R0 estimates (average), data source, Lower confidence limit, Upper confidence limit, Cumulative Incidence (CI), data1 description, Ascertainment Rate (q), Number of confirmed cases, Intrinsic growth rate, Scaling of growth parameter, Number of deaths, Confidence interval (95%), Case fatality rate, total cases worldwide, has research problem, location, has unit, same as\n#Text\nBy 27 February 2020, the outbreak of coronavirus disease 2019 (COVID-19) caused 82 623 confirmed cases and 2858 deaths globally, more than severe acute respiratory syndrome (SARS) (8273 cases, 775 deaths) and Middle East respiratory syndrome (MERS) (1139 cases, 431 deaths) caused in 2003 and 2013, respectively. COVID-19 has spread to 46 countries internationally. Total fatality rate of COVID-19 is estimated at 3.46% by far based on published data from the Chinese Center for Disease Control and Prevention (China CDC). Average incubation period of COVID-19 is around 6.4 days, ranges from 0 to 24 days. The basic reproductive number (R 0) of COVID-19 ranges from 2 to 3.5 at the early phase regardless of different prediction models, which is higher than SARS and MERS. A study from China CDC showed majority of patients (80.9%) were considered asymptomatic or mild pneumonia but released large amounts of viruses at the early phase of infection, which posed enormous challenges for containing the spread of COVID-19. Nosocomial transmission was another severe problem. A total of 3019 health workers were infected by 12 February 2020, which accounted for 3.83% of total number of infections, and extremely burdened the health system, especially in Wuhan. Limited epidemiological and clinical data suggest that the disease spectrum of COVID-19 may differ from SARS or MERS. We summarize latest literatures on genetic, epidemiological, and clinical features of COVID-19 in comparison to SARS and MERS and emphasize special measures on diagnosis and potential interventions. This review will improve our understanding of the unique features of COVID-19 and enhance our control measures in the future. and , COVID-19 has caused 82 623 confirmed cases and 2858 deaths globally. The total case-fatality rate is 3.46% as shown in . Because COVID-19 started from Wuhan, the capital city of Hubei province with a large population of nearly 14 million people, 58.3% cases are in Wuhan. A total of 1932 health workers have been infected in Wuhan alone, which overwhelmed the local health system and resulted in the highest case-fatality rate (4.42%). Excluding Hubei province, the rest of China has 13 045 cases, 109 fatalities (0.84%). Outside of China, COVID-19 has spread to 46 countries and has caused 3664 infections and 67 fatalities (1.83%). Overall, the case-fatality rate of COVID-19 so far is much lower than either SARS (9.6%) or MERS (34.5%). Here, we summarized common and discrete features of SARS-CoV-2 in comparison to its two predecessors (SARS-CoV and MERS-CoV) in genetics, epidemiology, clinical features, and further discussed challenges for diagnosis and special control measures for COVID-19. | GENETIC BIOLOGY OF SARS-COV-2Full-length genome sequences of SARS-CoV-2 were obtained from early infected individuals related to a wild animal market in Wuhan by different research groups through next-generation sequencing. Full genomic length of this novel coronavirus ranges from 29 891 to 29 903 nucleotides (nt). All viral genome sequences obtained are extremely similar, showing more than 99.98% sequence identity. SARS-CoV-2 is 96.2% identical at the whole-genome level to a bat coronavirus isolate RaTG13 (Global initiative on sharing all influenza data [GISAID] accession no. EPI_ISL_402131) collected from Yunnan province, China, and is 88% identical to two bat-derived SARS-like coronaviruses, bat-SL-CoVZC45 and bat-SL-CoVZXC21, collected in 2018 in Zhoushan, Eastern China. The close phylogenetic relationship to RaTG13 suggests bats are probably natural hosts for SARS-CoV-2. Human SARS-CoV-2 have a unique RRAR motif in the spike protein which is not found in coronaviruses isolated from pangolins, suggesting SARS-CoV-2 may not come directly from pangolins. An evolutionary study based on 86 genomic sequences from GISAID ( showed three deletions were found in isolates from Japan, USA, and Australia, 93 mutations found over the entire genomes. Of note, eight mutations were found in the spike surface glycoprotein, especially three mutations (D 354 , Y 364 , and F 367 ) located in the spike surface glycoprotein receptor-binding domain (RBD), which suggested SARS-CoV-2 may rapidly evolve to evade immune response and adapt to other hosts in the future. SARS-CoV-2 share 79% nt sequence identity to SARS-CoV and around 50% to MERS-CoV. However, the seven conserved replicase domains in ORF1ab (used for CoV species classification) of SARS-CoV-2 are 94.6% identical to SARS-CoV, implying the two belong to same species. The receptor-binding protein spike (S) gene of SARS-CoV-2 is highly divergent to all previously described SARSr-CoVs with less than 75% nt sequence identity to except a 93.1% nt identity to RaTG13. Homology modeling revealed SARS-CoV-2 had a similar RBD structure to that of SARS-CoV. Further study showed SARS-CoV-2 uses the same cell entry receptor, ACE2, as SARS-CoV, not CD26 as MERS-CoV. Structural analysis by cryo-electron microscopy revealed SARS-CoVs protein binds ACE2 with 10 to 20 folds higher affinity than SARS-CoV, which suggests that SARS-CoV-2 may be more infectious to human than SARS-CoV. | EPIDEMIOLOGY OF COVID-19Transmission of infectious diseases must rely on three conditions: sources of infection, routes of transmission, and susceptible hosts. Virus transmission mode Person-to-person transmission through droplets, opportunistic airborne transmission, nosocomial transmission, sporadic zoonotic transmission, aerosol transmission, and fecal-oral transmission. Respiratory transmission, sporadic zoonotic transmission, nosocomial transmission, via aerosols, and limited human-to-human transmission. Person-to-person transmission through respiratory droplets, contact and fomites, zoonotic transmission, nosocomial transmission, fecal-oral transmission, and aerosol transmission is highly possible. Median incubation period 4.6 d (95% CI, 3.8-5.8 d). 5.2 d (95% CI, 1.9-14.7 d). 6.4 d (range, 0-24.0 d). Case-fatality rate Worldwide (WHO): 9.6%, mainland China: 6.4%, and Hong Kong: 17%. Worldwide (WHO): 34.5% and South Korea: 20.4%. Wuhan, China: 3%. Abbreviations: CI, confidence interval; MERS-CoV, Middle East respiratory syndrome coronavirus; SARS-CoV, severe acute respiratory syndrome coronavirus; WHO, World Health Organization. and Germany 32 and other places. A study showed high viral loads were detected in upper respiratory specimens of patients with COVID-19, and viral shedding pattern of patients resembles that of patients with influenza. This suggests SARS-CoV-2 may stay around for some time like influenza viruses. | CLINICAL FEATURES OF COVID-19The full spectrum of disease severity as shown in the guidelines for diagnosis and treatments for COVID-19 Less common symptoms included muscle ache, confusion, headache, sore throat, rhinorrhoea,\n###\n", "summary": " Has value: 1.83\n, Time period: Time interval\n, has end: 2020-02-27\n, data source: China CDC\n, Number of confirmed cases: 3664\n, Number of deaths: 67\n, Case fatality rate: Case fatality rate estimate value specification\n, has research problem: COVID-19 case fatality rate\n, location: International (46 countries)\n, has unit: Percent\n###"}
{"text": "#Properties\nHas value, Global Mean Sea level Rise Projection, has lower limit for likely range, has upper limit for likely range, has  start of period, has end of period, climate scenario, has research problem, has unit\n#Text\nView the article online for updates and enhancements. Related content Carbon budgets and energy transition pathways Detlef P van Vuuren, Heleen van Soest, Keywan -Flood damage costs under the sea level rise with warming of 1.5 \u00b0C and 2 \u00b0C S Jevrejeva, L P Jackson, A MethodsSSP details are available via the public SSP database ( hosted by the International Institute for Applied Systems Analysis (IIASA). Global and regional projections are provided for greenhouse gas (GHG) emissions, energy mix, climate and land cover, demographic and socioeconomic parameters like population growth, gross domestic product (GDP), and consumption. . The SSP scenarios are defined until the year 2100. So-called marker scenarios have been proposed as the main representatives of the underlying respective socioeconomic storylines. These marker scenarios are derived by one IAM for each SSP. The non-marker scenarios are complementary realizations of each SSP storyline by the remaining IAMs. For both marker and non-marker realizations, individual SSPs are provided for the baseline, FT 2.6, FT 3.4, FT 4.5, and FT 6.0 cases described in the introduction. The resulting 105 quantified SSP scenarios are available from the SSP database and were used to force our climate and sea level model. By also using the non-marker scenarios for each SSP, we are able to cover parts of the IAM structural uncertainties underlying the individual pathways. For consistency, all SSP GHG emissions have been harmonized to 2010 RCP8.5 emission levels. Post-2100 extension pathways, as provided for the RCPs by (2011a), have not been defined yet. Please see the legend of figures 3 or 4 for the full list of SSP scenarios.In order to translate the full suite of SSP GHG emission data into a global climate and sea level signal, we apply the recently developed comprehensive sea level emulator by , which is directly coupled to the simple climate carbon-cycle model MAGICC version 6 (2011b). The sea level emulator is part of a group of simplified approaches that resolve the main sea level components . It is calibrated with IPCC AR5 consistent process-based SLR projections and provides global mean SLR estimates based on all major climate-driven contributions including thermal expansion, global glacier mass changes, and the surface mass balance and solid ice discharge of the Greenland and Antarctic ice sheets, as well as the non-climate-driven land water storage contribution. We have updated the Antarctic ice sheet (AIS) solid ice discharge (SID) component of the MAGICC sea level model to cover higher Antarctic sensitivity through hydrofracturing and subsequent ice cliff instability that substantially increases future SLR projections (DeConto and Pollard 2016). We present an AIS SID parameterization with a slow discharge term that depends quadratically on the global mean temperature deviation from a reference temperature and a fast discharge term that can be triggered by passing a threshold temperature. The parameterization is calibrated against AIS discharge projections provided by DeConto and Pollard (2016) that run from the year 1950-2500. The four free parameters are optimized by together minimizing the residual sum of squares under three RCP scenarios for which calibration data was available (RCP8.5, RCP4.5, RCP2.6). Please see the supplementary information available at stacks.iop.org/ERL/12/114002/mmedia for more details on the parameterization and calibration. Our main results incorporate SLR contributions based on the revised AIS SID parametrization, while the IPCC AR5 consistent SLR analysis using the original (2017) design is provided in the supplementary information for comparison. SLR projections are presented relative to 1986-2005 levels throughout the manuscript.For the projections, we have generated a probabilistic ensemble of 600 runs for every scenario with historically constrained parameter sets applying a Metropolis-Hastings Markov chain Monte Carlo approach . The parameter ensemble also reflects the IPCC AR5 equilibrium climate sensitivity estimates (2012, 2014). The probabilistic modeling framework consistently covers model and climate related uncertainties. For our projections, we follow the IPCC guidelines by adopting a likely range that reflects the 66%-100% probability of an outcome (2011).In order to facilitate the interpretation of the SSP SLR projections and to allow for a direct comparison to the RCP scenarios, we have also pooled the scenarios according to their FTs. Like this, we can clearly separate the effects of different climate mitigation levels from the effects of socioeconomic uncertainty in the non-mitigation baseline scenarios. Additionally, we have grouped the 2100 median SLR estimates of each SSP scenario according to individually defined categories for the SSP indicators shown in figures 3 and 4. The category ranges are chosen based on the scenario distribution for the individual SSP indicator. This visual aid is introduced to allow for a more nuanced analysis of the SLR implications of the selected SSP emission and socioeconomic indicators. We use boxplots with 90% range whiskers, the standard first and third quartiles (50% range) as boxes and the corresponding medians for the grouping of the individual 2100 SSP SLR medians. ResultsWe show the resulting projections of global mean temperature and global mean SLR for all SSP scenarios in figure 1. SLR varies strongly between non-mitigation baseline scenarios because the varying socioeconomic assumptions for the SSPs result in different emission and temperature outcomes (figure 2(a)). Median estimates for 2100 SLR across all SSP realizations range from 89 cm (likely range: 57-130 cm) for SSP1, 105 cm (73-150 cm) for SSP2, 105 cm (75-147 cm) for SSP3, 93 cm (63-133 cm) for SSP4, 132 cm (95-189 cm) for SSP5.Year-2100 SLR does not only depend on the end-ofcentury FT, but also on the pathway towards achieving this target. Nevertheless, SSP-FT SLR projections are dominated by the different FTs, with the SLR responses converging for each FT category, as opposed to the socioeconomic uncertainty driving the variation across the baseline scenarios (compare figures 2(b)-(e). 2100 median SLR is projected to be 52 cm (likely range: 34-75 cm) for the most ambitious climate mitigation efforts framed under FT 2.6, 62 cm (40-96 cm) under FT 3.4, 75 cm (47-113 cm) under FT 4.5, and 91 cm (61-132 cm) under FT 6.0. The highest individual likely SLR estimate for a scenario under a specific SSP-FT combination is 202\n###\n", "summary": " Has value: 0.55\n, Global Mean Sea level Rise Projection: Global Mean Sea Level Rise Projections\n, has lower limit for likely range: 0.45\n, has upper limit for likely range: 0.67\n, has  start of period: 1986-2005\n, has end of period: 2081-2100\n, climate scenario: RCP4.5\n, has research problem: Global Mean Sea Level Rise Projections\n, has unit: m\n###"}
{"summary": "has benchmark: Benchmark Atari 2600 Q*Bert/Benchmark Atari 2600 Frostbite/Benchmark Atari 2600 Montezuma's Revenge/Benchmark Atari 2600 Venture/Benchmark Atari 2600 Freeway\nhas research problem: Atari Games\nhas model: MP-EB\nsame as: https://en.wikipedia.org/wiki/Atari_Games", "text": "#Properties\nhas benchmark, has research problem, has model, same as\n#Text\nAchieving efficient and scalable exploration in complex domains poses a major challenge in reinforcement learning. While Bayesian and PAC-MDP approaches to the exploration problem offer strong formal guarantees, they are often impractical in higher dimensions due to their reliance on enumerating the state-action space. Hence, exploration in complex domains is often performed with simple epsilon-greedy methods. In this paper, we consider the challenging Atari games domain, which requires processing raw pixel inputs and delayed rewards. We evaluate several more sophisticated exploration strategies, including Thompson sampling and Boltzman exploration, and propose a new exploration method based on assigning exploration bonuses from a concurrently learned model of the system dynamics. By parameterizing our learned model with a neural network, we are able to develop a scalable and efficient approach to exploration bonuses that can be applied to tasks with complex, high-dimensional state spaces. In the Atari domain, our method provides the most consistent improvement across a range of games that pose a major challenge for prior methods. In addition to raw gamescores, we also develop an AUC-100 metric for the Atari Learning domain to evaluate the impact of exploration on this benchmark. PRELIMINARIESWe consider an infinite-horizon discounted Markov decision process (MDP), defined by the tuple (S, A, P, R, \u03c1 0 , \u03b3), where S is a finite set of states, A a finite set of actions, P : S \u00d7 A \u00d7 S \u2192 R the transition probability distribution, R : S \u2192 R the reward function, \u03c1 0 an initial state distribution, and \u03b3 \u2208 (0, 1) the discount factor. We are interested in finding a policy \u03c0 : S \u00d7 A \u2192 [0, 1] that maximizes the expected reward over all time. This maximization can be accomplished using a variety of reinforcement learning algorithms.In this work, we are concerned with online reinforcement learning wherein the algorithm receives a tuple (s t , a t , s t+1 , r t ) at each step. Here, s t \u2208 S is the previous state, a t \u2208 A is the previous action, s t+1 \u2208 S is the new state, and r t is the reward collected as a result of this transition. The reinforcement learning algorithm must use this tuple to update its policy and maximize longterm reward and then choose the new action a t+1 . It is often insufficient to simply choose the best action based on previous experience, since this strategy can quickly fall into a local optimum. Instead, the learning algorithm must perform exploration. Prior work has suggested methods that address the exploration problem by acting with \"optimism under uncertainty.\" If one assumes that the reinforcement learning algorithm will tend to choose the best action, it can be encouraged to visit state-action pairs that it has not frequently seen by augmenting the reward function to deliver a bonus for visiting novel states. This is accomplished with the augmented reward functionwhereis a novelty function designed to capture the novelty of a given state-action pair. Prior work has suggested a variety of different novelty functions e.g.,  based on state visitation frequency.While such methods offer a number of appealing guarantees, such as near-Bayesian exploration in polynomial time , they require a concise, often discrete representation of the agent's stateaction space to measure state visitation frequencies. In our approach, we will employ function approximation and representation learning to devise an alternative to these requirements. MODEL LEARNING FOR EXPLORATION BONUSESWe would like to encourage agent exploration by giving the agent exploration bonuses for visiting novel states. Identifying states as novel requires we supply some representation of the agent's state space, as well as a mechanism to use this representation to assess novelty. Unsupervised learning methods offer one promising avenue for acquiring a concise representation of the state with a good Encode the observations to obtain \u03c3(s t ) and \u03c3(s t+1 ) 5:Compute e(s t , a  2 and\u0113(s t , a t ) = e(st,at) maxe . :if e(s t , a t ) > max e then 8:max e = e(s t , a t ) :Store (s t , a t , R bonus ) in a memory bank \u2126. 11:Pass \u2126 to the reinforcement learning algorithm to update \u03c0.12:if t mod EpochLength == 0 then 13:Use \u2126 to update M.14:Optionally, update \u03c3.15:end if 16: end for 17: return optimized policy \u03c0 similarity metric. This can be accomplished using dimensionality reduction, clustering, or graphbased techniques . In our work, we draw on recent developments in representation learning with neural networks, as discussed in the following section. However, even with a good learned state representation, maintaining a table of visitation frequencies becomes impractical for complex tasks. Instead, we learn a model of the task dynamics that can be used to assess the novelty of a new state.Formally, let \u03c3(s) denote the encoding of the state s, and let M \u03c6 : \u03c3(S)\u00d7A \u2192 \u03c3(S) be a dynamics predictor parameterized by \u03c6. M \u03c6 takes an encoded version of a state s at time t and the agent's action at time t and attempts to predict an encoded version of the agent's state at time t + 1. The parameterization of M is discussed further in the next section.For each state transition (s t , a t , s t+1 ), we can attempt to predict \u03c3(s t+1 ) from (\u03c3(s t ), a t ) using our predictive model M \u03c6 . This prediction will have some errorLet e T , the normalized prediction error at time T , be given by e T := e T max t\u2264T {et} . We can assign a novelty function towhere C > 0 is a decay constant. We can now realize our augmented reward function asThis approach is motivated by the idea that, as our ability to model the dynamics of a particular state-action pair improves, we have come to understand the state better and hence its novelty is lower. When we don't understand the state-action pair well enough to make accurate predictions, we assume that more knowledge about that particular area of the model dynamics is needed and hence a higher novelty measure is assigned.Using learned model dynamics to assign novelty functions allows us to address the exploration versus exploitation problem in a non-greedy way. With an appropriate representation \u03c3(s t ), even when we encounter a new state-action pair (s t , a t ), we expect M \u03c6 (\u03c3(s t ), a t ) to be accurate so long as enough similar state-action pairs have been encountered.Our model-based exploration bonuses can be incorporated into any online reinforcement learning algorithm that updates the policy based on state, action, reward tuples of the formsuch as Q-learning or actor-critic algorithms. Our method is summarized in Algorithm 1. At each step, we receive a tuple (s t , a t , s t+1 , R(s t , a t )) and compute the Euclidean distance between the encoded state \u03c3(s t+1 ) to the prediction made by our model M \u03c6 (\u03c3(s t ), a t ). This is used to compute the exploration-augmented reward R Bonus using Equation (4). The tuples (s t , a t , s t+1 , R Bonus ) are stored in a memory bank \u2126 at the end of every step. Every step, the policy is updated.  Once per epoch, corresponding to 50000 observations in our implementation, the dynamics model M \u03c6 is updated to improve its accuracy. If desired, the representation encoder \u03c3 can also be updated at this time. We found that retraining \u03c3 once every 5 epochs to be sufficient.This approach is modular and compatible with any representation of \u03c3 and M, as well as any reinforcement learning method that updates its policy based on a continuous stream of observation, action, reward tuples. Incorporating exploration bonuses does make the reinforcement learning task nonstationary, though we did not find this to be a major issue in practice, as shown in our experimental evaluation. In the following section, we discuss the particular choice for \u03c3 and M that we use for learning policies for playing Atari games from raw images. DEEP LEARNING ARCHITECTURESThough the dynamics model M \u03c6 and the encoder \u03c3 from the previous section can be parametrized by any appropriate method, we found that using deep neural networks for both achieved good empirical results on the Atari games benchmark. In this section, we discuss the particular networks used in our implementation. AUTOENCODERSThe most direct way of learning a dynamics model is to directly predict the state at the next time step, which in the Atari games benchmark corresponds to the next frame's pixel intensity values. However, directly predicting these pixel intensity values is unsatisfactory, since we do not expect pixel intensity to capture the salient features of the environment in a robust way. In our experiments, a dynamics model trained to predict raw frames exhibited extremely poor behavior, assigning exploration bonuses in near equality at most time steps, as discussed in our experimental results section.To overcome these difficulties, we seek a function \u03c3 which encodes a lower dimensional representation of the state s. For the task of representing Atari frames, we found that an autoencoder could be used to successfully obtain an encoding function \u03c3 and achieve dimensionality reduction and feature extraction . Our autoencoder has 8 hidden layers, followed by a Euclidean loss layer, which computes the distance between the output features and the original input image. The hidden layers are reduced in dimension until maximal compression occurs with 128 units. After this, the activations are decoded by passing through hidden layers with increasingly large size. We train the network on a set of 250,000 images and test on a further set of 25,000 images. We compared two separate methodologies for capturing these images. Static AE:A random agent plays for enough time to collect the required images. The auto-encoder \u03c3 is trained offline before the policy learning algorithm begins. 2. Dynamic AE: Initialize with an epsilon-greedy strategy and collect images and actions while the agent acts under the policy learning algorithm. After 5 epochs, train the auto encoder from this data. Continue to collect data and periodically retrain the auto encoder in parallel with the policy training algorithm.We found that the reconstructed input achieves a small but non-trivial residual on the test set regardless of which auto encoder training technique is utilized, suggesting that in both cases it learns underlying features of the state space while avoiding overfitting.To obtain a lower dimensional representation of the agent's state space, a snapshot of the network's first six layers is saved. The sixth layer's output (circled in figure one) is then utilized as an encoding for the original state space. That is, we construct an encoding \u03c3(s t ) by running s t through the first six hidden layers of our autoencoder and then taking the sixth layers output to be \u03c3(s t ).In practice, we found that using the sixth layer's output (rather than the bottleneck at the fifth layer) obtained the best model learning results. See the appendix for further discussion on this result. MODEL LEARNING ARCHITECTUREEquipped with an encoding \u03c3, we can now consider the task of predicting model dynamics. For this task, a much simpler two layer neural network M \u03c6 suffices. M \u03c6 takes as input the encoded version of a state s t at time t along with the agent's action a t and seeks to predict the encoded next frame \u03c3(s t+1 ). Loss is computed via a Euclidean loss layer regressing on the ground truth \u03c3(s t+1 ). We find that this model initially learns a representation close to the identity function and consequently the loss residual is similar for most state-action pairs. However, after approximately 5 epochs, it begins to learn more complex dynamics and consequently better identify novel states. We evaluate the quality of the learned model in the appendix. EXPERIMENTAL RESULTSWe evaluate our approach on 14 games from the Arcade Learning Environment . The task consists of choosing actions in an Atari emulator based on raw images of the screen. Previous work has tackled this task using Q-learning with epsilon-greedy exploration , as well as Monte Carlo tree search  and policy gradient methods . We use Deep Q Networks (DQN)  as the reinforcement learning algorithm within our method, and compare its performance to the same DQN method using only epsilon-greedy exploration, Boltzman exploration, and a Thompson sampling approach.The results for 14 games in the Arcade Learning Environment are presented in . We chose those games that were particularly challenging for prior methods and ones where human experts outperform prior learning methods. We evaluated two versions of our approach; using either an autoencoder trained in advance by running epsilon-greedy Q-learning to collect data (denoted as \"Static AE\"), or using an autoencoder trained concurrently with the model and policy on the same image data (denoted as \"Dynamic AE\").  also shows results from the DQN implementation reported in previous work, along with human expert performance on each game . Note that our DQN implementation did not attain the same score on all of the games as prior work due to a shorter running time. Since we are primarily concerned with the rate of learning and not the final results, we do not consider this a deficiency. To directly evaluate the benefit of including exploration bonuses, we compare the performance of our approach primarily to our own DQN implementation, with the prior scores provided for reference.In addition to raw-game scores, and learning curves, we also analyze our results on a new benchmark we have named Area Under Curve 100 (AUC-100). For each game, this benchmark computes the area under the game-score learning curve (using the trapezoid rule to approximate the integral). This area is then normalized by 100 times the score maximum game score achieved in , which represents 100 epochs of play at the best-known levels. This metric more effectively captures improvements to the game's learning rate and does not require running the games for 1000 epochs as in . For this reason, we suggest it as an alternative metric to raw game-score.Bowling The policy without exploration tended to fixate on a set pattern of nocking down six pins per frame. When bonuses were added, the dynamics learner quickly became adept at predicting this outcome and was thus encouraged to explore other release points.Frostbite This game's dynamics changed substantially via the addition of extra platforms as the player progressed. As the dynamics of these more complex systems was not well understood, the system was encouraged to visit them often (which required making further progress in the game).Seaquest A submarine must surface for air between bouts of fighting sharks. However, if the player resurfaces too soon they will suffer a penalty with effects on the game's dynamics. Since these effects are poorly understood by the model learning algorithm, resurfacing receives a high exploration bonus and hence the agent eventually learns to successfully resurface at the correct time.Q * bert Exploration bonuses resulted in a lower score. In Q * bert, the background changes color after level one. The dynamics predictor is unable to quickly adapt to such a dramatic change in the environment and consequently, exploration bonuses are assigned in near equality to almost every state that is visited. This negatively impacts the final policy.Learning curves for each of the games are shown in 3). Note that both of the exploration bonus algorithms learn significantly faster than epsilon-greedy Q-learning, and often continue learning even after the epsilon-greedy strategy converges. All games had the inputs normalized according : Full learning curves and AUC-100 scores for all Atari games. We present the raw AUC-100 scores in the appendix.to  and were run for 100 epochs (where one epoch is 50,000 time steps). Between each epoch, the policy was updated and then the new policy underwent 10,000 time steps of testing. The results represent the average testing score across three trials after 100 epoch each.  auto encoder on 250000 raw game frames prior to policy optimization (raw frames are taken from random agent play). Dynamic AE retrains the auto encoder after each epoch, using the last 250000 images as a training set. Note that exploration bonuses help us to achieve state of the art results on Bowling and Frostbite. Each of these games provides a significant exploration challenge. Bolded numbers indicate the best-performing score among our experiments. Note that this score is sometimes lower than the score reported for DQN in prior work as our implementation only one-tenth as long as in .The results show that more nuanced exploration strategies generally improve on the naive epsilon greedy approach, with the Boltzman and Thompson sampling methods achieving the best results on three of the games. However, exploration bonuses achieve the fastest learning and the best results most consistently, outperforming the other three methods on 7 of the 14 games in terms of AUC-100. CONCLUSIONIn this paper, we evaluated several scalable and efficient exploration algorithms for reinforcement learning in tasks with complex, high-dimensional observations. Our results show that a new method based on assigning exploration bonuses most consistently achieves the largest improvement on a range of challenging Atari games, particularly those on which human players outperform prior learning methods. Our exploration method learns a model of the dynamics concurrently with the policy. This model predicts a learned representation of the state, and a function of this prediction error is added to the reward as an exploration bonus to encourage the policy to visit states with high novelty.One of the limitations of our approach is that the misprediction error metric assumes that any misprediction in the state is caused by inaccuracies in the model. While this is true in determinstic environments, stochastic dynamics violate this assumption. An extension of our approach to stochastic systems requires a more nuanced treatment of the distinction between stochastic dynamics and uncertain dynamics, which we hope to explore in future work. Another intriguing direction for future work is to examine how the learned dynamics model can be incorporated into the policy learning process, beyond just providing exploration bonuses. This could in principle enable substantially faster learning than purely model-free approaches. APPENDIX ON AUTO ENCODER LAYER SELECTIONRecall that we trained an auto-encoder to encode the game's state space. We then trained a predictive model on the next auto-encoded frame rather than directly training on the pixel intensity values of the next frame.To obtain the encoded space, we ran each state through an eight layer auto-encoder for training and then utilized the auto-encoder's sixth layer as an encoded state space. We chose to use the sixth layer rather than the bottleneck fourth layer because we found that, over 20 iterations of Seaquest at 100 epochs per iteration, using this layer for encoding delivered measurably better performance than using the bottleneck layer. The results of that experiment are presented below. : Game score averaged over 20 Seaquest iterations with various choices for the state-space encoding layer. Notice that choosing the sixth layer to encode the state space significantly outperformed the bottleneck layer. ON THE QUALITY OF THE LEARNED MODEL DYNAMICSEvaluating the quality of the learned dynamics model is somewhat difficult because the system is rewarded achieving higher error rates. A dynamics model that converges quickly is not useful for exploration bonuses. Nevertheless, when we plot the mean of the normalized residuals across all games and all trials used in our experiments, we see that the errors of the learned dynamics models continually decrease over time. The mean normalized residual after 100 epochs is approximately half of the maximal mean achieved. This suggests that each dynamics model was able to correctly learn properties of underlying dynamics for its given game.   : AUC-100 is computed by comparing the area under the game-score learning curve for 100 epochs of play to the area under of the rectangle with dimensions 100 by the maximum DQN score the game achieved in . The integral is approximated with the trapezoid rule. This more holistically captures the games learning rate and does not require running the games for 1000 epochs as in . For this reason, we suggest it as an alternative metric to raw game-score.\n###\n"}
{"text": "#Properties\nimplementation, metric, has research problem, evaluation, Approach type, Document type, Summary usage, Summary characteristics\n#Text\nWe present and evaluate SumUM, a text summarization system that takes a raw technical text as input and produces an indicative informative summary. The indicative part of the summary identifies the topics of the document, and the informative part elaborates on some of these topics according to the reader's interest. SumUM motivates the topics, describes entities, and defines concepts. It is a first step for exploring the issue of dynamic summarization. This is accomplished through a process of shallow syntactic and semantic analysis, concept identification, and text regeneration. Our method was developed through the study of a corpus of abstracts written by professional abstractors. Relying on human judgment, we have evaluated indicativeness, informativeness, and text acceptability of the automatic summaries. The results thus far indicate good performance when compared with other summarization technologies. 1.interpreting the text 2. extracting the relevant information, which ideally includes the \"topics\" of the source 3. condensing the extracted information and constructing a summary representation 4. presenting the summary representation to the reader in natural language.Even though some approaches to text summarization produce acceptable summaries for specific tasks, it is generally agreed that the problem of coherent selection and expression of information in text summarization is far from being resolved. Sparck stated the need for a research program in text summarization that would study the relation between source document and summary, the different types of summaries and their functions, the development of new methods and/or combination of already existing techniques for text summarization, and the development of evaluation procedures for summaries and systems. proposes the following typology of different types of document condensations:\u2022 the extract, which is a set of passages selected from a source document to represent the whole document\u2022 the summary, which occurs at the end of the document and is a restatement of the salient findings of a work\u2022 the abridgment, which is a reduction of the original document that necessarily omits secondary points\u2022 the precis, which stands for the main points of an argument\u2022 the digest, which is a condensation of a book or news article\u2022 the highlight, which is a comment included in specific parts of a document to alert a reader\u2022 the synopsis, which in cinematography represents a script of a film.In our research, we are concerned only with summaries of technical articles, which are called abstracts. In this context, two main types of abstracts are considered : indicative abstracts, which point to information alerting the reader about the content of an article in a given domain (these abstracts will contain sentences like \"The work of Consumer Advice Centres is examined.\"), and informative abstracts, which provide as much quantitative or qualitative information contained in the source document as possible (these abstracts will contain sentences like \"Consumer Advice Centres have dealt with preshopping advice, education on consumers' rights and complaints about goods and services, advising the client and often obtaining expert assessments.\"). In the course of our research, we have studied the relation between abstracts and source documents, and as a result, we have developed SumUM (Summarization at Universit\u00e9 de Montr\u00e9al), a text summarization system that produces an indicative-informative abstract for technical documents. The abstracts are produced in two steps: First, the reader is presented with an indicative abstract that identifies the topics of the document (what the authors present, discuss, etc.). Then, if the reader is interested in some of the topics, specific information about them from the source document is presented in an informative abstract. shows an automatic abstract produced by our system. The abstract was produced by a process of conceptual identification and text re-generation we call selective analysis. The indicative abstract contains information about the topic of the document. It describes the topics of sections and introduces relevant entities. The identified topics are terms either appearing in the indicative abstract or obtained from the terms and words of the indicative abstract through a process of term expansion. The one particular feature of these terms is that they can be used to obtain more conceptual information from the source document, such as definitions or statements of relevance, usefulness, and development, as can be seen in .This article is organized as follows. In the next section, we describe the analysis of a corpus of professional abstracts used to specify selective analysis; conceptual and linguistic information for the task of summarization of technical texts deduced from this corpus is also presented. An overview of selective analysis and the implementation Designing for human-robot symbiosis Presents the views on the development of intelligent interactive service robots. The authors have observed that a key research issue in service robotics is the integration of humans into the system. Discusses some of the technologies with particular emphasis on human-robot interaction, and system integration; describes human direct local autonomy (HuDL) in greater detail; and also discusses system integration and intelligent machine architecture (IMA). Gives an example implementation; discusses some issues in software development; and also presents the solution for integration, the IMA. Shows the mobile robot.Identified Topics: HuDL -IMAaid systemsarchitectureholonic manufacturing systemhumanhuman-robot interactionintelligent interactive service robotsintelligent machine architectureintelligent machine software interactionkey issuewidely used interactionnovel software architecture overall interactionrobotsecond issueserviceservice robotssoftware system -Technologies Figure 1Indicative abstract and identified topics for the text \"Designing for Human-Robot Symbiosis,\" D. M. Development of a service robot is an extremely challenging task. In the IRL, we are using HuDL to guide the development of a cooperative service robot team. IMA is a two-level software architecture for rapidly integrating these elements, for an intelligent machine such as a service robot. A holonic manufacturing system is a manufacturing system having autonomous but cooperative elements called holons . Communication between the robot and the human is a key concern for intelligent service robotics. Figure 2Informative abstract elaborating some topics. of our experimental prototype, SumUM, is then presented in section 3. In section 4, we discuss the limitations of our approach; then, in section 5, we present an evaluation and comparison of our method with state-of-the art summarization systems and human abstracts. Related work on\n###\n", "summary": " implementation: SumUM\n, metric: F-measure/Precision/Recall/Relative utility\n, has research problem: Automatic text summarization\n, evaluation: Evaluation\n, Approach type: Algebraic\n, Document type: Multiple documents\n, Summary characteristics: Abstractive\n###"}
{"summary": "has research problem: Global Mean Sea Level Rise Projections/Global Mean Sea Level Rise Projections/Global Mean Sea Level Rise Projections/Global Mean Sea Level Rise Projections/Global Mean Sea Level Rise Projections/Global Mean Sea Level Rise Projections\nGlobal Mean Sea level Rise Projection: Global Mean Sea Level Rise Projections/Global Mean Sea Level Rise Projections/Global Mean Sea Level Rise Projections/Global Mean Sea Level Rise Projections/Global Mean Sea Level Rise Projections/Global Mean Sea Level Rise Projections/Global Mean Sea Level Rise Projections\nhas upper limit for likely range: 2.09/1.25/0.78/0.40/0.36/0.33\nhas  start of period: 2000/2000/2000/2000/2000/2000\nhas lower limit for likely range: 1.09/0.66/0.37/0.22/0.18/0.16\nhas unit: m/m/m/m/m/m\nHas value: 1.46/0.91/0.56/0.31/0.26/0.23\nhas end of period: 2100/2100/2100/2050/2050/2050\nclimate scenario: RCP8.5/RCP4.5/RCP2.6/RCP8.5/RCP4.5/RCP2.6", "text": "#Properties\nhas research problem, Global Mean Sea level Rise Projection, has upper limit for likely range, has  start of period, has lower limit for likely range, has unit, Has value, has end of period, climate scenario\n#Text\nshelf hydrofracturing and ice-cliff collapse mechanisms highlights ambiguity in post-2050 sea-level projections. \u2022 These mechanisms make post-2050 sea level more heavily emissions dependent and can significantly revise high-emissions projections upwards. \u2022 Current Antarctic retreat by different processes than and exhibits little correlation with late-century changes. Methods Projections FrameworkThe framework employed to generate GMSL and RSL projections in this analysis is based on that of K14. The K14 framework combines multiple lines of information to construct probability distributions for key contributors to GMSL and RSL change. It employs a joint probability distribution for global mean thermal expansion and regional ocean dynamics derived from the Coupled Model Intercomparison Project Phase 5 (CMIP5)  ensemble. Its projections of glacier mass-balance changes are derived from the  surface mass-balance model, forced by the CMIP5 ensemble. Following the approach of , its projections of the global-mean contribution of anthropogenic changes in land-water storage are based upon historical relationships between human population, dam construction, and groundwater withdrawal . The regional contributions of non-climatic effects such as glacio-isostatic adjustment, tectonics, and sediment compaction are based upon a spatiotemporal statistical model of tide-gauge observations. Ice sheet contributions are derived from the AR5 expert assessment and the structured expert elicitation of , as described below. Glacier and ice sheet projections are translated into RSL changes using static-equilibrium fingerprints for eighteen glacier regions, the Greenland Ice Sheet, the West Antarctic Ice Sheet (WAIS), and the East Antarctic Ice Sheet (EAIS) .  elicited from fourteen experts central 90% probability estimates for the rate of GMSL rise in 2100 due to the Greenland ice sheet, WAIS and EAIS. They did not distinguish between surface-mass balance and dynamic contributions, nor did they distinguish between emissions scenarios. In turning these rates into cumulative 21st century GMSL rise contributions, they assumed a linear increase in rates based on the experts' rate estimates for the last decade and for 2100.AR5 assessed the likely (central 66% probability; see exegesis by ) range of Greenland and Antarctic contributions in 2080-2099, distinguishing between surfacemass balance and dynamic terms. They did not distinguish between EAIS and WAIS. For the dynamic AIS contribution, they did not distinguish among RCPs.  and AR5 approaches in a manner intended to retain consistency with the likely ranges of AR5. In particular, K14: (1) calculated probability distributions for EAIS, WAIS, and Greenland changes over time from , assuming linear changes in rates; (2) calculated probability distributions for AIS and Greenland over time based on the AR5 likely ranges for 2080-2099, again assuming linear changes in rates to achieve these values; (3) added a time-varying factor to the first set of distributions so the medians of the two sets align; (4) separated the AR5-derived Antarctic distribution into EAIS and WAIS terms by assuming the EAIS/WAIS ratio is the same as in the median projection from the first set; and (5) applied multipliers (separately for values greater than and less than the median) to the difference of the values in the final distri-bution from the distribution's median, so that the central 66% probability range matches that of AR5. K14 combined the Revised Antarctic projectionsIn this paper, we compare two sets of projections. The first, which we label K14, follows the original methodology of K14, extended in space and time. The second, which we label DP16, replaces the AIS projections of K14 with projections based on new physical modeling [DeConto and . These processes include the influence of surface meltwater, driven by summer temperatures above freezing and the increasing ratio of rain to snow in a warming climate, on the penetration into ice shelves of surface crevasses that can lead to hydrofracturing. Hence, in DP16, buttressing ice shelves can thin or be lost entirely due to sub-ice ocean warming, the extensive spread of surface meltwater, or a combination of the two. In places where thick, marine-terminating grounding lines have lost their buttressing ice shelves, a wastage rate of ice is applied locally at the tidewater grounding line in places where vertical ice cliffs are tall enough to produce stresses that exceed the yield strength of the ice (see DeConto and  and  for complete formulation).Three uncertain but key model parameters relate to (1) the rate of sub-ice shelf melt rates in response to warming ocean temperatures (OCFAC), (2) the sensitivity of crevasse penetration to meltwater input (hydrofracturing) (CREVLIQ), and (3) the maximum rate of cliff collapse (VCLIF). Because, as discussed below, there are no modern analogues to widespread ice-cliff failure, model performance cannot be adequately judged relative to Holocene or recent trends in ice-sheet behavior. Instead, the new model physics were tested relative to past episodes of ice sheet retreat during the Pliocene (\u223c3 Ma) and the Last Interglacial (LIG, \u223c125 Ma), when Antarctic ocean and surface air temperatures were warmer than today . The three key parameters were varied systematically. From an initial 64 versions of the ice sheet model, 29 were found to satisfy both Pliocene and LIG sea-level targets, with Antarctic contributions to GMSL ranging between 5 to 15 m (Pliocene) and 3.6 to 7.4 m (LIG). The range of oceanic melt rate model parameters passing the Pliocene and LIG sea-level tests are comparable to those determined from a large, 625-member ensemble of the last deglacial retreat of the West Antarctic Ice Sheet using the same ice sheet model ; however, the deglacial simulations do not provide guidance on hydrofracturing and ice-cliff physics, because the background climate was too cold to trigger those processes.One challenge of formulating a parameterization of ice-cliff physics is the lack of observations of marine-terminating ice without buttressing ice shelves and of sufficient thickness (\u223c1000 m) to allow subaerial ice cliffs tall enough (\u223c100 m) to drive structural collapse . The few calving fronts of this scale that exist today (e.g., Helheim and Jakobshavn Glaciers on Greenland, and Crane Glacier on the Antarctic Peninsula) are experiencing rates of calving and structural failure at the terminus, comparable to the seaward flow of the glaciers, on the order of \u223c2 to > 12 km/yr [e.g., . Unlike several major Antarctic outlet glaciers, these Greenland outlet glaciers are in relatively narrow (5-12 km wide), restricted fjords, with substantial m\u00e9lange (a mix of ice bergs and sea ice that can provide some supporting buttressing/back pressure at the terminus), and supportive, lateral shear along the fjord walls. Hence, using observed rates of cliff collapse to constrain the model physics representing these processes could lead to underestimates.In Antarctica, there is potential for much wider ice cliffs to form along vast stretches of the coastline if floating ice tongues and shelves are lost. For example, the throat of Thwaites Glacier is about 120 km wide, but at present its grounding line is mostly on bedrock too shallow (about 600 m deep; ) to drive extensive structural failure at the terminus . In the DP16, the highest maximum allowable rate of cliff collapse (VCLIF) -the maximum horizontal rate of ice loss applied at the marine \"tide-water\" calving terminus where ice cliffs are tall enough to generate stresses that exceed the strength of the ice -is 5 km/yr. This rate is about half the rate of mass wastage at the front of Jakobshavn, which currently has a relatively stable terminus position but is flowing seaward at > 12 km/year . To include the potential for even faster rates of ice sheet mass loss than in the existing model formulation, future work should consider a wider range of parameter space.We note that the paleo-sea-level targets used to test and calibrate the model physics provide limited guidance regarding potential rates of ice-sheet retreat. While  do provide an estimate of the rate of sea-level rise contributed by the Antarctic Ice Sheet during LIG retreat, both their temporal resolution and their ability to attribute GMSL changes to AIS are limited. Moreover, given limited Antarctic atmospheric warming during the LIG relative to the Pliocene, initial WAIS retreat was more likely driven by oceanic warming than atmospheric warming [DeConto and , and therefore offers little in terms of validating rates of retreat driven by extensive surface melt, hydrofracture, and cliff collapse.As described in DeConto and , the 29 versions of the ice sheet model satisfying geological constraints were used to simulate future ice-sheet retreat following RCP 2.6, 4.5, and 8.5 greenhouse gas pathways. In the future simulations, time-evolving oceanic melt rates were driven by NCAR CCSM4  subsurface ocean temperatures. Surface mass balance and meltwater production rates were calculated from monthly air temperatures and precipitation provided by the RegCM3 regional climate model  run offline and bias-corrected relative to a modern climatology [DeConto and .Coupled atmosphere-ocean models are known to struggle with subsurface ocean temperatures in the circum-Antarctic . To minimize the effects of a general cold bias in NCAR CCSM4 Antarctic Shelf Bottom Water in the Amundsen and Bellingshausen Seas, a correction of 3 \u2022 C was applied to ocean temperatures at 400 m depth. This bias correction is meant to compensate for the recent warming observed there . The correction is greater than the actual temperature offset, but given the formulation of the sub-ice melt rate parameterization used in DP16, a 3 \u2022 C correction is required to bring modern oceanic sub-ice shelf melt rates closer to observations . The effect of not using the ocean temperature/melt-rate correction in future simulations is shown in Supporting Information. Detection simulationTo simulate the process by which new observations of GMSL change can help detect whether the world is on a path leading to high or low levels of GMSL rise, we first define GMSL scenarios in a manner similar to . In particular, we pool the simulations of GMSL rise under RCP 2.6, RCP 4.5, and RCP 8.5, and then filter the pooled set to arrive at sets of simulations consistent with either 50 \u00b1 10 cm or 200 \u00b1 10 cm of GMSL rise between 2000 and 2100. We use the 5th-95th percentile range of these filtered sets to define scenario time paths. At each decade from 2000 to 2100, for each scenario, we compute the simulation frequency distribution of GMSL rise in 2100, conditional upon the observed GMSL in the decade being within the bounds of the scenario's time path. Finally, we compare the resulting conditional distributions to assess the detectability in a given decade of the difference between a pathway leading to about 50 cm of GMSL rise in 2100 and one leading to about 200 cm of GMSL rise. Extensions of the spatial and temporal domainThe projections framework in this paper has a more extended spatial domain than the original K14 projections. While the original K14 projections were generated only at the pre-cise location of tide gauges, here we also generate projections at points on a 2 \u2022 \u00d72 \u2022 -resolution global grid that intersect world coastlines. At these points, we use the spatiotemporal statistical model described in K14 to estimate (with larger errors than at the tide-gauge sites) the long-term, non-climatic, background contribution to RSL change. The projection assumes that the background rate of change estimated from tide-gauge data continues unchanged over the duration of the projections.The projection framework also has a more extended temporal domain than the original K14 projections. Whereas the original K14 projections end in 2200, here we generated projections to 2300. This extension requires no computational modifications to the K14 framework. However, we regard this time frame as more appropriate when considering projections in which Antarctic ice sheet behavior are determined by a physical model rather than by a simple, temporally quadratic projection. Assessment of population exposureAs an integrative metric of RSL changes, we assess the population currently occupying land threatened with submergence under different sea level rise projections. To do this, we compare land elevations from NASA's 1-arcsec SRTM 3.0 digital elevation model [NASA JPL, 2013] against nearest-neighbor water elevations derived from adding the K14 and DP16 projection grids to measured local mean sea surface elevation augmented by a modeled tidal supplement. We intersect the resulting inundation surfaces with contemporary population  and national boundary  data to estimate current national populations occupying land at risk of permanent submergence. SRTM data are the most practical option and widely used for global coastal exposure research, but bias estimates low . For each set of sea-level rise projections, we assess the population exposed assuming each grid cell followed its 50th, 5th, or 95th percentile RSL projection. Further details are provided in the Supporting Information. We emphasize that the resulting values are not projections of the impacts of RSL change, which would require a dynamic model considering both population growth and migration away from inundated regions; rather, population here serves as a convenient integrative metric. ResultsThe K14 Antarctic projections -like those of Bamber and Aspinall  and , among others -assumed that changes in the rates of change in ice-sheet mass balance occurred linearly. For example,  elicited expert opinion on the rate of Antarctic ice sheet mass change in 2100, and assumed that the elicited rate was achieved following a linear growth rate. The result is a quadratic change in ice volume over time. K14 took the same approach . By contrast, process modeling as in DP16 shows considerably more complex behavior, with periods of rapid increases in mass loss rate as individual sectors of ice sheet collapse, and other intervals of stable or declining rates of retreat ( ). Sizable non-linearity appears in all simulations under strong forcing , RCP 8.5) and under all forcings in almost all simulations with high maximum rates of ice-cliff collapse ( , purple and magenta curves).In the first half of the 21st century, the range spanned by DP16 Antarctic projections is similar to that spanned by K14 (-10 to +23 cm contribution to GMSL in 2050, vs. a 1st-99th percentile range of -2 to +14 cm under K14). Both sets of projections show minimal emissions-scenario dependency in the first half of the 21st century. The central tendency among the DP16 projections is slightly higher, with a median contribution to GMSL of about +5 cm under DP16, compared to a median of +2 cm under K14. These slightly higher values are driven by the ocean-temperature bias correction, which is needed to improve consistency with observed oceanic sub-ice melt rates in the Amundsen and Bellingshausen Sea sectors of West Antarctica . Without this correction, there is a tendency toward Antarctic growth in the early decades of the century (in 2050, RCP 8.5: median -3 cm, range of -9 to +12 cm, RCP 2.6: median -2 cm, range of -10 to +6 cm). However, even with the bias correction, Antarctica's median contribution to GMSL is still 0.1 mm/yr, which is about a factor of 3 less than that currently observed for the early 21st-century . Overall, the substitution of DP16 has a very limited effect on mid-century GMSL projections .Under strong forcing, the overall picture changes dramatically by the end of the 21st century, with several of the DP16 simulations leading to AIS contributions to GMSL exceeding +1 m by 2100 under RCP 8.5 ( ). These high projections are driven by high maximum rates of ice-cliff retreat (VCIF = 5 km/yr) in combination with non-zero sensitivity of ice shelves to hydrofracturing (CREVLIQ > 0) . As a consequence, the median DP16 GMSL projections for 2100 under RCP 8.5 reaches 146 cm, the 98th percentile projection under K14. The low tail is curtailed by the incorporation of physical modeling, with a 1st percentile of 80 cm exceeding the median of K14. With a high VCLIF, the median GMSL projection reaches 213 cm (in excess of the 99th percentile of K14); with no cliff collapse mechanism or no hydrofracturing, it is reduced to about 125 cm (96th percentile of K14) (Supporting  A significant enhancement of the AIS contribution to GMSL also occurs for 2100 under moderate forcing: the median DP16 total GMSL projection of 91 cm under RCP 4.5 is consistent with the 93rd percentile of K14. The low tail is modestly curtailed: the DP16 1st-99th percentile values for RCP 4.5 (39-180 cm) resemble the K14 9th-99.8th percentile range. Under low forcing (RCP 2.6), there is little effect, with the DP16 1st-99th percentile range (18-111 cm) resembling the K14 0.5th-99th percentile range. These differences build over the 22nd and 23rd century. By 2300, under RCP 8.5, the median DP16 GMSL projection of 11.7 m exceeds the K14 99th percentile. Although the ice-cliff collapse mechanism contributes to this projection, the median projection remains as high as 10.0 m by 2300 even without it (Supporting ). Without protective measures, median DP16 RSL projections would submerge land currently home to 950 million people worldwide, a roughly three-fold increase relative to K14 ( . The DP16 1st-99th percentile range (8.6-17.5 m) resembles the K14 97th-99.8th percentile range. Under RCP 4.5, the median DP16 GMSL projection of 4.2 m resembles the K14 90th percentile, and the DP16 1st-99th percentile range (1.6-8.1 m) resembles the K14 42nd-98th percentile range. The median is reduced to 3.0 m (75th percentile of K14) without the ice-cliff collapse mechanism, and 3.2 m (79th percentile of K14) without the hydrofracturing mechanism. Under RCP 2.6, by contrast, the median DP16 GMSL projection of 1.4 m matches the K14 median, and the DP16 1st-99th percentile range (0.2-4.0 m) resembles the K14 14th-92nd percentile range.Taken together, the incorporation of the DP16 AIS ensemble pulls the projections much higher by 2100 and beyond under RCP 4.5 and especially RCP 8.5 ). It thus leads to a significant reduction in overlap among projections of GMSL change based upon different emissions scenarios. This is to be expected based on the difference in construction between the K14 Antarctic projections and the DP16 projections. In K14, as in AR5, AIS surface mass balance is scenario-dependent, but the ice-sheet dynamic term is treated as scenario-independent: it is assumed that the uncertainty in physical understanding of icesheet behavior swamps the forcing uncertainty. By contrast, the physical model of DP16 yields a strong forcing dependence.As a consequence of this difference, the proportion of total projection variance attributable to emissions changes significantly with the incorporation of the DP16 ensemble ( ). Under K14, relative to RCP 4.5, thermal expansion is initially the dominant contributor to projection variance (accounting for about 70% of total variance in 2020). By 2060, AIS accounts for one-third of total variance and is the single largest contributor. The AIS share grows over time, accounting for more than 60% of total variance by 2300. Assuming RCP 2.6, 4.5 and 8.5 are all treated as equally likely, scenario uncertainty accounts for only \u223c10% of total variance in 2050, a share that grows to 20-30% by 2070 and stays in this range through 2300.Under DP16, physical uncertainty in AIS initially dominates total variance (89% in 2020). This share declines over time, predominantly losing out to emissions scenario uncertainty, which grows from 8% of total variance in 2050, to 45% in 2070, to 65% in 2100, and continues to grow to 89% in 2250. This shift reflects the larger sensitivity of the DP16 AIS projections to emissions scenario.The assumption of a simple linear change in rate of mass loss underlying K14 leads to a perfect correlation between the rate of AIS mass loss observed in the near term and that projected for the long term ). If this assumption were correct, knowing that AIS mass loss in the first decades of this century fell in the middle of the estimated distribution would rule out high-end mass loss late in the century or beyond. By contrast, DP16 projections reveal no correlation between the AIS contribution to GMSL in 2020 and that in 2100 (r = \u22120.08, pooling across RCPs and both with and without an ocean temperature adjustment), and only a weak correlation between the AIS contribution in 2050 and in 2100 (r = 0.26). In the second half of the century, observed AIS behavior becomes more strongly predictive of long-term behavior; the correlation with the AIS contribution to 2300 grows from r = 0.26 in 2050 to r = 0.82 in 2100, r = 0.97 in 2150, and r = 0.997 in 2200. The general lack of correlation between the AIS contributions in 2020 and 2100, and the rapidly strengthening correlations in the second half of the 21st century, are caused by a transition in the model from an ocean-dominated driver of ice-shelf loss (and reduced buttressing) to an atmosphere-dominated driver via hydrofracturing. The lack of correlation between early 21st century and subsequent projections has important implications for the ability of GMSL observations to constrain future GMSL rise. In K14, simulations consistent with 50 \u00b1 10 cm of GMSL rise in 2100 diverge from those consistent with 200 \u00b1 10 cm of GMSL rise by the 2020s ). The median conditional projections for 2100 under the 200 cm scenario exceed the 95th percentile under the 50 cm scenario by 2030, and the 5th percentile of the 200 cm conditional distribution exceeds the 95th percentile under the 50 cm scenario shortly thereafter. 95% of projections for 2100 under the 200 cm time path exceed 100 cm in the 2030s and 150 cm in the 2040s ). By contrast, the more complex temporal dynamics of the DP16 simulations delays the divergence of the 50 and 200 cm time paths until around 2050 ). The median conditional projection for 2100 under the 200 cm scenario does not exceed the 95th percentile of the 50 cm scenario until the 2050s, with the 5th percentile of the 200 cm conditional distribution exceeding the 95th percentile under the 50 cm scenario in the 2060s. 95% of projections for 2100 under the 200 cm time path exceed 50 cm in the 2040s, 100 cm in the 2060s, and 150 cm in the 2070s ).The effect of DP16 on RSL projections is as would be expected based on the change in projected WAIS and EAIS contributions and their associated static-equilibrium fingerprints (  and Supporting Information). Relative to K14, the effect on RSL projections for 2050 is minimal (< 4 cm). By 2100, however, the increase in median ASL contribution decreases projected RSL rise in the Antarctic, while enhancing it most strongly in in a geographic swath including North America, the central Pacific, Australia, southeast Asia, and parts of India and Africa same. (Detailed simulation frequency distributions of RSL at tide gauge sites and on the global coastal grid are provided in the Supporting Information.) .6 (5.5 -10.6) 8.0 (5.7 -11.1) 8.9 (6.5 -14.0) 14.0 (8.9 -23.5) India 1,173 6.8 (5.2 -9.1) 7.1 (5.3 -9.4) 8.0 (5.8 -11.4) 11.5 (8.0 -18.5) Indonesia 243 5.4 (3.8 -8.0) 5.6 (4.0 -8.4) 6.3 (4.6 -9.9) 9.8 (6.1 -16.6) Vietnam 90 11.5 (9.1 -15.1) 11.7 (9.3 -15.5) 12.9 (10.1 -17.0) 17.0 (12.7 -23.9)Current population occupying land exposed to inundation under 2300 RSL projections Contents1. Text S1 2. Data Set S1 3. Data Set S2 4. Data Set S3 5. Data Set S4 6. Data Set S5 7. Data Set S6Text S1: Population Exposure Assessment Detailed Methods.To assess topography as required for this analysis, we employ the NASA SRTM digital elevation model, which is based on a radar satellite mission in 2000. This Data Set has nearly global coverage (including latitudes 60N-54S, covering land inhabited by more than 99.9 percent of global population), and is available at a 1 arcsec (SRTM 3.0) horizontal resolution . SRTM, as distributed, is referenced to the EGM96 geoid (henceforth denoted by SRT M EG M96 ) at a 1 m vertical resolution, with RMSE less than 10 m . Derived from radar measurements, SRT M EG M96 is an unclassified (surface) elevation model, and significant positive bias is expected in regions of dense urban development and vegetation .To convert elevations to a tidal vertical datum, we use the global mean sea level (MSL) model MSS_CNES_CLS_11 , based on TOPEX/Poseidon satellite measurements, and referenced to the GLAS ellipsoid (MSL GL AS ). We also employ mean higherhigh water (M HHW M SL ) deviations from MSL provided by Mark Merrifield, University of Hawaii, developed using the model TPX08 . We convert MSL and SRTM to a common ellipsoidal datum (WGS84) using NOAA's VDatum tool  version 3.7, and subtract the MHHW grids from MSL W GS84 to find M HHW W GS84 . We then subtract this tidal grid from SRT M W GS84 to produce our final elevation map, SRT M M H HW .We resample a given relative sea-level rise projection grid X using bilinear interpolation to match the horizontal resolution of SRTM, and threshold elevation against these water heights to produce the inundation surface T HRE SH x . Hydrological connectivity to the Corresponding author: R. E. Kopp, robert.kopp@rutgers.edu ocean is typically enforced in such analyses, but we find that high-frequency errors present in SRTM create significant speckle noise in the flood maps, causing some truly connected areas to appear isolated. We instead perform connected components analysis at the 20m water height above MHHW, producing surface CONT IG 20m , and perform the intersection HY BRI D x = T HRE SH x \u2229 CONT IG 20m . Isolated, low-lying land separated from ocean by at least a 20m high ridge is therefore removed from the final surface, HY BRI D x , while low-lying ocean-side land is included, reducing sensitivity to speckle noise. We compute inundation surfaces given different projection models (K14 and DP16), emissions scenarios (RCP 2.6 and RCP 8.5), percentiles of RSL projections (5th, 50th, and 95th), and years (2100 and 2300).To assess population exposure, we employ the LandScan 2010 High Resolution global Population Data Set, which provides total estimated populations living in 1 km square cells . We refine this data using the SRTM Water Body Data Set (SWBD), which defines land cells at 1-arcsecond resolution. We resample Landscan at 1-arcsecond resolutions to align with the SRTM grids, assuming zero population in water cells, while proportionally increasing the population density in land cells to ensure total population in each 1 km square remains unchanged. We integrate exposure under each inundation surface and tabulate according to national boundaries defined by the Global Administrative Areas (GADM) 2.0 Data Set .Linked to the positive bias in SRTM elevation data, a notable negative bias has been detected in coastal population flood exposure analysis based on SRTM, at least within the United States, where higher quality, LIDAR-based elevation models are freely available for comparison . LIDAR-based US national exposure estimates are \u223c45% higher than SRTM-based estimates at 1 m above MHHW; \u223c150% higher at 2 m and 3 m; and monotonically decline to \u223c33% higher at 10 m . The exposure values presented here can thus be viewed as likely to significantly underestimate the true hazard. We nevertheless include them to provide some illustration of the ramifications of different projections. SRTM data are widely used in analysis of global exposure to sea level rise and coastal flooding [e.g., , and the major available alternative, the Global Land One-kilometer Base Elevation (GLOBE) gridded elevation model, is far coarser in resolution and based on underlying data of varying and unknown quality by region .The LandScan 2010 High Resolution global Population Data Set is copyrighted by UT-Battelle, LLC, operator of Oak Ridge National Laboratory under Contract No. DE-AC05-00OR22725 with the United States Department of Energy. The United States Government has certain rights in this Data Set. Neither UT-Battelle, LLC nor the United States Department of Energy, nor any of their employees, makes any warranty, express or implied, or assumes any legal liability or responsibility for the accuracy, completeness, or usefulness of the data set. Data Set S1Data Set S1 provides time series of WAIS, EAIS and total AIS contributions to GMSL from 2000 to 2300 from DP16. Data Set S2Data Set S2 provides estimated non-climatic background rates and IDs, latitude, and longitude of tide-gauge and grid point locations.\n###\n"}
{"summary": "has benchmark: Benchmark Sequential CIFAR-10/Benchmark Penn Treebank (Character Level)/Benchmark WikiText-103/Benchmark Penn Treebank (Word Level)\nhas research problem: Language Modelling\nhas model: Trellis Network", "text": "#Properties\nhas benchmark, has research problem, has model\n#Text\nWe present trellis networks, a new architecture for sequence modeling. On the one hand, a trellis network is a temporal convolutional network with special structure, characterized by weight tying across depth and direct injection of the input into deep layers. On the other hand, we show that truncated recurrent networks are equivalent to trellis networks with special sparsity structure in their weight matrices. Thus trellis networks with general weight matrices generalize truncated recurrent networks. We leverage these connections to design high-performing trellis networks that absorb structural and algorithmic elements from both recurrent and convolutional models. Experiments demonstrate that trellis networks outperform the current state of the art methods on a variety of challenging benchmarks, including word-level language modeling and character-level language modeling tasks, and stress tests designed to evaluate long-term memory retention. The code is available here 1 . SEQUENCE MODELING AND TRELLIS NETWORKSSequence modeling. Given an input x 1:T = x 1 , . . . , x T with sequence length T , a sequence model is any function G :where y t should only depend on x 1:t and not on x t+1:T (i.e. no leakage of information from the future). This causality constraint is essential for autoregressive modeling.In this section, we describe a new architecture for sequence modeling, referred to as a trellis network or TrellisNet. In particular, we provide an atomic view of TrellisNet, present its fundamental features, and highlight the relationship to convolutional networks. Section 4 will then elaborate on the relationship of trellis networks to convolutional and recurrent models.Notation. We use x 1:T = (x 1 , . . . , x T ) to denote a length-T input sequence, where vector x t \u2208 R p is the input at time step t. Thus x 1:T \u2208 R T \u00d7p . We use z (i) t \u2208 R q to represent the hidden unit at time t in layer i of the network. We use Conv1D(x; W ) to denote a 1D convolution with a kernel W applied to input x = x 1:T .A basic trellis network. At the most basic level, a feature vector z (i+1) t+1 at time step t + 1 and level i + 1 of TrellisNet is computed via three steps, illustrated in : Padding Padding (b) TrellisNet on a sequence of units : The interlayer transformation of TrellisNet, at an atomic level (time steps t and t + 1, layers i and i + 1) and on a longer sequence (time steps 1 to 8, layers i and i + 1).1. The hidden input comprises the hidden outputs zt+1 \u2208 R q from the previous layer i, as well as an injection of the input vectors x t , x t+1 . At level 0, we initialize to zt+1 \u2208 R r is produced by a feed-forward linear transformation:where  are weights, and r is the size of the pre-activation output\u1e91t+1 . (Here and throughout the paper, all linear transformations can include additive biases. We omit these for clarity.) The output zis produced by a nonlinear activation function f : R r \u00d7 R q \u2192 R q applied to the pre-activation output\u1e91 (i+1) t+1 and the output z  t from the previous layer. More formally,A full trellis network can be built by tiling this elementary procedure across time and depth. Given an input sequence x 1:T , we apply the same production procedure across all time steps and all layers, using the same weights. The transformation is the same for all elements in the temporal dimension and in the depth dimension. This is illustrated in . Note that since we inject the same input sequence at every layer of the TrellisNet, we can precompute the linear transformatio\u00f1xt+1 for all layers i. This identical linear combination of the input can then be added in each layer i to the appropriate linear combination of the hidden units,. Now observe that in each level of the network, we are in effect performing a 1D convolution over the hidden units z (i) 1:T . The output of this convolution is then passed through the activation function f . Formally, with W \u2208 R r\u00d7q as the kernel weight matrix, the computation in layer i can be summarized as follows ( :The resulting network operates in feed-forward fashion, with deeper elements having progressively larger receptive fields. There are, however, important differences from typical (temporal) convolutional networks. Notably, the filter matrix is shared across all layers. That is, the weights are tied not only across time but also across depth.  have previously tied weights across depth in image processing.) Another difference is that the transformed input sequencex 1:T is directly injected into each hidden layer. These differences and their importance will be analyzed further in Section 4.The activation function f in Equation (3) can be any nonlinearity that processes the pre-activation output\u1e91 (i+1) 1:T and the output from the previous layer z (i) 1:T \u22121 . We will later describe an activation function based on the LSTM cell. The rationale for its use will become clearer in light of the analysis presented in the next section. TRELLISNET, TCN, AND RNNIn this section, we analyze the relationships between trellis networks, convolutional networks, and recurrent networks. In particular, we show that trellis networks can serve as a bridge between convolutional and recurrent networks. On the one hand, TrellisNet is a special form of temporal convolutional networks (TCN); this has already been clear in Section 3 and will be discussed further in Section 4.1. On the other hand, any truncated RNN can be represented as a TrellisNet with special structure in the interlayer transformations; this will be the subject of Section 4.2. These connections allow TrellisNet to harness architectural elements and regularization techniques from both TCNs and RNNs; this will be summarized in Section 4.3. TRELLISNET AND TCNWe briefly introduce TCNs here, and refer the readers to  for a more thorough discussion. Briefly, a temporal convolutional network (TCN) is a ConvNet that uses one-dimensional convolutions over the sequence. The convolutions are causal, meaning that, at each layer, the transformation at time t can only depend on previous layer units at times t or earlier, not from later points in time. Such approaches were used going back to the late 1980s, under the name of \"time-delay neural networks\" , and have received significant interest in recent years due to their application in architectures such as WaveNet .In essence, TrellisNet is a special kind of temporal convolutional network. TCNs have two distinctive characteristics: 1) causal convolution in each layer to satisfy the causality constraint and 2) deep stacking of layers to increase the effective history length (i.e. receptive field). Trellis networks have both of these characteristics. The basic model presented in Section 3 can easily be elaborated with larger kernel sizes, dilated convolutions, and other architectural elements used in TCNs; some of these are reviewed further in Section 4.3.However, TrellisNet is not a general TCN. As mentioned in Section 3, two important differences are: 1) the weights are tied across layers and 2) the linearly transformed inputx 1:T is injected into each layer. Weight tying can be viewed as a form of regularization that can stabilize training, support generalization, and significantly reduce the size of the model. Input injection mixes deep features with the original sequence. These structural characteristics will be further illuminated by the connection between trellis networks and recurrent networks, presented next. TRELLISNET AND RNNRecurrent networks appear fundamentally different from convolutional networks. Instead of operating on all elements of a sequence in parallel in each layer, an RNN processes one input element at a time and unrolls in the time dimension. Given a non-linearity g (which could be a sigmoid or a more elaborate cell), we can summarize the transformations in an L-layer RNN at time-step t as follows:Despite the apparent differences, we will now show that any RNN unrolled to a finite length is equivalent to a TrellisNet with special sparsity structure in the kernel matrix W . We begin by formally defining the notion of a truncated (i.e. finite-horizon) RNN. Definition 1. Given an RNN \u03c1, a corresponding M-truncated RNN \u03c1 M , applied to the sequence x 1:T , produces at time step t the output y t by applying \u03c1 to the sequence x t\u2212M +1:t (here x <0 = 0).Theorem 1. Let \u03c1 M be an M -truncated RNN with L layers and hidden unit dimensionality d. Then there exists an equivalent TrellisNet \u03c4 with depth (M + L \u2212 1) and layer width (i.e. number of channels in each hidden layer) Ld. Specifically, for anythe TrellisNet outputs contain the RNN outputs).Theorem 1 states that any M -truncated RNN can be represented as a TrellisNet. How severe of a restriction is M -truncation? Note that M -truncation is intimately related to truncated backpropagation-through-time (BPTT), used pervasively in training recurrent networks on long sequences. While RNNs can in principle retain unlimited history, there is both empirical and theoretical evidence that the memory horizon of RNNs is bounded . Furthermore, if desired, TrellisNets can recover exactly a common method of applying RNNs to long sequences -hidden state repackaging, i.e. copying the hidden state across subsequences. This is accomplished using an analogous form of hidden state repackaging, detailed in Appendix B.Proof of Theorem 1. Let h (i) t,t \ufffd \u2208 R d be the hidden state at time t and layer i of the truncated RNN \u03c1 t\u2212t \ufffd +1 (i.e., the RNN begun at time t \ufffd and run until time t). Note that without truncation, history starts at time t \ufffd = 1, so the hidden state h (i) t of \u03c1 can be equivalently expressed as h (i) t,1 . When t \ufffd > t, we define h t,t \ufffd = 0 (i.e. no history information if the clock starts in the future).By assumption, \u03c1 M is an RNN defined by the following parameters:hx \u2208 R w\u00d7d for all i = 2, . . . , L are the weight matrices at each layer (w is the dimension of pre-activation output). We now construct a TrellisNet \u03c4 according to the exact definition in Section 3, with parameters {W 1 , W 2 , f }, where  . We define a nonlinearity f by f (\u03b1, \u03b2) = g(\u03b1) (i.e. applying g only on the first entry).Let t \u2208 [T ] , j \u2265 0 be arbitrary and fixed. We now claim that the hidden unit at time t and layer j of TrellisNet \u03c4 can be expressed in terms of hidden units at time t in truncated forms of \u03c1:whereis the time-t hidden state at layer j of \u03c4 and h (i) t,t \ufffd is the time-t hidden state at layer i of \u03c1 t\u2212t \ufffd +1 .We prove Eq. (6) by induction on j. As a base case, consider j = 0; i.e. the input layer of \u03c4 . Since h t,t \ufffd = 0 when t \ufffd > t, we have that z (0) j = [0 0 . . . 0] \ufffd . (Recall that in the input layer of TrellisNet we initialize z (0) t = 0.) For the inductive step, suppose Eq. (6) holds for layer j, and consider layer j + 1. By the feed-forward transformation of TrellisNet defined in Eq. (2) and the nonlinearity f we defined above, we have:where in Eq. (10) we apply the RNN non-linearity g following Eq. (4). Therefore, by induction, we have shown that Eq. (6) holds for all j \u2265 0.If TrellisNet \u03c4 has M +L\u22121 layers, then at the final layer we have) is exactly the output of \u03c1 M at time t.x1x2x4x4 h  4,3 h(2) 4,4  In other words, we have shown that \u03c1 M is equivalent to a TrellisNet with sparse kernel matrices W 1 , W 2 . This completes the proof.Note that the convolutions in the TrellisNet \u03c4 constructed in Theorem 1 are sparse, as shown in Eq. (5). They are related to group convolutions , but have an unusual form because group k at time t is convolved with group k \u2212 1 at time t + 1. We refer to these as mixed group convolutions. Moreover, while Theorem 1 assumes that all layers of \u03c1 M have the same dimensionality d for clarity, the proof easily generalizes to cases where each layer has different widths.For didactic purposes, we recap and illustrate the construction in the case of a 2-layer RNN. The key challenge is that a na\u00efve unrolling of the RNN into a feed-forward network does not produce a convolutional network, since the linear transformation weights are not constant across a layer. The solution, illustrated in , is to organize each hidden unit into groups of channels, such that each TrellisNet unit represents 3 RNN units simultaneously (for xt, h  t , h(2) t ). Each TrellisNet unit thus has (p + 2d) channels. The interlayer transformation can then be expressed as a mixed group convolution, illustrated in . This can be represented as a sparse convolution with the structure given in Eq. (5) (with L = 2). Applying the nonlinearity g on the pre-activation output, this exactly reproduces the transformations in the original 2-layer RNN.The TrellisNet that emerges from this construction has special sparsity structure in the weight matrix. It stands to reason that a general TrellisNet with an unconstrained (dense) weight matrix W may have greater expressive power: it can model a broader class of transformations than the original RNN \u03c1 M . Note that while the hidden channels of the TrellisNet \u03c4 constructed in the proof of Theorem 1 are naturally arranged into groups that represent different layers of the RNN \u03c1 M (Eq. (6)), an unconstrained dense weight matrix W no longer admits such an interpretation. A model defined by a dense weight matrix is fundamentally distinct from the RNN \u03c1 M that served as our point of departure. We take advantage of this expressivity and use general weight matrices W , as presented in Section 3, in our experiments. Our ablation analysis will show that such generalized dense transformations are beneficial, even when model capacity is controlled for.The proof of Theorem 1 did not delve into the inner structure of the nonlinear transformation g in RNN (or f in the constructed TrellisNet). For a vanilla RNN, for instance, f is usually an elementwise sigmoid or tanh function. But the construction in Theorem 1 applies just as well to RNNs with structured cells, such as LSTMs and GRUs. We adopt LSTM cells for the TrellisNets in our experiments and provide a detailed treatment of this nonlinearity in Section 5.1 and Appendix A. TRELLISNET AS A BRIDGE BETWEEN RECURRENT AND CONVOLUTIONAL MODELSIn Section 4.1 we concluded that TrellisNet is a special kind of TCN, characterized by weight tying and input injection. In Section 4.2 we established that TrellisNet is a generalization of truncated RNNs. These connections along with the construction in our proof of Theorem 1 allow TrellisNets to benefit significantly from techniques developed originally for RNNs, while also incorporating architectural and algorithmic motifs developed for convolutional networks. We summarize a number of techniques here. From recurrent networks, we can integrate 1) structured nonlinear activations (e.g. LSTM and GRU gates); 2) variational RNN dropout ; 3) recurrent DropConnect ; and 4) history compression and repackaging. From convolutional networks, we can adapt 1) larger kernels and dilated convolutions ; 2) auxiliary losses at intermediate layers ; 3) weight normalization ; and 4) parallel convolutional processing. Being able to directly incorporate techniques from both streams of research is one of the benefits of trellis networks. We leverage this in our experiments and provide a more comprehensive treatment of these adaptations in Appendix B. In our description of generic trellis networks in Section 3, the activation function f can be any nonlinearity that computes EXPERIMENTSIn experiments, we use a gated activation based on the LSTM cell. Gated activations have been used before in convolutional networks for sequence modeling . Our choice is inspired directly by Theorem 1, which suggests incorporating an existing RNN cell into TrellisNet. We use the LSTM cell due to its effectiveness in recurrent networks . We summarize the construction here; a more detailed treatment can be found in Appendix A.In an LSTM cell, three information-controlling gates are computed at time t. Moreover, there is a cell state that does not participate in the hidden-to-hidden transformations but is updated in every step using the result from the gated activations. We integrate the LSTM cell into the TrellisNet as follows :Thus the linear transformation in each layer of the TrellisNet produces a pre-activation featur\u00ea z t+1 with r = 4q feature channels, which are then processed by elementwise transformations and Hadamard products to yield the final output\ufffd of the layer. RESULTSWe evaluate trellis networks on word-level and character-level language modeling on the standard Penn Treebank (PTB) dataset , large-scale word-level modeling on WikiText-103 (WT103) , and standard stress tests used to study long-range information propagation in sequence models: sequential MNIST, permuted MNIST (PMNIST), and sequential . Note that these tasks are on very different scales, with unique properties that challenge sequence models in different ways. For example, word-level PTB is a small dataset that a typical model easily overfits, so judicious regularization is essential. WT103 is a hundred times larger, with less danger of overfitting, but with a vocabulary size of 268K that makes training more challenging (and precludes the application of techniques such as mixture of softmaxes ). A more complete description of these tasks and their characteristics can be found in Appendix C.   66M 73.4 NAS Cell  54M 62.4 AWD-LSTM  24M 58.8 (Black-box tuned) NAS  24M 59.7 (Black-box tuned) LSTM + skip conn.  58.3 AWD-LSTM-MoC  22M 57.55 DARTS  23M 56.10 AWD-LSTM-MoS  24M 55.97 ENAS   37.2 AWD-QRNN  159M 33.0 Relational Memory Core  The prior state of the art on these tasks was set by completely different models, such as AWD-LSTM on character-level PTB , neural architecture search on word-level PTB , and the self-attention-based Relational Memory Core on WikiText-103 . We use trellis networks on all tasks and outperform the respective state-of-the-art models on each. For example, on word-level Penn Treebank, TrellisNet outperforms by a good margin the recent results of , which used the Google Vizier service for exhaustive hyperparameter tuning, as well as the recent neural architecture search work of . On WikiText-103, a trellis network outperforms by 7.6% the Relational Memory Core  and by 11.5% the thorough optimization work of .Many hyperparameters we use are adapted directly from prior work on recurrent networks. (As highlighted in Section 4.3, many techniques can be carried over directly from RNNs.) For others, we perform a basic grid search. We decay the learning rate by a fixed factor once validation error plateaus. All hyperparameters are reported in Appendix D, along with an ablation study.Word-level language modeling. For word-level language modeling, we use PTB and WT103. The results on PTB are listed in . TrellisNet sets a new state of the art on PTB, both with and without mixture of softmaxes , outperforming all previously published results by more than one unit of perplexity.WT103 is 110 times larger than PTB, with vocabulary size 268K. We follow prior work and use the adaptive softmax , which improves memory efficiency by assigning higher capacity to more frequent words. The results are listed in . TrellisNet sets a new state of the art on this dataset as well, with perplexity 29.19: about 7.6% better than the contemporaneous   3.0M 1.31 Independently RNN  12.0M 1.23 Hyper LSTM  14.4M 1.219 NAS Cell  16.3M 1.214 Fast-Slow-LSTM-2  7.2M1.19 Quasi-RNN  13.8M 1.187 AWD-LSTM  13.8M 1.175 Ours -TrellisNet 13.4M 1.158 Dilated GRU  99.0 94.6 -IndRNN  99.0 96.0 -Generic TCN  99.0 97.2 r-LSTM w/ Aux.  98.4 95.2 72.2 Transformer (self-attention)  98.9 97.9 62.2 Ours -TrellisNet 99.20 98.13 73.42self-attention-based Relational Memory Core (RMC) . TrellisNet achieves this better accuracy with much faster convergence: 25 epochs, versus 90 for RMC.Character-level language modeling. When used for character-level modeling, PTB is a mediumscale dataset with stronger long-term dependencies between characters. We thus use a deeper network as well as techniques such as weight normalization  and deep supervision . The results are listed in . TrellisNet sets a new state of the art with 1.158 bpc, outperforming the recent results of Merity et al. (2018a) by a comfortable margin.Long-range modeling with Sequential MNIST, PMNIST, and CIFAR-10. We also evaluate the TrellisNet for ability to model long-term dependencies. In the Sequential MNIST, PMNIST, and CIFAR-10 tasks, images are processed as long sequences, one pixel at a time . Our model has 8M parameters, in alignment with prior work. To cover the larger context, we use dilated convolutions in intermediate layers, adopting a common architectural element from TCNs . The results are listed in . Note that the performance of prior models is inconsistent. The Transformer works well on MNIST but fairs poorly on CIFAR-10, while r-LSTM with unsupervised auxiliary losses achieves good results on CIFAR-10 but underperforms on Permuted MNIST. TrellisNet outperforms all these models on all three tasks.\n###\n"}
{"text": "#Properties\nHas value, Global Mean Sea level Rise Projection, has lower limit for likely range, has upper limit for likely range, has  start of period, has end of period, climate scenario, has research problem, has unit\n#Text\nSea level has been steadily rising over the past century, predominantly due to anthropogenic climate change. The rate of sea level rise will keep increasing with continued global warming, and, even if temperatures are stabilized through the phasing out of greenhouse gas emissions, sea level is still expected to rise for centuries. This will affect coastal areas worldwide, and robust projections are needed to assess mitigation options and guide adaptation measures. Here we combine the equilibrium response of the main sea level rise contributions with their last century's observed contribution to constrain projections of future sea level rise. Our model is calibrated to a set of observations for each contribution, and the observational and climate uncertainties are combined to produce uncertainty ranges for 21st century sea level rise. We project anthropogenic sea level rise of 28-56 cm, 37-77 cm, and 57-131 cm in 2100 for the greenhouse gas concentration scenarios RCP26, RCP45, and RCP85, respectively. Our uncertainty ranges for total sea level rise overlap with the process-based estimates of the Intergovernmental Panel on Climate Change. The \"constrained extrapolation\" approach generalizes earlier global semiempirical models and may therefore lead to a better understanding of the discrepancies with processbased projections.sea level rise | climate change | climate impacts S ea level has been rising between 16 and 19 cm since 1900 2) with a rate of around 3 cm per decade since 1990 . Thermal expansion of the oceans and retreating glaciers are the main contributors to sea level rise in the past century and the near future. On multicentennial timescales, the Greenland and Antarctic ice sheets will likely dominate global sea level rise . Future sea level rise will pose challenges to coastal regions around the globe, and robust projections are needed to guide adaptation investment and provide incentives for climate mitigation .Projecting sea level relies on the understanding of the processes that drive sea level changes and on reliable data to verify and calibrate models. So-called process-based models now deliver projections for the main components of climate-driven sea level rise-thermal expansion, glaciers and ice caps, the Greenland ice sheet, and the Antarctic ice sheet-although solid ice discharge (SID) from the ice sheets is still difficult to constrain . Semiempirical models follow a different approach and use the statistical relation between global mean temperature or radiative forcing and sea level from past observations. Without aiming to capture the full physics of the sea level components, they project future sea level assuming that the past statistical relation also holds in the future. Their simpler nature makes them feasible for probabilistic assessments and makes their results easier to reproduce.The long-term multicentennial to millennial sensitivity of the main individual sea level contributors to global temperature changes can be constrained by paleoclimatic data and is more easily computed with currently available process-based large-scale models than are decadal to centennial variations . In addition, there is an increasing number of observations available for the historical individual contributions to sea level rise, which capture the early response to global temperature changes.Here we seek to combine the long-term sensitivity (or longterm commitment) and the individual observations to constrain estimates of near-future sea level rise by semiempirical relations for each sea level contributor. This expands the classical semiempirical approach that has so far been based on total sea level rise. We use a pursuit curve to estimate sea level rise in accordance with the respective long-term sensitivity. We define S(t) as the time-dependent sea level contribution, S eq (T, \u03b1) is the long-term sensitivity for the sea level component as a function of global mean temperature T and the commitment factor \u03b1 (see methods), and \u03c4 is the response timescale. We can then model the short-term rate of sea level rise as a function of global mean temperature asThis ordinary differential equation describes a physical system in which S seeks to approach its equilibrium value (here S eq ) with speed linearly dependent on the deviation from the equilibrium and the inverse of \u03c4. The approach has already been applied to project total sea level rise . The integrated equation yields the sea level evolution. Uncertainty in the long-term sensitivity S eq is covered by variation of commitment parameters. We calibrate \u03c4 SignificanceAnthropogenic sea level rise poses challenges to coastal areas worldwide, and robust projections are needed to assess mitigation options and guide adaptation measures. Here we present an approach that combines information about the equilibrium sea level response to global warming and last century's observed contribution from the individual components to constrain projections for this century. This \"constrained extrapolation\" overcomes limitations of earlier global semiempirical estimates because long-term changes in the partitioning of total sea level rise are accounted for. While applying semiempirical methodology, our method yields sea level projections that overlap with the process-based estimates of the Intergovernmental Panel on Climate Change. The method can thus lead to a better understanding of the gap between process-based and global semiempirical approaches.by minimizing the sum of the squared residuals (\"least-squares\") between observed and modeled sea level evolution for the past for each contributor and each observational dataset. ResultsThermal Expansion. Past thermosteric sea level rise can be inferred from observations of ocean temperature that are available for several ocean depth ranges ). The upper ocean layer (0\u2212700 m) is best observed . Fewer observations are available for the middepth and abyssal ocean . To encompass the uncertainty from the different observational datasets, we create all possible combinations of the observations from different depths to yield 12 estimates for total thermosteric sea level rise (see Supporting Information for details). For the given range of commitment factors, our calibration method yields equilibration times \u03c4 between 82 and 1,290 y ). Driven by observed global mean temperature change , our model can reproduce the different time series of observed thermosteric sea level rise (see for a subset and for the full set). The estimates for the full time period since 1900 encompass the Coupled Model Intercomparison Project Phase 5 (CMIP5) model mean (3)\n###\n", "summary": " Has value: 0.85\n, Global Mean Sea level Rise Projection: Global Mean Sea Level Rise Projections\n, has  start of period: 1986-2005\n, has end of period: 2081-2100\n, climate scenario: RCP8.5\n, has research problem: Global Mean Sea Level Rise Projections\n, has unit: m\n###"}
{"summary": "has benchmark: Benchmark SQuAD1.1/Benchmark TriviaQA\nhas research problem: Question Answering/Question Answering\nhas model: BiDAF + Self Attention (single model)/S-Norm\nsame as: https://en.wikipedia.org/wiki/Question_answering/https://en.wikipedia.org/wiki/Question_answering", "text": "#Properties\nhas benchmark, has research problem, has model, same as\n#Text\nWe consider the problem of adapting neural paragraph-level question answering models to the case where entire documents are given as input. Our proposed solution trains models to produce well calibrated confidence scores for their results on individual paragraphs. We sample multiple paragraphs from the documents during training, and use a sharednormalization training objective that encourages the model to produce globally correct output. We combine this method with a stateof-the-art pipeline for training models on document QA data. Experiments demonstrate strong performance on several document QA datasets. Overall, we are able to achieve a score of 71.3 F1 on the web portion of Triv-iaQA, a large improvement from the 56.7 F1 of the previous best system. Pipelined MethodIn this section we propose an approach to training pipelined question answering systems, where a single paragraph is heuristically extracted from the context document(s) and passed to a paragraphlevel QA model. We suggest using a TF-IDF based paragraph selection method and argue that a summed objective function should be used to handle noisy supervision. We also propose a refined model that incorporates some recent modeling ideas for reading comprehension systems. Paragraph SelectionOur paragraph selection method chooses the paragraph that has the smallest TF-IDF cosine distance with the question. Document frequencies are computed using just the paragraphs within the relevant documents, not the entire corpus. The advantage of this approach is that if a question word is prevalent in the context, for example if the word \"tiger\" is prevalent in the document(s) for the question \"What is the largest living subspecies of the tiger?\", greater weight will be given to question words that are less common, such as \"largest\" or \"sub-species\". Relative to selecting the first paragraph in the document, this improves the chance of the selected paragraph containing the correct answer from 83.1% to 85.1% on Triv-iaQA web. We also expect this approach to do a better job of selecting paragraphs that relate to the question since it is explicitly selecting paragraphs that contain question words. Handling Noisy LabelsQuestion: Which British general was killed at Khartoum in 1885? Answer: Gordon Context: In February 1885 Gordon returned to the Sudan to evacuate Egyptian forces. Khartoum came under siege the next month and rebels broke into the city, killing Gordon and the other defenders. The British public reacted to his death by acclaiming 'Gordon of Khartoum', a saint. However, historians have suggested that Gordon defied orders and refused to evacuate... : Noisy supervision causes many spans of text that contain the answer, but are not situated in a context that relates to the question, to be labelled as correct answer spans (highlighted in red). This risks distracting the model from learning from more relevant spans (highlighted in green).In a distantly supervised setup we label all text spans that match the answer text as being correct. This can lead to training the model to select unwanted answer spans.  contains an example. To handle this difficulty, we use a summed objective function similar to the one from , that optimizes the sum of the probabilities of all answer spans. The models we consider here work by independently predicting the start and end token of the answer span, so we take this approach for both predictions. Thus the objective for the span start boundaries becomes:where A is the set of tokens that start an answer span, n is the number of context tokens, and s i is a scalar score computed by the model for span i. This optimizes the negative log-likelihood of selecting any correct start token. This objective is agnostic to how the model distributes probability mass across the possible answer spans, thus the model can \"choose\" to focus on only the more relevant spans. We use a model with the following layers (shown in ): ModelEmbedding: We embed words using pretrained word vectors. We also embed the characters in each word into size 20 vectors which are learned, and run a convolution neural network followed by max-pooling to get character-derived embeddings for each word. The character-level and word-level embeddings are then concatenated and passed to the next layer. We do not update the word embeddings during training.Pre-Process: A shared bi-directional GRU  is used to map the question and passage embeddings to contextaware embeddings.Attention: The bi-directional attention mechanism from the Bi-Directional Attention Flow (BiDAF) model  is used to build a query-aware context representation. Let h i be the vector for context word i, q j be the vector for question word j, and n q and n c be the lengths of the question and context respectively. We compute attention between context word i and question word j as:where w 1 , w 2 , and w 3 are learned vectors and is element-wise multiplication. We then compute an attended vector c i for each context token as:We also compute a query-to-context vector q c :The final vector computed for each token is built by concatenating h i , c i , h i c i , and q c c i . In our model we subsequently pass the result through a linear layer with ReLU activations.Self-Attention: Next we use a layer of residual self-attention. The input is passed through another bi-directional GRU. Then we apply the same attention mechanism, only now between the passage and itself. In this case we do not use query-tocontext attention and we set a ij = \u2212inf if i = j.As before, we pass the concatenated output through a linear layer with ReLU activations. This layer is applied residually, so this output is additionally summed with the input.Prediction: In the last layer of our model a bidirectional GRU is applied, followed by a linear layer that computes answer start scores for each token. The hidden states of that layer are concatenated with the input and fed into a second bidirectional GRU and linear layer to predict answer end scores. The softmax operation is applied to the start and end scores to produce start and end probabilities, and we optimize the negative loglikelihood of selecting correct start and end tokens.Dropout: We also employ variational dropout, where a randomly selected set of hidden units are set to zero across all time steps during training . We dropout the input to all the GRUs, including the word embeddings, as well as the input to the attention mechanisms, at a rate of 0.2. Confidence MethodWe adapt this model to the multi-paragraph setting by using the un-normalized and un-exponentiated (i.e., before the softmax operator is applied) score given to each span as a measure of the model's confidence. For the boundary-based models we use here, a span's score is the sum of the start and end score given to its start and end token. At test time we run the model on each paragraph and select the answer span with the highest confidence. This is the approach taken by .Applying this approach without altering how the model is trained is, however, a gamble; the training objective does not require these confidence scores to be comparable between paragraphs. Our experiments in Section 5 show that in practice these models can be very poor at providing good confidence scores.  shows some qualitative examples of this phenomenon.We hypothesize that there are two key reasons a model's confidence scores might not be well calibrated. First, for models trained with the softmax objective, the pre-softmax scores for all spans can be arbitrarily increased or decreased by a constant value without changing the resulting softmax probability distribution. As a result, nothing prevents models from producing scores that are arbitrarily all larger or all smaller for one paragraph than another. Second, if the model only sees paragraphs that contain answers, it might become too confident in heuristics or patterns that are only effective when it is known a priori that an answer exists. For example, in  we observe that the model will assign high confidence values to spans that strongly match the category of the answer, even if the question words do not match the context. This might work passably well if an answer is present, but can lead to highly over-confident extractions in other cases. Similar kinds of errors have been observed when distractor sentences are added to the context .We experiment with four approaches to training models to produce comparable confidence scores, shown in the follow subsections. In all cases we will sample paragraphs that do not contain an answer as additional training points. Shared-NormalizationIn this approach all paragraphs are processed independently as usual. However, a modified objective function is used where the normalization factor in the softmax operation is shared between all paragraphs from the same context. Therefore, the probability that token a from paragraph p starts an answer span is computed as:where P is the set of paragraphs that are from the same context as p, and s ij is the score given to token i from paragraph j. We train on this objective by including multiple paragraphs from the same context in each mini-batch. This is similar to simply feeding the model multiple paragraphs from each context concatenated together, except that each paragraph is processed independently until the normalization step. The key idea is that this will force the model to produce scores that are comparable between paragraphs, even though it does not have access to information about the other paragraphs being considered. MergeAs an alternative to the previous method, we experiment with concatenating all paragraphs sampled from the same context together during training. A paragraph separator token with a learned embedding is added before each paragraph. Our motive is to test whether simply exposing the model to more text will teach the model to be more adept at ignoring irrelevant text. No-Answer OptionWe also experiment with allowing the model to select a special \"no-answer\" option for each paragraph. First, note that the independent-bounds objective can be re-written as:where s j and g j are the scores for the start and end bounds produced by the model for token j, and a and b are the correct start and end tokens. We have the model compute another score, z, to represent ...many species of lobates have four auricles, gelatinous projections edged with cilia that produce water currents that help direct microscopic prey toward the mouth...The Cestida are ribbon -shaped planktonic animals, with the mouth and aboral organ aligned in the middle of opposite edges of the ribbon : Examples from SQuAD where a paragraph-level model was less confident in a correct extraction from one paragraph (left) than in an incorrect extraction from another (right). Even if the passage has no correct answer, the model still assigns high confidence to phrases that match the category the question is asking about. Because the confidence scores are not well-calibrated, this confidence is often higher than the confidence assigned to the correct answer span.the weight given to a \"no-answer\" possibility. Our revised objective function becomes:where \u03b4 is 1 if an answer exists and 0 otherwise. If there are multiple answer spans we use the same objective, except the numerator includes the summation over all answer start and end tokens.We compute z by adding an extra layer at the end of our model. We compute a soft attention over the span start scores, p i = e s i n j=1 e s j , and then take the weighted sum of the hidden states from the GRU used to generate those scores,We compute a second vector, v 2 in the same way using the end scores. Finally, a step of learned attention is performed on the output of the Self-Attention layer that computes:where w is a learned weight vector and h i is the vector for token i.We concatenate these three vectors and use them as input to a two layer network with an 80 dimensional hidden layer and ReLU activations that produces z as its only output. SigmoidAs a final baseline, we consider training models with the sigmoid loss objective function. That is, we compute a start/end probability for each token in the context by applying the sigmoid function to the start/end scores of each token. A cross entropy loss is used on each individual probability. The intuition is that, since the scores are being evaluated independently of one another, they will be comparable between different paragraphs. Experimental Setup DatasetsWe evaluate our approach on three datasets: Triv-iaQA unfiltered , a dataset of questions from trivia databases paired with documents found by completing a web search of the questions; TriviaQA web, a dataset derived from TriviaQA unfiltered by treating each questiondocument pair where the document contains the question answer as an individual training point; and SQuAD , a collection of Wikipedia articles and crowdsourced questions. PreprocessingWe note that for TriviaQA web we do not subsample as was done by , instead training on the full 530k question-document training pairs. We also observed that the metrics for TriviaQA are computed after applying a small amount of text normalization (stripping punctuation, removing articles, ect.) to both the ground truth text and the predicted text. As a result, some spans of text that would have been considered an exact match after normalization were not marked as answer spans during preprocessing, which only detected exact string matches. We fix this issue by labeling all spans of text that would have been considered an exact match by the official evaluation script as an answer span.In TriviaQA, documents often contain many small paragraphs, so we merge paragraphs together as needed to get paragraphs of up to a target size. We use a maximum size of 400 unless stated otherwise. Paragraph separator tokens with learned embeddings are added between merged paragraphs to preserve formatting information. SamplingOur confidence-based approaches are all trained by sampling paragraphs, including paragraphs that do not contain an answer, during training. For SQuAD and TriviaQA web we take the top four paragraphs ranked by TF-IDF score for each question-document pair. We then sample two different paragraphs from this set each epoch. Since we observe that the higher-ranked paragraphs are much more likely to contain the context needed to answer the question, we sample the highest ranked paragraph that contains an answer twice as often as the others. For the merge and shared-norm approaches, we additionally require that at least one of the paragraphs contains an answer span.For TriviaQA unfiltered, where we have multiple documents for each question, we find it beneficial to use a more sophisticated paragraph ranking function. In particular, we use a linear function with five features: the TF-IDF cosine distance, whether the paragraph was the first in its document, how many tokens occur before it, and the number of case insensitive and case sensitive matches with question words. The function is trained on the distantly supervised objective of selecting paragraphs that contain at least one answer span. We select the top 16 paragraphs for each question and sample pairs of paragraphs as before. ImplementationWe train the model with the Adadelta optimizer (Zeiler, 2012) with a batch size 60 for Triv-iaQA and 45 for SQuAD. At test time we select the most probable answer span of length less than Model EM F1 baseline  : Results on TriviaQA web using our pipelined method. We significantly improve upon the baseline by combining the preprocessing procedures, TF-IDF paragraph selection, the sum objective, and our model  or equal to 8 for TriviaQA and 17 for SQuAD. The GloVe 300 dimensional word vectors released by  are used for word embeddings. On SQuAD, we use a dimensionality of size 100 for the GRUs and of size 200 for the linear layers employed after each attention mechanism. We find for TriviaQA, likely because there is more data, using a larger dimensionality of 140 for each GRU and 280 for the linear layers is beneficial. During training, we maintain an exponential moving average of the weights with a decay rate of 0.999. We use the weight averages at test time. Results TriviaQA WebFirst, we do an ablation study on TriviaQA web to show the effects of our proposed methods for our pipeline model. We start with an implementation of the baseline from . Their system selects paragraphs by taking the first 400 tokens of each document, uses BiDAF  as the paragraph model, and selects a random answer span from each paragraph each epoch to be used in BiDAF's cross entropy loss function during training. Paragraphs of size 800 are used at test time. As shown in , our implementation of this approach outperforms the results reported by  significantly, likely because we are not subsampling the data. We find both TF-IDF ranking and the sum objective to be effective; even without changing the model we achieve state-of-the-art results. Using our refined model increases the gain by another 4 points.Next we show the results of our confidencebased approaches. In this setting we group each document's text into paragraphs of at most 400 tokens and rank them using our TF-IDF heuristic. Then we measure the performance of our proposed  : Results on TriviaQA web (left) and verified TriviaQA web (right) when applying our models to multiple paragraphs from each document. The shared-norm, merge, and no-answer training methods improve the model's ability to utilize more text, with the shared-norm method being significantly ahead of the others on the verified set and tied with the merge approach on the general set. ModelAll Verified EM F1 EM F1 baseline  40.74 47.06 49.54 55.80 MEMEN*  43.16 46.90 49.28 55.83 Mnemonic Reader  46.94 52.85 54.45 59.46 Reading Twice for NLU    approaches as the model is used to independently process an increasing number of these paragraphs and the model's most confident answer is returned. We additionally measure performance on the verified portion of TriviaQA, a small subset of the question-document pairs in TriviaQA web where humans have manually verified that the document contains sufficient context to answer the question. The results are shown in .On these datasets even the model trained without any of the proposed training methods (\"none\") improves as it is allowed to use more text, showing it does a passable job at focusing on the correct paragraph. The no-answer option training approach lead to a significant improvement, and the shared-norm and merge approach are even better. On the verified set, the shared-norm approach is solidly ahead of the other options. This suggests the shared-norm model is better at extracting answers when it is clearly stated in the text, but worse at guessing the answer in other cases.We use the shared-norm approach for evaluation on the TriviaQA test set. We found that increasing the paragraph size to 800 at test time, and re-training the model on paragraphs of size 600, was slightly beneficial, allowing our model to reach 66.04 EM and 70.98 F1 on the dev set. We submitted this model to be evaluated on the Triv-iaQA test set and achieved 66.37 EM and 71.32 F1, firmly ahead of prior work, as shown in Table 3. Note that human annotators have estimated that only 75.4% of the question-document pairs contain sufficient evidence to answer the question , which suggests we are approaching the upper bound for this task. However, the score of 83.7 F1 on the verified set suggests that there is still room for improvement. TriviaQA UnfilteredNext we apply our confidence methods to Trivi-aQA unfiltered. This dataset is of particular interest because the system is not told which document contains the answer, so it provides a plausible simulation of attempting to answer a question using a document retrieval system. We show the same graph as before for this dataset in . On this dataset it is more important to train the model to produce well calibrated confidence scores. Note the base model starts to lose performance as more paragraphs are used, showing that errors are being caused by the model being overly confident in incorrect extractions.  : Results for our confidence methods on Triv-iaQA unfiltered. Here we see a more dramatic difference between these models. The shared-norm approach is the strongest, while the base model starts to lose performance as more paragraphs are used. SQuADWe additionally evaluate our model on SQuAD. SQuAD questions were not built to be answered independently of their context paragraph, which makes it unclear how effective of an evaluation tool they can be for document-level question answering. To assess this we manually label 500 random questions from the training set. We categorize questions as:1. Context-independent, meaning it can be understood independently of the paragraph.2. Document-dependent, meaning it can be understood given the article's title. For example, \"What individual is the school named after?\" for the document \"Harvard University\".3. Paragraph-dependent, meaning it can only be understood given its paragraph. For example, \"What was the first step in the reforms?\".  : Results for our confidence methods on document-level SQuAD. The base model does poorly in this case, rapidly losing performance once more than two paragraphs are used. While all our approaches had some benefit, the shared-norm model is the strongest, and is the only one to not lose performance as large numbers of paragraphs are used.We find 67.4% of the questions to be contextindependent, 22.6% to be document-dependent, and the remaining 10% to be paragraphdependent. The many document-dependent questions stem from the fact that questions are frequently about the subject of the document, so the article's title is often sufficient to resolve coreferences or ambiguities that appear in the question. Since a reasonably high fraction of the questions can be understood given the document they are from, and to isolate our analysis from the retrieval mechanism used, we choose to evaluate on the document-level. We build documents by concatenating all the paragraphs in SQuAD from the same article together into a single document.The performance of our models given the correct paragraph (i.e., in the standard SQuAD setting), is shown in . Our paragraph-level model is competitive on this task, and our variations to handle the multi-paragraph setting only cause a minor loss of performance.We graph the document-level performance in . For SQuAD, we find it crucial to employ one of the suggested confidence training techniques. The base model starts to drop in performance once more than two paragraphs are used. However, the shared-norm approach is able to reach a peak performance of 72.37 F1 and 64.08 EM given 15 paragraphs. Given our estimate that 10% of the questions are ambiguous if the paragraph is unknown, our approach appears to have adapted to the document-level task very well.Finally, we compare the shared-norm model with the document-level result reported by . We re-evaluate our model using the documents used by , which consist of the same Wikipedia articles SQuAD was built from, but downloaded at different dates. The advantage of this dataset is that it does not allow the model to know a priori which paragraphs were filtered out during the construction of SQuAD. The disadvantage is that some of the articles have been edited since the questions were written, so some questions may no longer be answerable. Our model achieves 59.14 EM and 67.34 F1 on this dataset, which significantly outperforms the 49.7 EM reported by . ConclusionWe have shown that, when using a paragraph-level QA model across multiple paragraphs, our training method of sampling non-answer containing paragraphs while using a shared-norm objective function can be very beneficial. Combining this with our suggestions for paragraph selection, using the summed training objective, and our model design allows us to advance the state of the art on TriviaQA by a large stride. As shown by our demo, this work can be directly applied to building deep learning powered open question answering systems.\n###\n"}
{"summary": "Summarization Type: Abstractive\nMachine Learning Paradigm: Unsupervised\ndataset: AMI Meeting Corpus\nFuture work: design an end-to-end framework\nHas preprocessing steps: Topic segmentation, anaphora resolution, pronoun resolution, ambiguity resolver\nData Size: 139 meetings\nData Language: English\nData Domain: Different\nAutomatic Evaluation Metrics: ROUGE-2, ROUGE-SU4, LL\nAlgorithm and Techniques: Integer Linear Programming (ILP) fusion", "text": "#Properties\nSummarization Type, Machine Learning Paradigm, dataset, Future work, Has preprocessing steps, Data Size, Data Language, Data Domain, Automatic Evaluation Metrics, Algorithm and Techniques\n#Text\nAutomatic summarization techniques on meeting conversations developed so far have been primarily extractive, resulting in poor summaries. To improve this, we propose an approach to generate abstractive summaries by fusing important content from several utterances. Any meeting is generally comprised of several discussion topic segments. For each topic segment within a meeting conversation, we aim to generate a one sentence summary from the most important utterances using an integer linear programming-based sentence fusion approach. Experimental results show that our method can generate more informative summaries than the baselines. PROPOSED APPROACHDependency fusion on meeting data requires an algorithm that is robust for noisy data as utterances often have disfluencies. Our work applies fusion to all the important utterances within the topic segment to generate the best sub-tree that satisfies the constraints and maximizes the objective function of the optimization problem. Anaphora resolution step replaces pronouns with the original nouns in the previous utterance that they refer to in order to increase the chances of merging. Consider the following utterances:\"so we're designing a new remote control and um\" \"Um, as you can see it's supposed to be original\"Without pronoun resolution, these two utterances cannot be merged. Once we apply anaphora resolution, it in the second utterance is modified to a new remote control and then both the utterances are fused into a common structure. The utterances are parsed using the Stanford dependency parser. Every individual utterance has an explicit ROOT node. We add two dummy nodes in the graph -the start node and the end node to ensure defined start and end points of the merged structure. The words from the utterances are iteratively added onto the graph. The words that have the same word form and POS tag are assigned to the same nodes. Ambiguity resolver. Suppose that a new word wi that has k ambiguous nodes where it can be mapped to. The k ambiguous nodes are referred to as mappable nodes. For every ambiguous mapping candidate, we first find the words to the left and right of the mappable node of the sentences, and then compute the number of words in both the directions that are common to the words in either direction of the word wi. Finally, wi is mapped to the node that has the highest directed context. ILP formulation.  shows the sub-graph (marked using blue bold arrows) that we wish to retain from the merged graph structure to generate a one-sentence summary from several merged utterances. All the sentences generated from each meeting transcript are concatenated to produce the final abstractive summary. We need to maximize the information content of the generated sentence, keeping it grammatical. We model the problem as an integer linear programming (ILP) formulation, similar to the dependency graph fusion as proposed by Fillipova and Strube . The directed edges in the graph (binary variables) are represented as x g,d,l , where g, d and l denote the governor node, dependent node and the label of an edge, respectively. We maximize the following objective function:As shown in Equation (1), we introduce three different terms: p(l | g), I(d) and px N . Each relation in a dependency graph consists of the governing node, the dependent node and the relation type. The term p(l | g) denotes the probabilities of the labels given a governor node, g. For every node (word and POS) in the entire corpus, the probabilities are represented as the ratio of the sum of the frequency of a particular label and the sum of the frequencies of all the labels emerging from a node. In this work, we calculate  these values using Reuters corpora  to obtain dominant relations from non-conversational style of text. For example,  shows the probabilities of outgoing edges from a node, (produced/VBN). This term assigns the importance of grammatical relations to a node and only the relations that are more dominant from a node will be preferred. The term I(d) denotes the informativeness of a node calculated using Hori and Furui's formula . The last term in Equation is based on the idea of lexical cohesion. Towards the end of any segment, generally, more important discussions might happen that will conclude a particular topic and then start another.In order to take this fact into account, we introduce the term px N , where N and px denote the total number of extracted utterances in a segment and the position of the utterance (the edge x belongs to) in the set of N utterances, respectively.In order to solve the above ILP problem, we impose a number of constraints. Some of the constraints have been directly adapted from the original ILP formulation . For example, we use the same constraints for restricting one incoming edge per node, as well as we impose the connectivity constraint to ensure a connected graph structure. Further, we restrict the subtree to have just one start edge and one end edge. This helps in preserving one ROOT node, as well as it limits to one end node for the generated subtree. We also limit the generated subtree to have a maximum of 15 nodes that controls the length of the summary sentence. We also add few linguistic constraints that ensure the coherence of the output such as every node can have maximum of one determinant, etc. We also impose constraints to prevent cycles in the graph structure, otherwise finding the best path from start and end nodes might be difficult. The final graph is linearized to obtain a coherent sentence. In the linearization process, we order the nodes based on their original ordering in the utterance. EXPERIMENTAL RESULTSThe AMI Meeting corpus contains 20 meeting transcripts in the test set along with their corresponding abstractive (human-written) summaries as well as the annotations of topic segments. ROUGE is used to compare content selection of several approaches. We compared the content selection of our approach to an extractive summarizer , which works as a baseline. We also compared our model without using anaphora resolution to see the impact of resolving pronouns. All the summaries were compared against the human-written summaries as reference. The results in  show that our method outperforms the other techniques on both ROUGE-2 (R2) and ROUGE-SU4 (R-SU4) recall scores. Moreover, we computed a coarse estimate of grammaticality using the log-likelihood score (LL) from the parser. Our technique significantly outperforms the extractive method. In future work, we plan to design an end-to-end framework for summary generation from meetings.\n###\n"}
{"summary": "has benchmark: Benchmark WikiText-2/Benchmark WikiText-2/Benchmark WikiText-103/Benchmark WikiText-103/Benchmark WikiText-103\nhas research problem: Language Modelling/Language Modelling/Language Modelling/Language Modelling/Language Modelling", "text": "#Properties\nhas benchmark, has research problem\n#Text\nWe propose an extension to neural network language models to adapt their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language model datasets that our approach performs significantly better than recent memory augmented networks. LANGUAGE MODELINGA language model is a probability distribution over sequences of words. Let V be the size of the vocabulary; each word is represented by a one-hot encoding vector x in R V = V, corresponding to its index in the vocabulary. Using the chain rule, the probability assigned to a sequence of words x 1 , . . . , x T can be factorized asLanguage modeling is often framed as learning the conditional probability over words, given the history .This conditional probability is traditionally approximated with non-parameteric models based on counting statistics ). In particular, smoothed N-gram models  achieve good performance in practice . Parametrized alternatives are either maximum entropy language models , feedforward networks  or recurrent networks . In particular, recurrent networks are currently the best solution to approximate this conditional probability, achieving state-of-the-arts performance on standard language modeling benchmarks .Recurrent networks. Assuming that we have a vector h t \u2208 R d encoding the history x t , ..., x 1 , the conditional probability of a word w can be parametrized asThe history vector h t is computed by a recurrent network by recursively applying an equation of the formwhere \u03a6 is a function depending on the architecture of the network. Several architecture for recurrent networks have been proposed, such as the Elman network (Elman, 1990), the long short-term memory (LSTM)  or the gated recurrent unit (GRU) . One of the simplest recurrent networks is the Elman network , wherewhere \u03c3 is a non-linearity such as the logistic or tanh functions, L \u2208 R d\u00d7V is a word embedding matrix and R \u2208 R d\u00d7d is the recurrent matrix. The LSTM architecture is particularly interesting in the context of language modelling  and we refer the reader to  for details on this architecture.The parameters of recurrent neural network language models are learned by minimizing the negative log-likelihood of the training data. This objective function is usually minimized by using the stochastic gradient descent algorithm, or variants such as Adagrad . The gradient is computed using the truncated backpropagation through time algorithm .Cache model. After a word appears once in a document, it is much more likely to appear again. As an example, the frequency of the word tiger on the Wikipedia page of the same name is 2.8%, compared to 0.0037% over the whole Wikipedia. Cache models exploit this simple observation to improve n-gram language models by capturing long-range dependencies in documents. More precisely, these models have a cache component, which contains the words that appeared in the recent history (either the document or a fixed number of words). A simple language model, such as a unigram or smoothed bigram model, is fitted on the words of the cache and interpolated with the static language model (trained over a larger dataset). This technique has many advantages. First, this is a very efficient way to adapt a language model to a new domain. Second, such models can predict out-of-vocabulary words (OOV words), after seeing them once. Finally, this helps capture long-range dependencies in documents, in order to generate more coherent text. : The neural cache stores the previous hidden states in memory cells. They are then used as keys to retrieve their corresponding word, that is the next word. There is no transformation applied to the storage during writing and reading. NEURAL CACHE MODELThe Neural Cache Model adds a cache-like memory to neural network language models. It exploits the hidden representations h t to define a probability distribution over the words in the cache. As illustrated , the cache stores pairs (h i , x i+1 ) of a hidden representation, and the word which was generated based on this representation (we remind the reader that the vector h i encodes the history x i , ..., x 1 ). At time t, we then define a probability distribution over words stored in the cache based on the stored hidden representations and the current one h t aswhere the scalar \u03b8 is a parameter which controls the flatness of the distribution. When \u03b8 is equal to zero, the probability distribution over the history is uniform, and our model is equivalent to a unigram cache model .From the point of view of memory-augmented neural networks, the probabilitygiven by the neural cache model can be interpreted as the probability to retrieve the word w from the memory given the query h t , where the desired answer is the next word x t+1 . Using previous hidden states as keys for the words in the memory, the memory lookup operator can be implemented with simple dot products between the keys and the query. In contrast to existing memory-augmented neural networks, the neural cache model avoids the need to learn the memory lookup operator. Such a cache can thus be added to a pre-trained recurrent neural language model without fine tuning of the parameters, and large cache size can be used with negligible impact on the computational cost of a prediction.Neural cache language model. Following the standard practice in n-gram cache-based language models, the final probability of a word is given by the linear interpolation of the cache language model with the regular language model, obtaining:Instead of taking a linear interpolation between the two distribution with a fixed \u03bb, we also consider a global normalization over the two distribution:This corresponds to taking a softmax over the vocabulary and the words in the cache. The parameter \u03b1 controls the weight of the cache component, and is the counterpart of the \u03bb parameter for linear interpolation.The addition of the neural cache to a recurrent neural language model inherits the advantages of ngram caches in usual cache-based models: The probability distribution over words is updated online depending on the context, and out-of-vocabulary words can be predicted as soon as they have been seen at least once in the recent history. The neural cache also inherits the ability of the hidden states of recurrent neural networks to model longer-term contexts than small n-grams, and thus allows for a finer modeling of the current context than e.g., unigram caches. We use a cache model of size 500. The base model has a validation perplexity of 86.9. The best linear interpolation has a perplexity of 74.6, while the best global normalization has a perplexity of 74.9. Model Test PPLRNN+LSA+KN5+cache  90.3 LSTM  78.4 Variational LSTM  73.4 Recurrent Highway Network  66.0 Pointer Sentinel LSTM  70.9LSTM (our implem.) 82.3 Neural cache model 72.1 Training procedure. For now, we first train the (recurrent) neural network language model, without the cache component. We only apply the cache model at test time, and choose the hyperparameters \u03b8 and \u03bb (or \u03b1) on the validation set. A big advantage of our method is that it is very easy and cheap to apply, with already trained neural models. There is no need to perform backpropagation over large contexts, and we can thus apply our method with large cache sizes (larger than one thousand). Model wikitext2 wikitext103Zoneout + Variational LSTM  100.9 -Pointer Sentinel LSTM  80.  This approach has been successfully applied to question answering, when the answer is contained in a given paragraph . Similarly,  explores the use of this mechanism to reorder sequences of tokens. Their network uses an attention (or \"pointer\") over the input sequence to predict which element should be selected as the next output.  have shown that a similar mechanism called pointer softmax could be used in the context of machine translation, to decide which word to copy from the source to target.Independently of our work,  apply the same mechanism to recurrent network. Unlike our work, they uses the current hidden activation as a representation of the current input (while we use it to represent the output). This requires additional learning of a transformation between the current representation and those in the past. The advantage of our approach is that we can scale to very large caches effortlessly. EXPERIMENTSIn this section, we evaluate our method on various language modeling datasets, which have different sizes and characteristics. On all datasets, we train a static recurrent neural network language model with LSTM units. We then use the hidden representations from this model to obtain our cache, which is interpolated with the static LSTM model. We also evaluate a unigram cache model interpolated with the static model as another baseline. SMALL SCALE EXPERIMENTSDatasets. In this section, we describe experiments performed on two small datasets: the Penn Tree Bank  and the wikitext2  datasets. The Penn Tree Bank dataset is made of articles from the Wall Street Journal, contains 929k training tokens and has a vocabulary size of 10k. The wikitext2 dataset is derived from Wikipedia articles, contains 2M training tokens and has a vocabulary size of 33k. These datasets contain non-shuffled documents, therefore requiring models to capture inter-sentences dependencies to perform well.  : Test perplexity as a function of the number of words in the cache, for our method and a unigram cache baseline. We observe that our approach can uses larger caches than the baseline. Implementation details. We train recurrent neural network language models with 1024 LSTM units, regularized with dropout (probability of dropping out units equals to 0.65). We use the Adagrad algorithm, with a learning rate of 0.2, a batchsize of 20 and initial weight uniformly sampled in the range . We clip the norm of the gradient to 0.1 and unroll the network for 30 steps. We consider cache sizes on a logarithmic scale, from 50 to 10, 000, and fit the cache hyperparameters on the validation set.Results. We report the perplexity on the validation sets in , for various values of hyperparameters, for linear interpolation and global normalization. First, we observe that on both datasets, the linear interpolation method performs slightly better than the global normalization approach. It is also easier to apply in practice, and we thus use this method in the remainder of this paper. In , we report the test perplexity of our approach and state-of-the-art models. Our approach is competitive with previous models, in particular with the pointer sentinel LSTM model of . On Penn Tree Bank, we note that the improvement over the base model is similar for both methods. On the wikitext2 dataset, both methods obtain similar results when using the same cache size (100 words). Since our method is computationally cheap, it is easy to increase the cache to larger values (2, 000 words), leading to dramatic improvements (30% over the baseline, 12% over a small cache of 100 words). MEDIUM SCALE EXPERIMENTSDatasets and implementation details. In this section, we describe experiments performed over two medium scale datasets: text8 and wikitext103. Both datasets are derived from Wikipedia, but different pre-processing were applied. The text8 dataset contains 17M training tokens and has a vocabulary size of 44k words, while the wikitext103 dataset has a training set of size 103M, and a vocabulary size of 267k words. We use the same setting as in the previous section, except for the batchsize (we use 128) and dropout parameters (we use 0.45 for text8 and 0.25 for wikitext103). Since both datasets have large vocabularies, we use the adaptive softmax  for faster training.Results. We report the test perplexity as a function of the cache size in , for the neural cache model and a unigram cache baseline. We observe that our approach can exploits larger cache sizes, compared to the baseline. In , we observe that the improvement in perplexity of our method over the LSTM baseline on wikitext103 is smaller than for wikitext2 (approx. 16% v.s. 30%). The fact that improvements obtained with more advanced techniques decrease when the size of training data increases has already been observed by . Both wikitext datasets sharing the same test set, we also observe that the LSTM baseline, trained on 103M tokens (wikitext103), strongly outperforms more sophisticated methods, trained on 2M tokens (wikitext2). For these two reasons, we believe that it is important to evaluate and compare methods on relatively large datasets. Model TestLSTM-500  156 SCRNN  161 MemNN  147 LSTM-1024 (our implem.) 121.8 Neural cache model 99.9 (a) text8 Model Dev CtrlWB5  3125 285 WB5+cache  768 270 LSTM-512  5357 EXPERIMENTS ON THE LAMBADA DATASETFinally, we report experiments carried on the lambada dataset, introduced by . This is a dataset of short passages extracted from novels. The goal is to predict the last word of the excerpt. This dataset was built so that human subjects solve the task perfectly when given the full context (approx. 4.6 sentences), but fail to do so when only given the sentence with the target word. Thus, most state-of-the-art language models fail on this dataset. The lambada training set contains approximately 200M tokens and has a vocabulary size of 93, 215. We report results for our method in , as well the performance of baselines from . Adding a neural cache model to the LSTM baseline strongly improves the performance on the lambada dataset. We also observe in  that the best interpolation parameter between the static model and the cache is not the same for the development and control sets. This is due to the fact that more than 83% of passages of the development set include the target word, while this is true for only 14% of the control set. Ideally, a model should have strong results on both sets. One possible generalization of our model would be to adapt the interpolation parameter based on the current vector representation of the history h t . CONCLUSIONWe presented the neural cache model to augment neural language models with a longer-term memory that dynamically updates the word probablilities based on the long-term context. A neural cache can be added on top of a pre-trained language model at negligible cost. Our experiments on both language modeling tasks and the challenging LAMBADA dataset shows that significant performance gains can be expected by adding this external memory component.Technically, the neural cache models is similar to some recent memory-augmented neural networks such as pointer networks. However, its specific design makes it possible to avoid learning the memory lookup component. This makes the neural cache appealing since it can use larger cache sizes than memory-augment networks and can be applied as easily as traditional count-based caches.\n###\n"}
{"summary": "has research problem: how to effectively make use of relevant information while ignoring irrelevant information from the dependency trees\npropose: novel Attention Guided Graph Convolutional Networks (AGGCNs)\ncode: https://github.com/Cartus/AGGCN_TACRED\nHas: results/multi-class classification task/binary-class n-ary relation extraction/Baselines/Experiments/Attention Guided GCNs/binary relation extraction/ternary relation extraction\ndevelop: a soft pruning strategy\ntransforms: the original dependency tree into a fully connected edge weighted graph\non: TACRED dataset\nfor: sentence-level relation extraction task\nCompare: sequence-based model/dependency-based models\nconsider: state-of-the-art Position Aware LSTM (PA-LSTM) (Zhang et al., 2017)\ninclude: GCN and Contextualized GCN (C-GCN) (Zhang et al., 2018)/Tree-structured neural model (Tree-LSTM) (Tai et al., 2015)/Shortest Path LSTM (SDPLSTM) (Xu et al., 2015c)/logistic regression classifier (LR) (Zhang et al., 2017)/Graph State LSTM (GS GLSTM) (Song et al., 2018b)/bidirectional DAG LSTM (Bidir DAG LSTM) (Song et al., 2018b)/Graph LSTM (Peng et al., 2017)\nobtains: 8.0 and 5.7 points higher than the GS GLSTM model for ternary and binary relations, respectively\noutperforms: GCN/GS GLSTM/all the baselines\nsurpasses: the state-of-the-art Graph-structured LSTM model (GS GLSTM) by 6.8 and 3.8 points for the Single and Cross settings\nare: tree-structured LSTM method (SPTree) (Miwa and Bansal, 2016)/Graph convolutional networks (GCN) with pruned trees/Graph-structured LSTM methods/a feature-based classifier (Quirk and Poon, 2017)/neural networks that operate directly on graph structures (Kipf and Welling, 2017)\nhas two tasks: sentence-level relation extraction/cross-sentence n-ary relation extraction/multi-class n-ary relation extraction/binary-class n-ary relation extraction\nhas dataset: Semeval-10 Task 8 (Hendrickx et al., 2010)/TACRED dataset (Zhang et al., 2017)/introduced in (Peng et al., 2017), which contains 6,987 ternary relation instances and 6,087 binary relation instances extracted from PubMed\ncontains: 10,717 instances with 9 relations and a special \"other\" class\nintroduces: 41 relation types and a special \"no relation\" type to describe the relations between the mention pairs in instances\nhas basic components: Attention Guided Layer/GCNs\ncomposed of: M identical blocks\nconsists of three types of layers: linear combination layer/densely connected layer/attention guided layer\nintegrate: representations from N different densely connected layers\nIntroduce: dense connections (Huang et al., 2017) into the AGGCN model in order to capture more structural information on large graphs\ntrain: a deeper model, allowing rich local and non-local information to be captured for learning a better graph representation\ntransform: original dependency tree into a fully connected edge-weighted graph", "text": "#Properties\nhas research problem, propose, code, Has, develop, transforms, on, for, Compare, consider, include, obtains, outperforms, surpasses, are, has two tasks, has dataset, contains, introduces, has basic components, composed of, consists of three types of layers, integrate, Introduce, train, transform\n#Text\nDependency trees convey rich structural information that is proven useful for extracting relations among entities in text. However, how to effectively make use of relevant information while ignoring irrelevant information from the dependency trees remains a challenging research question. Existing approaches employing rule based hard-pruning strategies for selecting relevant partial dependency structures may not always yield optimal results. In this work, we propose Attention Guided Graph Convolutional Networks (AGGCNs), a novel model which directly takes full dependency trees as inputs. Our model can be understood as a soft-pruning approach that automatically learns how to selectively attend to the relevant sub-structures useful for the relation extraction task. Extensive results on various tasks including cross-sentence n-ary relation extraction and large-scale sentence-level relation extraction show that our model is able to better leverage the structural information of the full dependency trees, giving significantly better results than previous approaches. * * Equally Contributed. 1 Case study and attention visualization of this example are provided in the supplementary material. Attention Guided GCNsIn this section, we will present the basic components used for constructing our AGGCN model. GCNsGCNs are neural networks that operate directly on graph structures (Kipf and Welling, 2017).Here we mathematically illustrate how multi-layer GCNs work on a graph. Given a graph with n nodes, we can represent the graph with an n \u00d7 n adjacency matrix A.  extend GCNs for encoding dependency trees by incorporating directionality of edges into the model. They add a self-loop for each node in the tree. Opposite direction of a dependency arc is also included, which means A ij = 1 and A ji = 1 if there is an edge going from node i to node j, otherwise A ij = 0 and A ji = 0. The convolution computation for node i at the l-th layer, which takes the input feature representation h (l\u22121) as input and outputs the induced representation hi , can be defined as:where W (l) is the weight matrix, b (l) is the bias vector, and \u03c1 is an activation function (e.g., RELU). hi is the initial input x i , where V3The winery includes gardensAttention Guided Layer Attention Guided LayerThe AGGCN model is composed of M identical blocks as shown in . Each block consists of three types of layers: attention guided layer, densely connected layer and linear combination layer. We first introduce the attention guided layer of the AGGCN model. As we discuss in Section 1, most existing pruning strategies are predefined. They prune the full tree into a subtree, based on which the adjacency matrix is constructed. In fact, such strategies can also be viewed as a form of hard attention , where edges that connect nodes not on the resulting subtree will be directly assigned zero weights (not attended). Such strategies might eliminate relevant information from the original dependency tree. Instead of using rule-based pruning, we develop a \"soft pruning\" strategy in the attention guided layer, which assigns weights to all edges. These weights can be learned by the model in an end-to-end fashion.In the attention guided layer, we transform the original dependency tree into a fully connected edge-weighted graph by constructing an attention guided adjacency matrix\u00c3. Each\u00c3 corresponds to a certain fully connected graph and each entr\u1ef9 A ij is the weight of the edge going from node i to node j. As shown in ,\u00c3 (1) represents a fully connected graph G  .\u00c3 can be constructed by using self-attention mechanism , which is an attention mechanism (Bahdanau et al., 2015) that captures the interactions between two arbitrary positions of a single sequence. Once we get\u00c3, we can use it as the input for the computation of the later graph convolutional layer. Note that the size of\u00c3 is the same as the original adjacency matrix A (n \u00d7 n). Therefore, no additional computational overhead is involved. The key idea behind the attention guided layer is to use attention for inducing relations between nodes, especially for those connected by indirect, multi-hop paths. These soft relations can be captured by differentiable functions in the model.Here we compute\u00c3 by using multi-head attention , which allows the model to jointly attend to information from different representation subspaces. The calculation involves a query and a set of key-value pairs. The output is computed as a weighted sum of the values, where the weight is computed by a function of the query with the corresponding key.where Q and K are both equal to the collective representation h (l\u22121) at layer l \u2212 1 of the AG-GCN model. The projections are parameter matrices W Q i \u2208 R d\u00d7d and W K i \u2208 R d\u00d7d .\u00c3 (t) is the t-th attention guided adjacency matrix corresponding to the t-th head. Up to N matrices are constructed, where N is a hyper-parameter.  shows an example that the original adjacency matrix is transformed into multiple attention guided adjacency matrices. Accordingly, the input dependency tree is converted into multiple fully connected edge-weighted graphs. In practice, we treat the original adjacency matrix as an initialization so that the dependency information can be captured in the node representations for later attention calculation. The attention guided layer is included starting from the second block. Densely Connected LayerUnlike previous pruning strategies, which lead to a resulting structure that is smaller than the original structure, our attention guided layer outputs a larger fully connected graph. Following , we introduce dense connections  into the AGGCN model in order to capture more structural information on large graphs. With the help of dense connections, we are able to train a deeper model, allowing rich local and non-local information to be captured for learning a better graph representation.Dense connectivity is shown in . Direct connections are introduced from any layer to all its preceding layers. Mathematically, we first define g (l)j as the concatenation of the initial node representation and the node representations produced in layers 1,(3)In practice, each densely connected layer has L sub-layers. The dimensions of these sub-layers d hidden are decided by L and the input feature dimension d. In AGGCNs, we use d hidden = d/L. For example, if the densely connected layer has 3 sub-layers and the input dimension is 300, the hidden dimension of each sub-layer will be d hidden = d/L = 300/3 = 100. Then we concatenate the output of each sub-layer to form the new representation. Therefore, the output dimension is 300 (3 \u00d7 100). Different from the GCN model whose hidden dimension is larger than or equal to the input dimension, the AGGCN model shrinks the hidden dimension as the number of layers increases in order to improves the parameter efficiency similar to DenseNets ).Since we have N different attention guided adjacency matrices, N separate densely connected layers are required. Accordingly, we modify the computation of each layer as follows (for the t-th matrix\u00c3 (t) ):where t = 1, ..., N and t selects the weight matrix and bias term associated with the attention guided adjacency matrix\u00c3  . The column dimension of the weight matrix increases by d hidden per sub-layer, i.e., W Linear Combination LayerThe AGGCN model includes a linear combination layer to integrate representations from N different densely connected layers. Formally, the output of the linear combination layer is defined as:where h out is the output by concatenating outputs from N separate densely connected layers, i.e., h out = [h (1) ; ...;\u00d7d is a weight matrix and b comb is a bias vector for the linear transformation. AGGCNs for Relation ExtractionAfter applying the AGGCN model over the dependency tree, we obtain hidden representations of all tokens. Given these representations, the goal of relation extraction is to predict a relation among entities. Following , we concatenate the sentence representation and entity representations to get the final representation for classification. First we need to obtain the sentence representation h sent . It can be computed as:where h mask represents the masked collective hidden representations. Masked here means we only select representations of tokens that are not entity tokens in the sentence. f :is a max pooling function that maps from n output vectors to 1 sentence vector. Similarly, we can obtain the entity representations. For the i-th entity, its representation h e i can be computed as:where h e i indicates the hidden representation corresponding to the i-th entity. 4 Entity representations will be concatenated with sentence representation to form a new representation. Following , we apply a feed-forward neural network (FFNN) over the concatenated representations inspired by relational reasoning works :where h f inal will be taken as inputs to a logistic regression classifier to make a prediction.3 Experiments DataWe evaluate the performance of our model on two tasks, namely, cross-sentence n-ary relation extraction and sentence-level relation extraction. For the cross-sentence n-ary relation extraction task, we use the dataset introduced in , which contains 6,987 ternary relation instances and 6,087 binary relation instances extracted from PubMed. 5 Most instances contain multiple sentences and each instance is assigned with one of the five labels, including: \"resistance or nonresponse\", \"sensitivity\", \"response\", \"resistance\" and \"None\". We consider two specific tasks for evaluation, i,e., binary-class n-ary relation extraction and multi-class n-ary relation extraction. For binary-class n-ary relation extraction, we follow  to binarize multi-class labels by grouping the four relation classes as \"Yes\" and treating \"None\" as \"No\".For the sentence-level relation extraction task, we follow the experimental settings in  to evaluate our model on the TACRED dataset  and Semeval-10 Task 8 . With over 106K instances, the TACRED dataset introduces 41 relation types and a special \"no relation\" type to describe the relations between the mention pairs in instances. Subject mentions are categorized into person and organization, while object mentions are categorized into 16 fine-grained types, including date, location, etc. Semeval-10 Task 8 is a public dataset, which contains 10,717 instances with 9 relations and a special \"other\" class. SetupWe tune the hyper-parameters according to results on the development sets. For the cross-sentence nary relation extraction task, we use the same data split used in  , while for the sentence-level relation extraction task, we use the same development set from .We choose the number of heads N for attention guided layer from {1, 2, 3, 4}, the block number M from {1, 2, 3}, the number of sublayers L in each densely connected layer from {2, 3, 4}. Through preliminary experiments on the development sets, we find that the combina-give the best results on cross-sentence n-ary relation extraction and sentence-level relation extraction, respectively. GloVe  vectors are used as the initialization for word embeddings.Models are evaluated using the same metrics as previous work . We report the test accuracy averaged over five cross validation folds  for the cross-sentence n-ary relation extraction task. For the sentence-level relation extraction task, we report the micro-averaged F1 scores for the TA-CRED dataset and the macro-averaged F1 scores for the SemEval dataset . For TACRED dataset, we report the mean test F1 score by using 5 models from independent runs. Results on Cross-Sentence n-ary Relation ExtractionFor cross-sentence n-ary relation extraction task, we consider three kinds of models as baselines: 1) a feature-based classifier  based on shortest dependency paths between all entity pairs, 2) Graph-structured LSTM methods, including Graph LSTM , bidirectional DAG LSTM (Bidir DAG LSTM)  and Graph State LSTM (GS GLSTM) . These methods extend LSTM to encode graphs constructed from input sentences with dependency edges, 3) Graph convolutional networks (GCN) with pruned trees, which have shown efficacy on the relation extraction task  . Addition- Feature-Based  74.7 77.7 73.9 75.2 --SPTree  --75.9 75.9 --Graph LSTM-EMBED  80.6 74.3 76.5 --Graph LSTM-FULL  77.9 80.7 75.6 76.7 --00000000000000000 + multi-task -82.0 -78.5 --Bidir DAG LSTM  75.6 77.3 76.9 76.4 51.7 50.7 GS GLSTM  80  : Average test accuracies in five-fold validation for binary-class n-ary relation extraction and multi-class n-ary relation extraction. \"T\" and \"B\" denote ternary drug-gene-mutation interactions and binary drug-mutation interactions, respectively. Single means that we report the accuracy on instances within single sentences, while Cross means the accuracy on all instances. K in the GCN models means that the preprocessed pruned trees include tokens up to distance K away from the dependency path in the LCA subtree.ally, we follow  to consider the tree-structured LSTM method (SPTree) (Miwa and Bansal, 2016) on drug-mutation binary relation extraction. Main results are shown in .We first focus on the binary-class n-ary relation extraction task. For ternary relation extraction (first two columns in  ), our AGGCN model achieves accuracies of 87.1 and 87.0 on instances within single sentence (Single) and on all instances (Cross), respectively, which outperform all the baselines. More specifically, our AG-GCN model surpasses the state-of-the-art Graphstructured LSTM model (GS GLSTM) by 6.8 and 3.8 points for the Single and Cross settings, respectively. Compared to GCN models , our model obtains 1.3 and 1.2 points higher than the best performing model with pruned tree (K=1). For binary relation extraction (third and fourth columns in ), AGGCN consistently outperforms GS GLSTM and GCN as well.These results suggest that, compared to previous full tree based methods, e.g., GS GLSTM, AGGCN is able to extract more information from the underlying graph structure to learn a more expressive representation through graph convolutions. AGGCN also performs better than GCNs, although its performance can be boosted via pruned trees. We believe this is because of the combination of densely connected layer and attention guided layer. The dense connections could facilitate information propagation in large graphs, enabling AGGCN to efficiently learn from long-distance dependencies without pruning techniques. Meanwhile, the attention guided layer can further distill relevant information and filter out noises from the representation learned by the densely connected layer.We next show the results on the multi-class classification task (last two columns in ). We follow ) to evaluate our model on all instances for both ternary and binary relations. This fine-grained classification task is much harder than coarse-grained classification task. As a result, the performance of all models degrades a lot. However, our AGGCN model still obtains 8.0 and 5.7 points higher than the GS GLSTM model for ternary and binary relations, respectively. We also notice that our AGGCN achieves a better test accuracy than all GCN models, which further demonstrates its ability to learn better representations from full trees. Results on Sentence-level Relation ExtractionWe now report the results on the TACRED dataset for the sentence-level relation extraction task in . We compare our model against two kinds of models: 1) dependency-based models, 2) sequence-based models. Dependency-based models include the logistic regression classifier (LR)LR  73.5 49.9 59.4 SDP-LSTM )* 66.3 52.7 58.7 Tree-LSTM )** 66.0 59.2 62.4 PA-LSTM  65.7 64.5 65.1 GCN  69.8 59.0 64.0 C-GCN  69.9 63.3 66.4 AGGCN (ours) 69.9 60.9 65.1 C-  73.1 64.2 69.0  . Our model with default random seed (0) achieves 69.0 F1 score. The trained model is available for download, together with code and other details for reproducing the results. Following other practices in the community, we also conducted further experiments with 4 additional random seeds, and the averaged result over the 5 runs was: mean F1 68.2 with standard deviation 0.5. Model F1SVM  82.2 SDP-LSTM  83.7 SPTree  84.4 PA-LSTM  82.7 C-GCN  84.8 C-AGGCN (ours) 85.7 : Results on the SemEval dataset. , Shortest Path LSTM (SDP-LSTM) , Tree-structured neural model (Tree-LSTM) , GCN and Contextualized GCN (C-GCN) . Both GCN and C-GCN models use the pruned trees. For sequence-based model, we consider the state-of-the-art Position Aware LSTM (PA-LSTM) . As shown in , the logistic regression classifier (LR) obtains the highest precision score. We hypothesize that the reason behind this is due to the data imbalance. This feature-based method tends to predict the relation to be the highly frequent labels (e.g., \"per:title\"). Therefore, it has a high precision while has a relatively low recall. On the other hand, neural models achieve a better balance between precision and recall.Since GCN and C-GCN already show theirModel F1 C-AGGCN 69.0 0 -Attention-guided layer (AG) 67.1 0 -Dense connected layer  67.3 0 -AG, DC 66.7 0 -Feed-Forward layer  67.8 : An ablation study for C-AGGCN model. Model F1C-AGGCN (Full tree) 69.0 C-AGGCN (K=2) 67.5 C-AGGCN 67.9 C-AGGCN (K=0) 67.0 superiority over other dependency-based models and PA-LSTM, we mainly compare our AGGCN model with them. We can observe that AGGCN outperforms GCN by 1.1 F1 points. We speculate that the limited improvement is due to the lack of contextual information about word order or disambiguation. Similar to C-GCN , we extend our AGGCN model with a bidirectional LSTM network to capture the contextual representations which are subsequently fed into AGGCN layers. We term the modified model as C-AGGCN. Our C-AGGCN model achieves a F1 score of 69.0, which outperforms the state-ofart C-GCN model. We also notice that AGGCN and C-AGGCN achieve better precision and recall scores than GCN and C-GCN, respectively. The performance gap between GCNs with pruned trees and AGGCNs with full trees empirically show that the AGGCN model is better at distinguishing relevant from irrelevant information for learning a better graph representation.We also evaluate our model on the SemEval dataset under the same settings as . Results are shown in . This dataset is much smaller than TACRED (only 1/10 of TA-CRED in terms of the number of instances). Our C-AGGCN model (85.7) consistently outperforms the C-GCN model (84.8), showing the good generalizability. ConclusionWe introduce the novel Attention Guided Graph Convolutional Networks (AGGCNs). Experimental results show that AGGCNs achieve state-ofthe-art results on various relation extraction tasks. Unlike previous approaches, AGGCNs operate directly on the full tree and learn to distill the useful information from it in an end-to-end fashion. There are multiple venues for future work. One natural question we would like to ask is how to make use of the proposed framework to perform improved graph representation learning for graph related tasks . Supplemental MaterialA Case Study  shows an instance in cross-sentence nary relation extraction task, which suggests that tumors with L858E mutation in EGFR gene partially responds to the drug gefitinib. The relation between these three entities is sensitivity. We apply path-centric pruning on these dependency trees . The top one in  shows the pruned tree when K= 0 and the bottom one shows the pruned tree when K= 1. The state-of-the-art model, GCN over pruned tree (K= 0 and K= 1)  predict the relation to be response rather than sensitivity. We hypothesize the reason is that the pruned tree miss the crucial information (i.e., partial response). The GCN model is not able to capture the interactions between removed tokens between entities, since these tokens are not in the resulting structure.Our AGGCN model predict the correct relation for this instance by including the attention guided layer, which is able to distill relevant information from the full tree in an end-to-end fashion. As shown in , we visualize the attention scores of two heads in the attention guided layer. We can observe that relevant tokens including entity tokens and tokens that help to predict the correct relation (i.e., showed a partial response) can be attended by other tokens, especially the entity tokens. On the other hand, it is worthwhile to filter out noises from the dependency tree, especially when it is large. In the pruned tree shown in , most stop words are removed directly in order to eliminate irrelevant information. In the AGGCN model, stop words including on, of, in, with, an, were also have much lower scores. We believe the AGGCN model is able to maintain a balance between including and excluding information in the full tree for learning a better graph representation.  compare their Graph State LSTM model with the bidirectional DAG LSTM model against different sentence lengths and different maximal number of neighbors in order to better evaluate their model. Following the same setting, we compare our the test accuracies of the AGGCN model and Graph State LSTM (GS GLSTM) under these two settings on crosssentence n-ary relation extraction task as shown   : Example dependency trees for two sentences expressing a relation (sensitivity) among three entities L858E, EGFR and gefitinib. The top one shows the pruned tree when K=0 (highlighted in bold). The bottom one shows the pruned tree when K=1 (highlighted in bold). Tokens partial response are off these two paths. in . Accuracy against sentence length We can observe that AGGCN consistently outperforms GS GLSTM. Specifically, the performance of GS GLSTM drops when the sentence is short, while the performance of AGGCN is stable. This shows that the superiority of AGGCN over GS GLSTM in utilizing context information. B Additional AnalysisAccuracy against maximal number of neighbors Intuitively, it is easier to model graphs containing nodes with more neighbors, because these nodes can serve as a \"supernode\" that allow more efficient information exchange ). The AGGCN model performs well under the inputs having lower maximal number of neighbors. We hypothesize that is because the attention guided layer convert the original graph into a fully connected graph, which encourages the information propagation.\n###\n"}
{"summary": "has benchmark: Benchmark Penn Treebank (Character Level)/Benchmark WikiText-2/Benchmark Penn Treebank (Word Level)\nhas research problem: Language Modelling\nhas model: Past Decode Reg. + AWD-LSTM-MoS + dyn. eval.", "text": "#Properties\nhas benchmark, has research problem, has model\n#Text\nHighly regularized LSTMs achieve impressive results on several benchmark datasets in language modeling. We propose a new regularization method based on decoding the last token in the context using the predicted distribution of the next token. This biases the model towards retaining more contextual information, in turn improving its ability to predict the next token. With negligible overhead in the number of parameters and training time, our Past Decode Regularization (PDR) method achieves a word level perplexity of 55.6 on the Penn Treebank and 63.5 on the WikiText-2 datasets using a single softmax. We also show gains by using PDR in combination with a mixture-of-softmaxes, achieving a word level perplexity of 53.8 and 60.5 on these datasets. In addition, our method achieves 1.169 bits-per-character on the Penn Treebank Character dataset for character level language modeling. These results constitute a new state-of-the-art in their respective settings. PAST DECODE REGULARIZATION (PDR)be a sequence of tokens. In this paper, we will experiment with both word level and character level language modeling. Therefore, tokens can be either words or characters. The joint probability P (X) factorizes intodenote the context available to the language model for x t+1 . Let W denote the vocabulary of tokens, each of which is embedded into a vector of dimension d. Let E denote the token embedding matrix of dimension |W | \u00d7 d and e w denote the embedding of w \u2208 W . An LSTM computes a distributed representation of c t in the form of its hidden state h t , which we assume has dimension d as well. The probability that the next token is w can then be calculated using a linear decoder followed by a Softmax layer aswhere b w \u2032 is the entry corresponding to w \u2032 in a bias vector b of dimension |W | and | w represents projection onto w. Here we assume that the weights of the decoder are tied with the token embedding matrix E (Inan et al. ; ). To optimize the parameters of the language model \u03b8, the loss function to be minimized during training is set as the cross-entropy between the predicted distribution P \u03b8 (w|c t ) and the actual token x t+1 .Note that Eq.(2), when applied to all w \u2208 W produces a 1 \u00d7 |W | vector w t+1 , encapsulating the prediction the language model has about the next token x t+1 . Since this is dependent on and conditioned on c t , w t+1 clearly encodes information about it; in particular about the last token x t in c t . In turn, it should be possible to infer or decode some limited information about x t from w t+1 . We argue that by biasing the model to be more accurate in recalling information about past tokens, we can help it in predicting the next token better.To this end, we define the following decoding operation to compute a probability distribution over w c \u2208 W as the last token in the context. Here f \u03b8r is a non-linear function that maps vectors in R d to vectors in R d and b \u2032 \u03b8r is a bias vector of dimension |W |, together with parameters \u03b8 r . In effect, we are decoding the past -the last token in the context x t . This produces a vector w r t of dimension 1 \u00d7 |W |. The cross-entropy loss with respect to the actual last token x t can then be computed asHere P DR stands for Past Decode Regularization. L P DR captures the extent to which the decoded distribution of tokens differs from the actual tokens x t in the context. Note the symmetry between Eqs.(2) and (5). The \"input\" in the latter case is w t+1 and the \"context\" is provided by a nonlinear transformation of w t+1 E. Different from the former, the context in Eq. does not preserve any state information across time steps as we want to decode only using w t+1 . The term w t+1 E can be interpreted as a \"soft\" token embedding lookup, where the token vector w t+1 is a probability distribution instead of a unit vector.We add \u03bb P DR L P DR to the loss function in Eq. as a regularization term, where \u03bb P DR is a positive weighting coefficient, to construct the following new loss function for the language model.Thus equivalently PDR can also be viewed as a method of defining an augmented loss function for language modeling. The choice of \u03bb P DR dictates the degree to which we want the language model to incorporate our inductive bias i.e. decodability of the last token in the context. If it is too large, the model will fail to predict the next token, which is its primary task. If it is zero or too small, the model will retain less information about the last token which hampers its predictive performance. In practice, we choose \u03bb P DR by a search based on validation set performance.Note that the trainable parameters \u03b8 r associated with PDR are used only during training to bias the language model and are not used at inference time. This also means that it is important to control the complexity of the nonlinear function f \u03b8r so as not to overly bias the training. As a simple choice, we use a single fully connected layer of size d followed by a Tanh nonlinearity as f \u03b8r . This introduces few extra parameters and a small increase in training time as compared to a model not using PDR. EXPERIMENTSWe present extensive experimental results to show the efficacy of using PDR for language modeling on four standard benchmark datasets -two each for word level and character level language modeling. For the former, we evaluate our method on the Penn Treebank (PTB) ) and the WikiText-2 (WT2) ) datasets. For the latter, we use the Penn Treebank Character (PTBC) ) and the Hutter Prize Wikipedia Prize (Hutter (2018)) (also known as Enwik8) datasets. Key statistics for these datasets is presented in .As mentioned in the introduction, some of the best existing results on these datasets are obtained by using extensive regularization techniques on relatively large LSTMs ; ). We apply our regularization technique to these models, the so called AWD-LSTM. We consider two versions of the model -one with a single softmax (AWD-LSTM) and one with a mixture-of-softmaxes (AWD-LSTM-MoS). The PDR regularization term is computed according to Eq.(4) and Eq.(5). We call our model AWD-LSTM+PDR when using a single softmax and AWD-LSTM-MoS+PDR when using a mixture-of-softmaxes. We largely follow the experimental procedure of the original models and incorporate their dropouts and regularizations in our experiments. The relative contribution of these existing regularizations and PDR will be analyzed in Section 6.There are 7 hyperparameters associated with the regularizations used in AWD-LSTM (and one extra with MoS). PDR also has an associated weighting coefficient \u03bb P DR . For our experiments, we set \u03bb P DR = 0.001 which was determined by a coarse search on the PTB and WT2 validation sets. For the remaining ones, we perform light hyperparameter search in the vicinity of those reported for AWD-LSTM in  and for AWD-LSTM-MoS in . MODEL AND TRAINING FOR PTB AND WIKITEXT-2For the single softmax model (AWD-LSTM+PDR), for both PTB and WT2, we use a 3-layered LSTM with 1150, 1150 and 400 hidden dimensions. The word embedding dimension is set to d = 400. For the mixture-of-softmax model, we use a 3-layer LSTM with dimensions 960, 960 and 620, embedding dimension of 280 and 15 experts for PTB and a 3-layer LSTM with dimensions 1150, 1150 and 650, embedding dimension of d = 300 and 15 experts for WT2. Weight tying is used in all the models. For training the models, we follow the same procedure as AWD-LSTM i.e. a combination of SGD and NT-ASGD, followed by finetuning. We adopt the learning rate schedules and batch sizes of  and  in our experiments. MODEL AND TRAINING FOR PTBC AND ENWIK8For PTBC, we use a 3-layer LSTM with 1000, 1000 and 200 hidden dimensions and a character embedding dimension of d = 200. For Enwik8, we use a LSTM with 1850, 1850 and 400 hidden dimensions and the characters are embedded in d = 400 dimensions. For training, we largely follow the procedure laid out in . For each of the datasets, AWD-LSTM+PDR has less than 1% more parameters than the corresponding AWD-LSTM model (during training only). The maximum observed time overhead due to the additional computation is less than 3%. RESULTS ON WORD LEVEL LANGUAGE MODELINGThe results for PTB are shown in . With a single softmax, our method (AWD-LSTM+PDR) achieves a perplexity of 55.6 on the PTB test set, which improves on the current state-of-the-art with a single softmax by an absolute 1.7 points. The advantages of better information retention due to PDR are maintained when combined with a continuous cache pointer ), where our method yields an absolute improvement of 1.2 over AWD-LSTM. Notably, when coupled with dynamic evaluation ), the perplexity is decreased further to 49.3. To the best of our knowledge, ours is the first method to achieve a sub 50 perplexity on the PTB test set with a single softmax. Note that, for both cache pointer and dynamic evaluation, we coarsely tune the associated hyperparameters on the validation set.Using a mixture-of-softmaxes, our method (AWD-LSTM-MoS+PDR) achieves a test perplexity of 53.8, an improvement of 0.6 points over the current state-of-the-art. The use of dynamic evaluation pushes the perplexity further down to 47.3. PTB is a restrictive dataset with a vocabulary of 10K words. Achieving good perplexity requires considerable regularization. The fact that PDR can improve upon existing heavily regularized models is empirical evidence of its distinctive nature and its effectiveness in improving language models.  shows the perplexities achieved by our model on WT2. This dataset is considerably more complex than PTB with a vocabulary of more than 33K words. AWD-LSTM+PDR improves over the current state-of-the-art with a single softmax by a significant 2.3 points, achieving a perplexity of 63.5. The gains are maintained with the use of cache pointer (2.4 points) and with the use of dynamic evaluation (1.7 points). Using a mixture-of-softmaxes, AWD-LSTM-MoS+PDR achieves perplexities of 60.5 and 40.3 (with dynamic evaluation) on the WT2 test set, improving upon the current state-of-the-art by 1.0 and 0.4 points respectively. Model #Params Valid Test PERFORMANCE ON LARGER DATASETSWe consider the Gigaword dataset  with a truncated vocabulary of about 100K tokens with the highest frequency and apply PDR to a baseline 2-layer LSTM language model with embedding and hidden dimensions set to 1024. We use all the shards from the training set for training and a few shards from the heldout set for validation (heldout-0,10) and test . We tuned the PDR coefficient coarsely in the vicinity of 0.001. While the baseline model achieved a validation (test) perplexity of 44.3 (43.1), on applying PDR, the model achieved a perplexity of 44.0 (42.5). Thus, PDR is relatively less effective on larger datasets, a fact also observed for other regularization techniques on such datasets RESULTS ON CHARACTER LEVEL LANGUAGE MODELINGThe results on PTBC are shown in . Our method achieves a bits-per-character (BPC) performance of 1.169 on the PTBC test set, improving on the current state-of-the-art by 0.006 or 0.5%. It is notable that even with this highly processed dataset and a small vocabulary of only 51 tokens, our method improves on already highly regularized models. Finally, we present results on Enwik8 in . AWD-LSTM+PDR achieves 1.245 BPC. This is 0.012 or about 1% less than the 1.257 BPC achieved by AWD-LSTM in our experiments (with hyperparameters from Merity et al. (2018b)). ANALYSIS OF PDRIn this section, we analyze PDR by probing its performance in several ways and comparing it with current state-of-the-art models that do not use PDR.  : Validation perplexities for AWD-LSTM without any regularization and with only PDR.To verify that indeed PDR can act as a form of regularization, we perform the following experiment. We take the models for PTB and WT2 and turn off all dropouts and regularization and compare its performance with only PDR turned on. The results, as shown in , validate the premise   of PDR. The model with only PDR turned on achieves 2.4 and 5.1 better validation perplexity on PTB and WT2 as compared to the model without any regularization. Thus, biasing the LSTM by decoding the distribution of past tokens from the predicted next-token distribution can indeed act as a regularizer leading to better generalization performance.Next, we plot histograms of the negative log-likelihoods of the correct context tokens x t in the past decoded vector w r t computed using our best models on the PTB and WT2 validation sets in . The NLL values are significantly peaked near 0, which means that the past decoding operation is able to decode significant amount of information about the last token in the context.To investigate the effect of hyperparameters on PDR, we pick 60 sets of random hyperparameters in the vicinity of those reported by  and compute the validation set perplexity after training (without finetuning) on PTB, for both AWD-LSTM+PDR and AWD-LSTM. Their histograms are plotted in . The perplexities for models with PDR are distributed slightly to the left of those without PDR. There appears to be more instances of perplexities in the higher range for models without PDR. Note that there are certainly hyperparameter settings where adding PDR leads to lower validation complexity, as is generally the case for any regularization method. PTB WT2 ModelValid  : Ablation experiments on the PTB and WT2 validation sets. COMPARISON WITH AWD-LSTMTo show the qualitative difference between AWD-LSTM+PDR and AWD-LSTM, in , we plot a histogram of the entropy of the predicted next token distribution w t+1 for all the tokens in the validation set of PTB achieved by their respective best models. The distributions for the two models is slightly different, with some identifiable patterns. The use of PDR has the effect of reducing the entropy of the predicted distribution when it is in the higher range of 8 and above, pushing it into the range of 5-8. This shows that one way PDR biases the language model is by reducing the entropy of the predicted next token distribution. Indeed, one way to reduce the cross-entropy between x t and w r t is by making w t+1 less spread out in Eq.(5). This tends to benefits the language model when the predictions are correct.We also compare the training curves for the two models in  on PTB. Although the two models use slightly different hyperparameters, the regularization effect of PDR is apparent with a lower validation perplexity but higher training perplexity. The corresponding trends shown in  for WT2 have similar characteristics. ABLATION STUDIESWe perform a set of ablation experiments on the best AWD-LSTM+PDR models for PTB and WT2 to understand the relative contribution of PDR and the other regularizations used in the model. The results are shown in . In both cases, PDR has a significant effect in decreasing the validation set performance, albeit lesser than the other forms of regularization. This is not surprising as PDR does not influence the LSTM directly.\n###\n"}
{"summary": "has benchmark: Benchmark Habitat 2020 Point Nav test-std\nhas research problem: Robot Navigation\nhas model: OccupancyAnticipation\nsame as: https://en.wikipedia.org/wiki/Robot_navigation", "text": "#Properties\nhas benchmark, has research problem, has model, same as\n#Text\nState-of-the-art navigation methods leverage a spatial memory to generalize to new environments, but their occupancy maps are limited to capturing the geometric structures directly observed by the agent. We propose occupancy anticipation, where the agent uses its egocentric RGB-D observations to infer the occupancy state beyond the visible regions. In doing so, the agent builds its spatial awareness more rapidly, which facilitates efficient exploration and navigation in 3D environments. By exploiting context in both the egocentric views and top-down maps our model successfully anticipates a broader map of the environment, with performance significantly better than strong baselines. Furthermore, when deployed for the sequential decision-making tasks of exploration and navigation, our model outperforms state-of-the-art methods on the Gibson and Matterport3D datasets. Our approach is the winning entry in the 2020 Habitat PointNav Challenge. Project page: http: // vision. cs. utexas. edu/ projects/ occupancy_ anticipation/ ApproachWe propose an occupancy anticipation approach for efficient exploration and navigation. Our model anticipates areas not directly visible to the agent because of occlusion (e.g., behind a table, around a corner) or due to being outside its FoV. The agent's first-person view is provided in the form of RGB-D images (see . The goal is to anticipate the occupancy for a fixed region in front of the agent, and integrate those predictions over time as the agent moves about.Next, we define the task setup and notation, followed by our approach for occupancy anticipation (Sec. 3.1) and a new formulation for exploration that rewards correctly anticipated regions (Sec. 3.2). Then, we explain how our occupancy anticipation model can be integrated into a state-of-the-art approach  for autonomous exploration and navigation in 3D environments (Sec. 3.3). Occupancy anticipation modelWe formulate occupancy anticipation as a pixelwise classification task. The egocentric occupancy is represented as a two-channel top-down map p \u2208 [0, 1] 2\u00d7V \u00d7V which comprises a local area of V \u00d7 V cells in front of the camera. Each cell in the map represents a 25mm \u00d7 25mm region. The two channels contain the probabilities (confidence values) of the cell being occupied and explored, respectively. A cell is considered to be occupied if there is an obstacle, and it is explored if we know whether it is occupied or free. For training, we use the 3D meshes of indoor environments (Sec. 4.1) to obtain the ground-truth local occupancy of a V \u00d7 V region in front of the camera, which includes parts that may be occluded or outside the field of view .Our occupancy anticipation model consists of three main components ( ): (1) Feature extraction: Given egocentric RGB-D inputs, we compute: RGB CNN features: We encode the RGB images using blocks 1 and 2 of a ResNet-18 that is pre-trained on ImageNet, followed by three additional convolution layers that prepare these features to be passed forward with the visible occupancy map. This step extracts a mixture of textural and semantic features. Depth projection: We estimate a map of occupied, free, and unknown space by setting height thresholds on the point cloud obtained from depth and camera intrinsics . Consistent with past work , we restrict the projection-based estimates to points within \u223c 3m, the range at which modern depth sensors would provide reliable results. This yields the initial visible occupancy map.(2) Feature encoding: Given the RGB-D features, we independently encode them using UNet  encoders and project them to a common feature space. We encode the depth projection features using a stack of five convolutional blocks which results in featuresSince the RGB features are already at a lower resolution, we use only three convolutional blocks to encode them, which results in features f r = f r 3:5 . We then combine these features using the Merge module which contains layer-specific convolution blocks to merge each [f r i , f d i ]:For experiments with only the depth modality, we skip the RGB feature extractor and Merge layer and directly use the occupancy features obtained from the depth image. For experiments with only the RGB modality, we learn a model to infer the visible occupancy features from RGB (to be defined at the end of Sec. 4.1) and use that instead of the features computed from the depth image.(3) Anticipation decoding: Given the encoded features f , we use a UNet decoder that outputs a 2 \u00d7 V \u00d7 V tensor of probabilities:wherep \u2208 [0, 1] 2\u00d7V \u00d7V is the estimated egocentric occupancy and \u03c3 is the sigmoid activation function. For training the occupancy anticipation model, we use binary cross entropy loss per pixel and per channel:where p is the ground-truth (GT) occupancy map that is derived from the 3D mesh of training environments (see Sec. S5 in Supp. for details). So far, we have presented our occupancy anticipation approach supposing a single RGB-D observation as input. However, our model is ultimately used in the context of an embodied agent that moves in the environment and actively collects a sequence of RGB-D views to build a complete map of the environment. Next, we introduce a new reward function that utilizes the agent's anticipation performance to guide its exploration during training. Anticipation reward for exploration policy learningIn visual exploration, an agent must quickly map a new environment without having a specified target. Prior work on exploration  often uses area-coverage-the area seen in the environment during navigation-as a reward function to guide exploration. However, the traditional area-coverage approach is limited to rewarding the agent only for directly seeing areas. Arguably, an ideal exploration agent would obtain an accurate and complete map of the environment without necessarily directly observing all areas.Thus, we propose to encourage exploratory behaviors that yield a correctly anticipated map. In this case, the occupancy entries in the map need not be obtained via direct agent observations to register a reward; it is sufficient to correctly infer them. In particular, we reward agent actions that yield accurate occupancy predictions for the global environment map, i.e., the number of grid cells where the predicted occupancy matches the layout of the environment.More concretely, letm t \u2208 [0, 1] 2\u00d7G\u00d7G be the global environment map obtained by anticipating occupancy for the RGB-D observations {x r 1:t , x d 1:t } from time 1 to t, and then geometrically registering the predictions to a single global map based on the agent's pose estimates at each time step (see ). Note G > V . Let m be the ground-truth layout of the environment. Then, the unnormalized accuracy of a map predictionm is measured as follows:where 1[m ij = m ij ] is an indicator function that returns one ifm ij = m ij and zero otherwise. We reward the increase in map accuracy from time t \u2212 1 to t:This function rewards actions leading to correct global map predictions, irrespective of whether the agent actually observed those locations. For example, if the agent correctly anticipates free space behind a table and is rewarded for that, it then learns to avoid spending additional time around tables in the future to observe that space directly. Resources can be instead allocated to visiting more interesting regions that are harder to anticipate. Additionally, this reward  : Exploration with occupancy anticipation: We introduce two key upgrades to the original Active Neural SLAM (ANS) model  (see text):  We replace the projection unit in the mapper with our occupancy anticipation model (see ). We replace the area-coverage reward function with the proposed reward (Eqn. 5), which encourages the agent to efficiently explore and build accurate maps through occupancy anticipation. Note that the reward signals (in red) are provided only during training.provides a better learning signal while training under noisy conditions by accounting for mapping errors arising from noisy pose and map predictions. Thus, our approach encourages more intelligent exploration behavior by injecting our anticipated occupancy idea directly into the agent's sequential decision-making. Exploration and navigation with occupancy anticipationHaving defined the core occupancy anticipation components, we now demonstrate how our model can be used to benefit embodied navigation in 3D environments. We consider both exploration (discussed above) and PointGoal navigation , a.k.a PointNav, where the agent must navigate efficiently to a target specified by a displacement vector from the agent's starting position.For both tasks, we adapt the state-of-the-art Active Neural SLAM (ANS) architecture  that previously achieved the best exploration results in the literature and was the winner of the 2019 Habitat PointNav challenge. However, our anticipation model is generic and can be easily integrated with most mapbased embodied navigation models .The ANS model is a hierarchical, modular policy for exploration that consists of a mapper, a planner, a local policy, and a global policy (shown in ). Given RGB images, the mapper estimates the egocentric occupancy and agent pose, and then temporally aggregates the maps into a global top-down map using the pose estimates. At regular time intervals \u2206, the global policy picks a location on the global map to explore. A shortest-path planner decides what trajectory to take from the current position to the target and picks an intermediate goal (within 1.25m) to navigate to. The local policy then selects actions that lead to the intermediate goal; it gets another intermediate goal upon reaching the current goal. See  for details. Critically, and like other prior work, the model of  is supervised to generate occupancy estimates based solely on the visible occupancy obtained from the egocentric views.We adapt ANS by modifying the mapper and the reward function. For the mapper, we replace the projection unit from ANS with our anticipation model (see ). Additionally, we account for incorrect occupancy estimates in two ways: (1) we filter out high entropy predictions and (2) we maintain a moving average estimate of occupancy at each location in the global map (see Sec. S7 in Supp.). For the reward function, we use the anticipation-based reward presented in Sec. 3.2.We train the exploration policy with our anticipation model end-to-end, as this allows adapting to the changing distribution of the agent's inputs. Both the local and the global reinforcement learning policies are trained with Proximal Policy Optimization (PPO) . In our model, the reward of the global policy is our anticipation-based reward defined in Eqn. 5. This replaces the traditional area-coverage reward used in ANS and other current models , which rewards the increment in the actual area seen, not the correctly registered area in the map. The reward for the local policy is simply based on the reduction in the distance to the local goal: ExperimentsIn the following experiments we demonstrate that 1) our occupancy anticipation module can successfully infer unseen parts of the map (Sec. 4.2) and 2) trained together with an exploration and navigation policy, it accelerates active mapping and navigation in new environments (Sec. 4.3 and Sec. 4.4). Experimental setupWe use the Habitat  simulator along with Gibson [74] and Matterport3D  environments. Each dataset contains around 90 challenging large-scale photorealistic 3D indoor environments such as houses and office buildings. On average, the Matterport3D environments are larger. Our observation space consists of 128 \u00d7 128 RGB-D observations and odometry sensor readings that denote the change in the agent's pose x, y, \u03b8. Our action space consists of three actions: move-forward by 25cm, turn-left by 10 \u2022 , turn-right by 10 \u2022 . For navigation, we add a stop action, which the agent emits when it believes it has reached the goal. We simulate noisy actuation and odometer readings for realistic evaluation (see Sec. S6 in Supp.).We train our exploration models on Gibson, and then transfer them to Point-Goal navigation on Gibson and exploration on Matterport3D. We use the default train/val/test splits provided for both datasets  with disjoint environments across the splits. For evaluation on Gibson, we divide the validation environments into small (area less than 36m 2 ) and large (area greater than 36m 2 ) to observe the influence of environment size on results. For policy learning, we use  the Adam optimizer and train on episodes of length 1000 for 1.5 \u2212 2 million frames of experience. Please see Sec. S8 in Supp. for more details.Baselines: We define baselines based on prior work:-ANS(rgb) : This is the state-of-the-art Active Neural SLAM approach for exploration and navigation. We use the original mapper architecture , which infers the visible occupancy from RGB. 3 -ANS(depth): We use depth projection to infer the visible occupancy (similar to ) instead of predicting it from RGB. -View-extrap.: We extrapolate an 180 \u2022 FoV depth map from 90 \u2022 FoV RGB-D and project it to the top-down view. This is representative of scene completion approaches . See Sec. S11 in Supp. for network details. -OccAnt(GT): This is an upper bound that cheats by using the groundtruth anticipation maps for exploration and navigation.We implement all baselines on top of the ANS framework. Our goal is to show the impact of our occupancy model, while fixing the backbone navigation architecture and policy learning approach across methods for a fair comparison. We consider three versions of our models based on the input modality:-OccAnt(depth): anticipate occupancy given the visible occupancy map.-OccAnt(rgb): anticipate occupancy given only the RGB image. We replace the depth projections in  with the pre-trained ANS(rgb) estimates (kept frozen throughout training). -OccAnt(rgbd): anticipate occupancy given the full RGB-D inputs.By default, our methods use the proposed anticipation reward from Sec. 3.2. We denote ablations without this reward as \"w/o AR\". Occupancy anticipation resultsFirst we evaluate the per-frame prediction accuracy of the mapping models trained during exploration. We evaluate on a separate dataset of images sampled from validation environments in Gibson at uniform viewpoints from discrete locations on a 1m grid, a total of 1, 034 (input, output) samples. This allows standardized evaluation of the mapper, independent of the exploration policy.To quantify the local occupancy maps' accuracy, we compare the predicted maps to the ground truth. We report the Intersection over Union (IoU) and F1 scores for the \"free\" and \"occupied\" classes independently. In addition to the baselines from Sec. 4.1, we add two naive baselines that classify all locations as free (all-free), or occupied (all-occupied).  shows the results. Our anticipation models OccAnt are substantially better than all the baselines. Comparing different modalities, OccAnt(depth) is much better than OccAnt(rgb) under all the metrics. This makes sense, as visible occupancy is directly computable from the depth input, but must be inferred for RGB (see ). Interestingly, the rgbd models are not better than the depth-only models, likely because (1) geometric cues are more easily learned from depth than RGB, and (2) the RGB encoder contains significantly more parameters and could lead to overfitting. See  in Supp. for network sizes. Overall,  demonstrates our occupancy anticipation models successfully broaden the coverage of the map beyond the visible regions. Exploration resultsNext we deploy our models for visual exploration. The agent is given a limited time budget (T =1000) to intelligently explore and build a 2D top-down occupancy map of a previously unseen environment.To quantify exploration, we measure both map quality and speed (number of agent actions): (1) Map accuracy (m 2 ): the area in the global map built during exploration (both free and occupied) that matches with the ground-truth layout of the environment. The map is built using predicted occupancy maps which are registered using estimated pose (may be noisy). Note that this is an unnormalized accuracy measure (see Eqn. 4). (2) IoU: the intersection over union between that same global map and the ground-truth layout of the environment. (3) Area seen (m 2 ): the amount of free and occupied regions directly seen during exploration. The map for this metric is built using ground-truth pose and depth-projections (similar to ). (4) Episode steps: the number of actions taken by the agent. While the first two metrics measure the quality of the created map, the latter two are a function of how (and how long) the agent moved to get that map. Higher accuracy in fewer steps or lower area-seen is better.  : Per-frame local occupancy predictions: First and last columns show the RGB-D input and anticipation ground-truth, respectively. ANS(*) are restricted to only predicting occupancy for visible regions. View-extrap. extrapolates, but is unable to predict occupancy for occluded regions (first row) and struggles to make correct predictions in cluttered scenes (second row). Our model successfully anticipates with either RGB or depth. For example, in the first row, we successfully predict the presence of a corridor and another room on the left. In the second row, we successfully predict the presence of navigable space behind the table. In the third row, we are able to correctly anticipate the free space behind the chair and the corridor to the right.  We compare OccAnt with ANS  in Gibson under noisy actuation and odometry. The exploration trajectories and the corresponding maps are shown at the extremes and center, respectively. Row 1: Both methods cover similar area, but our method better anticipates the unseen parts with fewer registration errors. Row 2: Our method achieves better area coverage and mapping quality whereas the baseline gets stuck in a small room for extended periods of time. Row 3: A failure case for our method, where it gets stuck in one part of the house after anticipating that a narrow corridor leading to a different room was occupied. under noisy conditions (see Sec. S1 in Supp. for noise-free). Higher and steeper curves are better. Top: Our OccAnt approach rapidly attains higher map accuracy than the baselines (dotted lines). Bottom: OccAnt achieves higher map accuracy for the same area seen (we show the best variants here to avoid clutter). These results show the agent actively moves better to explore the environment with occupancy anticipation.    shows the exploration results. Our approach generally outperforms the baselines, improving the map quality more rapidly, whether in terms of time (top row) or area seen (bottom row). When compared on a same-modality basis, we see that OccAnt(rgb) converges much faster than ANS(rgb). Similarly, OccAnt(depth) is able to rapidly improve the map quality and outperforms ANS(depth) on all cases. This apples-to-apples comparison shows that anticipating occupancy leads to much more efficient mapping in unseen environments. Again, using depth generally provides more reliable mapping than pure RGB.Furthermore, the proposed anticipation reward generally provides significant benefits to map accuracy in the noisy setting (compare our full model to the \"w/o AR\" models in ). While map accuracy generally increases over time for noise-free conditions (see Sec. S1 in Supp.), it sometimes saturates early or even declines slightly over time in the noisy setting as noisy pose estimates accumulate and hurt map registration accuracy. This is most visible in Gibson small (top left plot). However, our anticipatory reward alleviates this decline.  summarizes the map accuracy and IoU for all methods at T =500. Our method obtains significant improvements, supporting our claim that occupancy anticipation accelerates exploration and mapping. Additionally, perfect anticipation with the OccAnt(GT) model gives comparably good noisy exploration, and good gains in noise-free exploration (+10-20% IoU). This shows that there is indeed a lot of mileage in anticipating occupancy; our model moves the stateof-the-art towards this ceiling.  shows example exploration trajectories and the final global map predictions on Gibson. Navigation resultsNext we evaluate the utility of occupancy anticipation for quickly reaching a target. In PointNav , the agent is given a 2D coordinate (relative to its position) and needs to reach that target as quickly as possible. Following , we use noise-free evaluation and directly transfer the mapper, planner, and local policy learned during exploration to this task. In this way, instead of navigating to a point specified by the global policy, the agent has to navigate to a fixed goal location. To evaluate navigation, we use the standard metrics-success rate, success rate normalized by inverse path length (SPL) , and time taken. The agent succeeds if it stops within 0.2m of the target under a time budget of T = 1000.  shows the navigation results on the Gibson validation set. Our approach outperforms the baselines. Thus, not only does occupancy anticipation successfully map the environment, but it also allows the agent to move to a specified goal more quickly by modeling the navigable spaces. This apples-to-apples comparison shows that our idea improves the state of the art for PointNav. As with exploration, using ground truth (GT) anticipation leads to good gains in the navigation performance, and our methods bridge the gap between the prior state of the art and perfect anticipation.In concurrent work, the DD-PPO approach [72] obtains 0.96 SPL for Point-Nav, but it requires 2.5 billion frames of experience to do so (and it fails for noisy conditions; see below). To achieve the performance of our method (0.8 SPL in 2M frames), DD-PPO requires more than 50\u00d7 the experience. Our sample efficiency can be attributed to explicit mapping along with occupancy anticipation.   Finally, we validate our approach on the 2020 Habitat PointNav Challenge , which requires the agent to adapt to noisy RGB-D sensors and noisy actuators, and to operate without an odometer. This presents a much more difficult evaluation setup than past work which assumes perfect odometry as well as noise-free sensing and actuation . See Sec. S13 in Supp. for more details.  shows the results. Our method won the challenge, outperforming the competing approaches by large margins. While our approach generalizes well to this setting, DD-PPO [72] fails (0 SPL) due to its reliance on perfect odometry. ConclusionWe introduced the idea of occupancy anticipation from egocentric views in 3D environments. By learning to anticipate the navigable areas beyond the agent's actual field of view, we obtain more accurate maps more efficiently in novel environments. We demonstrate our idea both for individual local maps, as well as integrated within sequential models for exploration and navigation, where the agent continually refines its (anticipated) map of the world. Our results clearly demonstrate the advantages on multiple datasets, including improvements to the state-of-the-art embodied AI model for exploration and navigation.69. Straub, J., Whelan, T., Ma, L., Chen, Y., Wijmans, E., Green, S., Engel, J.J., Mur-Artal, R., Ren, C., Verma, S., Clarkson, A., Yan, M., Budge, B., Yan, Y., Pan, X., Yon, J., Zou, Y., Leon, K., Carter, N., Briales, J., Gillingham, T., Mueggler, E., Pesqueira, L.,  correctly, AR is likely to be low even if the per-frame map estimates are very good. Therefore, in addition to covering more area, the agent also has to better train the pose estimator which would then lead to higher AR over time. Since noise correction is not needed under noise-free conditions, using AR has limited impact on the final performance. Matterport3D under noise-free conditions. Top: Our OccAnt approach (solid lines) rapidly attains higher map accuracy than the baselines (dotted lines). Using anticipation reward (AR) largely retains the original performance in the noise-free conditions (but improves significantly in the noisy conditions, see  main paper). Bottom: OccAnt achieves higher map accuracy for the same area covered (we show best variants here to avoid clutter). These results show the agent actively moves better to explore the environment with our occupancy anticipation idea. S2 Occupancy anticipation ablation studyAs discussed in the main paper, our key contributions are a novel framework for occupancy anticipation and a novel anticipation reward which encourages the agent to build more accurate maps (as opposed to covering more area). To isolate the gains achieved by these individual contributions, we view the results from the main paper , and 3 in main paper) in a different way. We first group the results based on the modality (rgb/depth/rgbd), and further sort the methods based on whether they use occupancy anticipation (OccAnt) or the anticipation reward (AR). We present these ablations for the per-frame map evaluation ), the exploration evaluation , and the navigation evaluation ). By default, the ANS baselines do not use occupancy anticipation or the anticipation reward and our methods always use occupancy anticipation.For per-frame maps, in  we see that adding occupancy anticipation to the base model significantly improves the IoU and F1 scores as expected. Adding the anticipation reward leads to comparable or better results, showing that it leads to better training of the mapper during the exploration training.For exploration, in  we see that adding occupancy anticipation generally leads to better map quality than ANS across different modalities and testing conditions. Adding the anticipation reward (AR) leads to significant improvements in the map quality under noisy conditions for both depth and rgbd modalities (rgb slightly underperforms). This is primarily due to improved training of the mapper module which leads to better map registration (see Sec. S4). As we also noted in Sec. S1, using AR in noise-free conditions has limited impact on the performance as the pose-estimation is assumed to be perfect in these cases. It mainly benefits exploration in the more real-world testing scenarios with noisy actuation and sensing.For navigation, in  we see that adding occupancy anticipation leads to significant improvements in all three metrics. The impact of using AR here is limited because we assume noise-free test conditions for PointNav (following ). However, the challenge results reported in the main paper remove this assumption to test PointNav with noisy odometry and actuation. S3 Occupancy anticipation qualitative examplesSee Figs. S2 and S3 for some successful cases and failure cases for our best method from  when compared with the baselines. S4 Exploration with occupancy anticipation examplesIn  and  from the main paper, and  in this supplementary, we see that adding occupancy anticipation on top of the ANS baseline leads to better performance, and adding anticipation reward (AR) leads to better mapping in the noisy cases.Here, we highlight some example episodes to show that (1) using occupancy anticipation avoids local navigation difficulties and obtains higher map qualities for lower area coverage , while sometimes being susceptible to inaccuracies in map predictions , and (2) the anticipation reward leads to better map registration (i.e., good pose estimates) which results in higher map quality . The color scheme for the trajectories (from ) and the predicted maps in the center (from ) are indicated below each plot.  : Occupancy anticipation failure cases: Our approach OccAnt(rgbd) incorrectly predicts narrow corridors as occupied, and is unable to handle cases where multiple solutions may exist. For example, in rows 2, 5 and 8, it predicts that the corridors in the center of the map are blocked. In row 1, it predicts that the two doors correspond to the same room, even though the wall colors are different and it is unlikely that a small room would have two doors. In row 3, 4 and 6, it predicts entrances to spaces that do not exist. Such predictions are generally difficult to make given only the context of the current first-person view, and therefore our model tends to fail at these cases. Map prediction color scheme : We enumerate some of the key advantages of exploration using occupancy anticipation by comparing OccAnt(depth) w/o AR with ANS(depth) in Gibson under noise-free conditions. The exploration trajectories and the map created during exploration are shown at the extremes and the center, respectively. 'ANS(depth) tends to achieve worse exploration in some cases where the visible occupancy is incorrectly estimated (top 3 rows), causing the agent to get stuck in local regions. In other cases, the map accuracy is generally higher for OccAnt(depth) w/o AR for similar amounts of area seen (bottom 3 rows) as it is better at filling up the occupancy for unvisited regions. Map prediction color scheme Fig. S5:We highlight one key weakness of exploration using occupancy anticipation, which is the impact of classification errors in occupancy estimates. We compare Oc-cAnt(depth) w/o AR with ANS(depth) in Gibson. In some cases, OccAnt(depth) w/o AR tends to generate false negatives for occupied regions, classifying some of the explored obstacles as free-space (gray regions in the first 3 rows, 2nd column). While this does not impact the area seen, it does reduce the map quality. On the flip side, OccAnt(depth) w/o AR may prematurely classify some narrow corridors as blocked (similar to ) causing the agent to stop exploring beyond that corridor (light green regions in last two rows, 2nd column). Map prediction color scheme : The impact of using anticipation reward: In  in Supp., we could see that models using anticipation reward generally leads to higher map qualities in noisy test conditions. Here, we show that, when the model that uses the anticipation reward (OccAnt(depth) on the left) accounts much better for the noise in map registration when compared to a vanilla anticipation model that does not use it (OccAnt(depth) w/o AR on the right).     Using 3D meshes of indoor environments from Gibson and Matterport3D, we obtain the ground-truth local occupancy of a V \u00d7V region in front of the camera which includes parts that may be occluded or outside the field-of-view (see  from main paper). However, this may include regions in the environment that are outside the bounds of the environment's mesh. To alleviate this problem, we devise a simple heuristic that generates the ground truth by masking out regions in the occupancy map that are outside the bounds of the environment (highlighted in ).We first obtain the visible occupancy via a geometric projection of the depth inputs (2nd column). We then selectively sample the ground-truth layout (last column) around the visible regions by growing a mask around the visible occupancy by sequential hole-filling and morphological dilation operations. We perform two iterations of this region growing to obtain the final ground-truth used to train our model (3rd & 4th columns). This heuristic captures the occupied regions that are closer to navigable space in the environment (likely to be objects, walls, etc), while ignoring regions outside the bounds of the environment. This is necessary since the occupancy map from the simulator does not distinguish between obstacles and regions outside the bounds of the environment mesh. Note that these steps apply only in training; during inference the occupancy anticipation proceeds solely in the end-to-end model. S6 Noise models for actuation and odometryFollowing , we simulate realistic actuation and odometry to train and evaluate our exploration agents. For this purpose, we use the PyRobot actuation model provided by Habitat which consists of truncated Gaussians for both the rotation and translation motions.  Specifically, we use the default LoCoBot noise-model with the ILQR controller. For simulating noise in the odometry, we similarly use truncated Gaussians for both rotation and translation measurements. For the translation measurement, we use a mean of 0.025m and a standard deviation of 0.001 For the rotation measurement, we use a mean of 0.9 \u2022 and standard deviation of 0.057 \u2022 . The distributions are truncated at 2 standard deviations. These are based on approximate values provided by the authors of ANS. S7 Differences in ANS implementationWe implemented the ANS approach using the published details in  and instructions obtained directly from the authors via private communication as code was not publicly available at the time of our research. Our implementation has a few differences from that in , which we discuss in the following. For shortest path planning, we use an A* planner instead of fast-marching  used in  since we were able to find a fast A* implementation that was publicly available.  For aggregating the local occupancy mapsp t from each observation  into the global mapm t\u22121 from the previous time-step , the authors in  use channelwise max-pooling of the local and global maps to obtain the updated global mapm t .Instead, we opt to perform a moving-average over time to allow the agent to account for errors in the map prediction by averaging predictions from multiple views over time.mWe found that this provided robustness to false positives in the map predictions and registration errors due to odometry noise. Additionally, since our proposed model anticipates occupancy beyond the visible regions, we found that it is helpful to filter out low-confidence predictions of occupancy on a per-frame basis using the EntropyFilter() operation. Given predictionp t , EntropyFilter() masks out the predictions for locations i, j in p t where the binary-entropy of the probabilities across the map channels are larger than a threshold \u03c4 ent before performing the moving-average aggregation. These low-confidence predictions generally correspond to regions that are hard to anticipate or may have multiple solutions. Hence, our global map update formula is:m t = \u03b1 emt\u22121 + (1 \u2212 \u03b1 e )EntropyFilter(p t ).(3) S8 Implementation detailsThe key hyperparameters for learning the policy and mapper are specified in . S10 ANS projection unit architectureThe projection unit architecture for the ANS(rgb) baseline is shown in . This is based on the architecture in  with some minor differences. It uses nn.BatchNorm + nn.ReLU blocks instead of nn.Dropout in the fully connected layers, it has a larger convolutional decoder to account for our larger map outputs, and it consists of nn.Conv2d + nn.Upsample layers instead of than nn.ConvTranspose2D layers as this has been shown to reduce checkerboard artifacts . S11 View extrapolation baselineWe now provide more details on the task-defintion and architecture for the Viewextrap. baseline introduced in Sec.   not natively support panoramic rendering, we use a simpler solution to account for this. We place two cameras with \u00b145 \u2022 heading offsets and aim to regress those from the egocentric view (see ). Since each camera has a 90 \u2022 FoV, Convolutional decoder : ANS projection unit: ResNet-18 features are extracted, followed by two fully connected layers (represented by 1 \u00d7 1 convolutions) and a convolutional decoder that uses \"Upsample\" blocks to increase the output resolution and predict the occupancy estimatesp. Note that this is supervised to predict the visible occupancy map, not the anticipated occupancy map (see  in main paper).this leads to an effective coverage of 180 \u2022 once the agent anticipates the unobserved portions. We base our architecture for view extrapolation on the model from [78] with a capacity similar to our model to permit online training during policy learning (see ). It takes as input the 90 \u2022 FoV RGB-D images and regresses the left and right cameras. It is trained to minimize the pixelwise 1 loss between the prediction and the ground-truth. S12 Comparing the model capacities of different methodsWe compare the overall model capacity of our approaches with the baselines in . The depth-only models (bottom 2 rows) tend to have fewer parameters than the rgb-only models as they rely on geometric projection for processing depth (no ResNet backbone). Our depth model has comparable number of parameters with the depth-only baselines. Our rgb model has slightly more parameters than the rgb baseline. However, this is due to the fact that OccAnt(rgb) takes the output of ANS(rgb) as an additional input. However, since ANS(rgb) is kept frozen throughout the training of OccAnt(rgb), this effectively gives us 5.7M trainable parameters. S13 Habitat Challenge 2020We detail the key issues we had to address for the PointNav track of Habitat Challenge 2020  and the changes to our system required to achieve our stateof-the-art results. Compared to the 2019 Habitat Challenge, there were two key changes that increased the task difficulty:Lack of GPS+Compass sensor: The presence of the GPS+Compass sensor used in earlier challenges continually provides the agent with a perfect estimate of the position and heading angle of the goal relative to its current position. Such perfect localization has been exploited in the past by purely geometric  and learned [72] approaches to achieve high-quality PointNav performance. However, such high precision localization is hard to achieve in practice. The 2020 challenge instead requires navigation in the absence of the GPS+Compass sensor. Instead, the goal location is only specified initially at the start of the episode, requiring the agent to accurately keep track of its position in the environment to successfully reach the goal.Noisy actuation and sensing: In the 2020 challenge, RGB-D sensing noise is simulated artificially by using a Gaussian noise-model for the RGB sensor and the Redwood noise model  for the depth sensor. Additionally, actuation noise in the robot motions is simulated by using a noise model obtained from the LoCoBot . We adapted our model in several ways to address these challenges. To address the lack of GPS+Compass sensor, we used an online pose estimator that uses RGB-D inputs x t and x t+1 to estimate the relative change in the pose \u2206p t+1 . These pose changes are summed up over time to track the agent's pose p t+1 . When compared to the original ANS model, we found that using RGB-D inputs gave slightly better estimates and was more computationally efficient than using top-down maps. The pose estimator consists of a 6 convolutional layers followed by 3 fully-connected layers to predict the pose for each modality (RGB, depth) independently. The predictions are combined by using input-conditioned weighting factors that are estimated using a learned MLP with 4 fully-connected layers.To handle noisy sensing, we train our occupancy anticipation model endto-end on the noisy inputs, which gave accurate predictions (see ). We found that OccAnt(depth) gave the best performance, and that adding RGB information to occupancy anticipation did not lead to significant changes in performance.To deal with noisy actuation, we found that the learned pose estimator gave robust estimates of the agent position. Despite having this pose estimator, we experienced large drifts in the estimate over time due to high variance in the actuation noise. To partially mitigate this issue, we focused on efficient navigation with safe planning that maintains sufficient distance from obstacles while planning shortest paths. In practice, we found that reducing the number of collisions leads to faster navigation and lower drift in the pose estimates. We achieve this by using a weighted variant of the classic A-star search algorithm .  Additionally, we incorporated some simple heuristics from the original Active Neural SLAM implementation to update the occupancy maps based on collisions, and used an analytical local policy for navigation instead of a learned policy. : Qualitative results from the 2020 Habitat Challenge: On the left, we show the noisy RGB and depth inputs. On the right, we show the corresponding anticipated and ground-truth occupancy. Our model learns to anticipate accurately in the presence of noise.\n###\n"}
{"text": "#Properties\nimplementation, dataset, has research problem, evaluation, Task, Test questions, Train questions, Question language, Language, On, Question analysis task, Phrase mapping task, Disambiguation task, Query construction task\n#Text\nOur purpose is to hide the complexity of formulating a query expressed in a graph query language such as SPARQL. We propose a mechanism allowing queries to be expressed in a very simple pivot language, mainly composed of keywords and relations between keywords. Our system associates the keywords with the corresponding elements of the ontology (classes, relations, instances). Then it selects pre-written query patterns, and instanciates them with regard to the keywords of the initial query. Several possible queries are generated, ranked and then shown to the user. These queries are presented by means of natural language sentences. The user then selects the query he/she is interested in and the SPARQL query is built. Overview of Our System Process DescriptionThe process of our system is the following. Each element of the user query expressed in the pivot language is matched to an element of the knowledge base. Elements of the knowledge base can either be a class, a property, an instance or a literal type (which can be any type supported by SPARQL, i.e. any type defined in RDF Schema). Then we map query patterns to these elements. The different mappings are presented to the user by means of natural language sentences. The selected sentence allows the final SPARQL query to be built. The Knowledge BaseIn order to allow a relevant interpretation of user queries, our system exploits a knowledge base, made up of an OWL ontology containing terminological knowledge from a specified domain including its associated triple base, notably containing instances from the ontology's classes, their labels and the relations linking them. In its current version, the SWIP system exploits the list of classes, properties, literal types and instances, as well as taxonomic relations between classes and properties. The knowledge base used in the illustrative examples and in the experimentations presented in 4 is made up of the MusicBrainz ontology and dataset, available on the project website 2 .For each class, property and instance, it is essential to define labels (with the rdf:label property) to ensure the smooth functioning of the system. Indeed, the success of the matching step, presented in 3.1, depends on quality and sufficiency of the terms associated with these elements. Definition 1. We define the function lab which associates with each ontology element e kb the set of labels of that element. Example 1. lab(mm:34c63966445c4613afe14f0e1e53ae9a) = {\"Fat Boy Slim\", \"Fat Boy Slin\", \"Norman Cook\"} The Query PatternsQuery patterns constitute the main originality of our approach. They are presented in this section. , r 7 }, and S p1 is the sentence \"An artist c1 , member of a band c2 , has for sibling r2 an artist c3 , parent of r3 an artist c4 , married to an artist c5 from a date l1 to/until a date l2 , collaborated with r4 an artist c6 \".The qualifying elements, and S p2 is the sentence \"a band c1 was formed on/in a date l1 , broke up on/in a date l2 \".For each knowledge base we want to query, we have to build corresponding query patterns. For the moment, this is made manually, but we plan to automate the process. The Pivot Language: Expression of a User QueryIn this section, the pivot language is firstly formally defined, then intuitively explained and illustrated by examples. r c e c h a r a c t e r e x c e p t \"\\\" or n e w l i n e or t h e quote > | ' \\ \\ ' | ' \\ \" ' )+ ' \" ' l i t e r a l : : = i n t e g e r L i t | d a t e L i t i n t e g e r L i t : : = ' i n t e g e r <' intValue | ' ? ' ' > ' d a t e L i t : : = ' date <' ( year '\u2212 ' month '\u2212 ' day )| ( year ' \u2212 ' month ) | ( year ) | ' ? ' ' > ' y e a r : : = i n t V a l u e month : : = ' 1 ' . . ' 1 2 ' day : : = ' 1 ' . . ' 3 1 ' i n t V a l u e : : = ' 0 ' . . ' 9 ' + Definition 4. In a pivot language query q, the elements e1q1, e1q23, e2q2, e2q3, e3q3 which can be extracted from the previous grammar are called user query elements. As we can deduce from the grammar, a user query element can be either a keyword or a literal.elem is the function which associates its set of query elements with q.A subquery is a subset of a query ; it can be of three different kinds: The pivot language we propose is an extension of the language composed of keywords. The optional \"?\" symbol before a keyword means that the user makes the concerned keyword object of the query, i.e. he wants to obtain specific results corresponding to that keyword. The user can express a query by means of simple keywords, without knowing if they can be associated with a class, a relation or an instance, like in the basic query ?\"singer\" which asks for the list of singers in the knowledge base. In that case, we are dealing with a subquery of type Q 1 .The user can also \"qualify\" a keyword with another keyword, separated by a colon, as in the query ?\"singer\": \"married\" which asks for the list of married singers. In this case, we are dealing with a subquery of type Q 2 .Finally, the pivot language also allows to qualify, by means of a keyword, the relationship the user wants to express between two other keywords, as in the query ?\"singer\": \"married\n###\n", "summary": " implementation: SWIP\n, has research problem: Question answering systems\n, Question analysis task: Dependency parser\n, Phrase mapping task: Knowledge base labels/String similarity\n, Disambiguation task: Local disambiguation/User feedback\n, Query construction task: Using templates\n###"}
{"summary": "Social network analysis: PageRank\nBibliographic data source: Web of Science\nScientific network(s): Institutional Citation\nhas research problem: Social Networks", "text": "#Properties\nSocial network analysis, Bibliographic data source, Scientific network(s), has research problem\n#Text\nThe objective assessment of the prestige of an academic institution is a difficult and hotly debated task. In the last few years, different types of university rankings have been proposed to quantify it, yet the debate on what rankings are exactly measuring is enduring.To address the issue we have measured a quantitative and reliable proxy of the academic reputation of a given institution and compared our findings with well-established impact indicators and academic rankings. Specifically, we study citation patterns among universities in five different Web of Science Subject Categories and use the PageRank algorithm on the five resulting citation networks. The rationale behind our work is that scientific citations are driven by the reputation of the reference so that the PageRank algorithm is expected to yield a rank which reflects the reputation of an academic institution in a specific field. Given the volume of the data analysed, our findings are statistically sound and less prone to bias, than, for instance, ad-hoc surveys often employed by ranking bodies in order to attain similar outcomes. The approach proposed in our paper may contribute to enhance ranking methodologies, by reconciling the qualitative evaluation of academic prestige with its quantitative measurements via publication impact. II. RELATION TO PRIOR WORKThe idea of university rankings that are based exclusively on bibliometric statistics is not new. Two well-known global classifications, the Leiden ranking , and the National Taiwan university ranking  rely solely on bibliometric data. Besides, prestige-based procedures to assess scientific impact have been flourishing since PageRank was introduced in the realm of academic evaluation (49): PageRank has indeed been applied to rank scientific journals [in this context, see for instance , or consider the wellknown cases of the Article Influence Score (41), the EigenFactor (5) or the Scimago Journal Ranking ], or to rank individual researchers . The rationale behind the use of PageRank has been well elucidated by , by showing how prestigious citations can be \"effectively and efficiently identified through the source affiliations of the citing paper\". (37) ranked authors of scientific publications based on citation analyses, through the examination of networks of publications, authors and journals. Their results stand in support of the use of PageRank based procedures, rather than non-iterative approaches. Also  evaluated different algorithms that can be applied to bibliographic citation networks to rank scientific papers and authors. While their results recognise the relevance of citations to measure high-impact, their findings also indicate that PageRank based algorithms are better suited to rank important papers or to identify highimpact authors.  propose to overcome the reliance of impact methods on pure quantitative measures by using context based on three specific quality factors, namely sentiment analysis of the text surrounding the citation, self-citations, and semantic similarity between citing and cited article. Their experimental results seem to improve traditional citation counts and are similar to those rendered by PageRank based methods.Surprisingly enough, however, PageRank has been scantly applied at the level of academic institutions, where the noise due to erroneous/missing publication attributions is certainly much smaller than for the case of single researchers. Here, we make the educated guess that universities aggregate citations in the same way as single researchers do, so that institutions with high PageRank have a higher reputation in the network of academic institutions in a given research field.To the best of our knowledge, the first attempt to use PageRank based procedures to evaluate institutions was accomplished by  with the introduction of the Wikipedia ranking of world universities: this ranking was based on the PageRank results applied to the directed networks between articles of 24 Wikipedia language editions. The Wikipedia ranking relies on a statistical evaluation of world universities which, according to their creators \"can be viewed as a new independent ranking being complementary to already existing approaches\". (27) compared their PageRank list of top 100 universities with the ARWU-500 list and found a 62% overlapping, indicating that their analysis gives reliable results.But Wikipedia citation patterns may escape the dynamics that actually shape reputation in the Academic world. Therefore, we aim in this work at applying PageRank readily to the network of citations that institutions build by citing each others' scientific publications. The dynamics that generate those networks should resemble more closely those that are behind the construction of academic prestige (or the perception thereof). Much in the same way as (27) used a well established global ranking to test the reliability of their results, we will also try to compare our results with well established evaluation efforts to check the validity of our approach. Clearly, citation patterns and reputation depend closely on the academic field. For this reason, we carry out our analysis on five distinct Web of Science categories which, incidentally, map oneto-one onto five scientific fields covered by the ARWU thematic Global Rankings of Academic Subjects (ARWU-GRAS). Our work helps to shed some light on how the academic prestige of ranked institutions is captured by the GRAS rankings and helps reconciling 'qualitative' and 'quantitative' ranking approaches by providing a method that combines publication metrics and reputation in a quantitative fashion. III. MATERIALS AND METHODS A. A brief account of the ARWU Global Ranking of Academic SubjectsLaunched in 2017, the Global Ranking of Academic Subjects (ARWU-GRAS) ranks institutions, presenting a minimum number of research articles in a five year period, in 52 subjects across natural sciences, engineering, life sciences, medical sciences, and social sciences. Four bibliometric indicators (related to the scientific production from 2011 to 2015 for the 2017 edition of the ranking) are present in all the subjects. For each institution and academic subject, those indicators are: PUB number of papers \"article\" type) authored by an institution. Besides, a fifth indicator, AWARD, related to winners of specific awards applies to 30% of the ARWU-GRAS academic subjects. Different publication thresholds and sets of indicator weights are used depending on the academic subject. The Ranking Methodology webpage(? ) describes the indicators in more detail. For each indicator, ARWU-GRAS scores are calculated following a procedure that can be summarized as follows: first multiply each value of the gathered raw data by a fixed scaling factor so that the largest raw value is scaled to 10000. Then, compress the dynamic range of the scaled raw data by taking its square root to form the indicator score . Finally, using the weights allocated by ARWU-GRAS to each of the ranked subjects, compute the weighted sum of the indicator scores.The weights used by ARWU-GRAS for the indicators in the five academic subjects are listed in . Five Web of Science categories have been analysed in this paper: Dentistry, Oral Surgery & Medicine; Business, Finance; Information Science & Library Science; Telecommunications; Veterinary Sciences. The choice of those five Categories is not accidental: indeed, they correspond to ARWU-GRAS equivalents as listed in . Moreover, three of the chosen WoS categories (Dentistry, Oral Surgery & Medicine, Business, Finance, and Veterinary Sciences) can also be qualitatively mapped to three corresponding QS thematic rankings. QS thematic rankings are produced annually to identify top universities in a specific subjects. To do so, QS uses citations as well as global surveys of employers and academics.The choice of scientific subjects, therefore, enables us to make meaningful comparisons between our method and well established academic rankings. To that effect, we have downloaded the overall scores in the QS ranking of the institutions shown in their official webpage -50 institutions in Veterinary Science and Dentistry, and 200 in Accounting and Finance. Since we can reproduce the results of ARWU-GRAS using direct data from the bibliometric suite InCites, we have made use of the official scores for institutions that are listed on the ARWU-GRAS website, and have extended the subject rankings to include all the institutions over the threshold of the minimum number of publications set by the ARWU-GRAS methodology. The list of institutions (ARWU-GRAS webpage) comprised 200 universities in all the subjects considered here, but Telecommunication Engineering (300) and Library & Information Science (100). The total number of institutions analysed in the paper is shown in . We analyse bibliometric records classified as 'articletype' in the Clarivate's Web of Science database, in any of the research categories corresponding with the list of five subjects included in . Data from the InCites platform containing all articles published in the time window 2010-2014 (both included) were provided by Clarivate Analytics in raw markup language files. For each publication, we retained the affiliation of all authors and of all references, respectively.The data comprise 188,533 unique bibliographic records ascribed to 5,063 unique affiliations and citing 2,907,556 indexed references. In order to compare results with the ARWU-GRAS rankings, for each WoS category only records pertaining to affiliations showing a total number of articles in excess of the publication threshold of the corresponding ARWU-GRAS subject ranking were retained. In turn, only cross-citations among those publications were considered to build the networks analysed and to compute the corresponding PageRank score. Once this specific subset of affiliations, publications, and citations was retained for each WoS category, we ended up with five different networks whose characteristics are reported in the next section. C. Network properties and metricsIn Network Theory, a weighted, directed networkand a (possibly) non-symmetric matrix of weights \u03c9 = {\u03c9 ij } N i,j=1 . For each \u03c9 ij = 0 it exists a weighted and directed link between nodes i and j of the network, with an associated weight \u03c9 ij . The adjacency matrix A of the network is given by A ij = 1 \u2212 \u03b4 0,\u03c9ij , where \u03b4 ab is the Kronecker delta: A ij is one whenever there is a (directed) link connecting node i to node j and zero otherwise.Based on the definition above, the in-degree kwhile the in-degree distribution is given byFinally, the degree centrality c i of a node i is equal toand it is upper bounded by 1 (when excluding self-links).Since the networks we analyse differ in size, their node can attain a different maximum degree. For this reason, to make meaningful comparison among the different networks, in  we plot the degree centrality distribution, defined as:and refer to degree centrality when mentioning 'central' nodes in Sec. IV. D. The PageRank algorithmThe PageRank algorithm was devised in the 1970s by  and then popularised in the 1990s to rank web pages according to their 'popularity': it was in fact recast as a search-engine ranking algorithm that would prioritise pages with either many incoming web-links or with a few incoming links from highly-ranked pages. In other words, the algorithm assigns a high score not only to those pages that are highly connected, but also to those ones that are linked by popular websites.In its web application, the model behind the algorithm assumes there is a web-surfer who follows links between web pages and who, after a series of moves, gets bored and lands on a random page. The PageRank of a given page is therefore linked to the probability a random surfer would land to the page. The model can therefore be seen as a Markov process whereby states are pages and the transition probabilities are given by the links among webpages. Therefore, it is not surprising that the calculation of the PageRank is very similar to the derivation of the Markov stationary distribution. Concretely, the equation fixing the PageRank \u03c0 reads:where 1 is the unitary N -dimensional vector and where the elements\u03c9 ij of the matrix\u03c9 are given b\u1ef9the weight matrix of the network considered (see Sec. III C above). The quantity d is called 'damping factor' and is linked to the probability of leaving the current page and landing on a random website. This factor, together with the first term on the rhs of Eq. (5) are included to ensure a transition when landing on a page without out-going links, so to preserve the ergodicity of the process and to ensure the convergence of \u03c0 to a unique stationary density . The PageRank is usually computed iteratively, with an initial guess \u03c0  that gets updated by applying Eq. (5) above as:The result of this computation yields the N -dimensional \u03c0 vector that expresses the probability of visiting any given page i = 1, . . . N , i.e., in other words, the 'popularity' of that page. In our work, we deal with bibliographic citations among institutions in a fashion akin to links among webpages. In this setting, we effectively derive the popularity \u03c0 of any given institution in a network of citations. Note that, at odds with other approaches (see, for instance, (38)), we use here an unweighted version of the PageRank algorithm, which does not take into account the total number of publications produced by each institution. We chose this path because the disciplinary areas we analyse are rather compact and the typology (and size) of the different actors is fairly comparable: the size of the institutions considered here is narrowly distributed around the mean, modulo some few extremely small institutions that (we checked) would be over-rewarded by a normalised version of the PageRank algorithm. For this reason, we finally decided to apply an unweighted version of the PageRank algorithm in the present work. IV. COMPUTING THE UNIVERSITY REPUTATION VIA PAGERANKOur aim is to measure aggregate citations from a certain institution to some other academic body, in a given field.To that end, we aggregated all publications at the affiliation level, and were thus able to reconstruct for each WoS category the web of cross-citations among institutions built in the 2010-2014 time period. The resulting system consists of a weighted network N (n, \u03c9), where each node i \u2208 n is an academic institution and where weighted edges \u03c9 ij \u2208 \u03c9 are the total number of citations occurring within the specific WoS category from publications produced by institution i to publications produced by institution j.The network characteristics we obtained for each category are summarised in , where we report the total number of records, the total number of institutions to which the authors of these publications were affiliated, and the total citations retrieved in the dataset.  shows instead the in-degree centrality distribution P (k (in) /(N \u2212 1)) of those networks (see Sec. III for details): these statistics allow to get a glimpse of the structure of each network and enable one to understand how connected the hubs in network are. For each WoS category analysed, we computed the weighted PageRank of the resulting network of citations. This calculation allowed us to assign a score to each academic institution: this score is related both to 'quantity' through the number of aggregated citations any given institution has received and to 'quality' according to the provenance of those citations (for more details see Sec. III). The rationale behind our ap-proach is that researchers are expected to cite the most reputable source in their publications and that entities cited by reputed sources are expected to be, in turn, reputable. As a consequence of this mechanism, one expects to be able to quantify academic reputation by measuring citation patterns via PageRank and without the means, for instance, of dedicated surveys.Note that since PageRank takes into account the reputation of citing institutions, one can circumvent shortcomings due, for instance, to the emergence of 'citation cartels' , which are not accounted for by mere citation counts. The above phenomenon has been in fact observed in a few instances , especially in relation to research evaluation schemes that take into account the citations researchers receive . Therefore our method allows to strongly discount those effects due to the appearance of clusters of institutions citing each other more frequently than expected.As we discuss further below in the following subsections, PageRank results are qualitatively in good agreement with the QS thematic rankings for all those areas covered by QS: this first finding suggests that PageRank is indeed capable of reproducing the academic reputation as perceived by surveys, only by means of quantitative methods. A. Viability of the PageRank algorithm to rank academic institutionsBefore getting into deeper analyses, we wanted to assess the soundness of our approach against some well established academic ranking standard. To that aim, we compared the PageRank scores we obtained for each institution in each WoS category with their respective score in the corresponding ARWU-GRAS ranking. ARWU-GRAS rankings are just one possible benchmark of our results. We chose to compare with them because: i. there is a precise mapping that links the different WoS Categories to each ARWU-GRAS Subject, so that we are sure we are comparing apples with apples, and ii. ARWU-GRAS are built on bibliometric indicators computed from the InCites suite, so that the data consistency is ensured.To carry out our comparisons we started by computing the Pearson and Spearman Correlations, as well as Kendall's coefficient of concordance between the two scoring systems. Before computing the Pearson Correlation we proceeded to normalise the PageRank scores using the ARWU-GRASS compression procedure explained in Section III A, i.e. we took the square root of the PageRank scores normalised over the maximum score.  reports Pearson's and Spearman's correlation values, as well as Kendall's coefficient of concordance between ARWU-GRAS and PageRank scores, while in  we show, for each WoS category, a scatter plot comparing the PageRank and the ARWU-GRAS normalised scores as well as the citation (CIT) score, which, albeit not considered in ARWU-GRAS could be considered as a viable, simpler alternative to PageRank. According to the results shown in , we found that ARWU-GRAS scores and PageRank results have a significant correlation. Besides, the concordance between rankings as signaled by the non parametric statistic Kendall's W, the most familiar measure for concordance , is strong across the five subjects. However, as one can observe in , ARWU-GRAS (and CIT, for that matter) and PageRank are by no means interchangeable: the point clouds are in fact rather widely scattered around a trend line at fixed PageRank values.To better understand the source of the observed correlations, and to check to what extent it is due, e.g., to size effects, we performed two further analyses:1. we performed a Principal Component Analysis (PCA) on the overall set of the 5 WoS Categories;2. we carried out a partial correlation analysis between the PageRank results and the ARWU-GRAS rankings.Thus, we first merged all WoS categories and performed a Principal Component Analysis (PCA) on the space spanned by the following metrics: Category Normalized Citation Impact (cnci), Total number of  publications (pub), Total number of citations (CIT), ARWU-GRAS total score (arwu), PageRank score (pr), and H-index (hindex). The merged dataset we used to perform PCA consists thus of 2145 observation points in a 6-dimensional space, with the correlation matrix shown in . We retained two principal components which jointly contribute to explain in excess of 89% of the variance in the sample. By projecting the original 6 dimensions onto a reduced 2-dimensional space, one is able to check which of the above metrics lie closer together (i.e. which metrics capture similar features). The result of this effort is shown in  , which shows how the metrics listed above lie in the PCA reduced space after varimax rotation. The first and second rotated component explain 61% and 26% of the variance of the sample, respectively. Surprisingly enough, the PageRank score and the ARWU-GRAS total score are found to be sitting very close together, suggesting that the ARWU-GRAS ranking is effectively capable of capturing the academic reputation of a given institution.Once we have uncovered and weighed the correlation between ARWU-GRAS and PageRank scores, we would like next to gauge the degree of association between the two rankings by removing a series of controlling underlying variables. To that effect, we have conducted a partial correlation analysis between ARWU-GRAS and PageRank scores, by controlling both for size-dependent (PUB and CIT) and size-independent (CNCI) variables, respectively. The rationale behind partial correlation analysis is that the value of correlation coefficients get reduced insofar as the controlling variable exerts influence in the association between the two ranking schemes.The results are shown in , and contribute to shed a clear light on the relationship between ARWU-GRAS and PageRank: FIG. 3. A Principal ComponentAnalysis plot that compares of a few metrics with the ARWU-GRAS scores. This plot allows to understand which metrics ARWU-GRAS are better captured by the ranking total score and which one resembles most the PageRank results: the ARWU-GRAS total score (score) is found to be sitting very close to the PageRank results (pra), meaning that PageRank and ARWU-GRAS capture very similar features of the cohort of academic institutions considered. Note that the first component of the plot is mostly aligned with the 'PUB' metric, which is tightly correlated with the size of the institution, while the second component is mainly aligned with the size-independent 'CNCI' metric: both PageRank and ARWU-GRAS are found to be somewhere in between these two limits.\u2022 The indicator related to publication impact (CNCI) does not bear any noticeable influence in the relationship between ARWU-GRAS and PageRank scoring. Since CNCI is an integral part of the composed ARWU-GRAS score, this fact stands in support of the difference in which ARWU-GRAS and PageRank acknowledge reputation.\u2022 The indicator related to the number of publications impact (PUB) exerts a moderate influence in the relationship between ARWU-GRAS and PageRank ranking, signaling that both ARWU-GRAS and PageRank scores are to a certain extent connected with institutional size.\u2022 Except for Business, Finance, the partial correlations between ARWU-GRAS and PageRank scores do not reach statistical significance when controlling for the indicator CIT. Both methods appear to use up the information provided by the size-dependent indicator related to the total number of citations. Not surprisingly, by being PageRank built upon citation patterns, the CIT variable is the one conveying most of the correlation between PageRank and ARWU-GRAS. However, results yielded by the two approaches differ markedly when one looks at a finer grain the institutional changes in positions in both classifications. Within each subject, we have computed the absolute value of the change of position of all the institutions as ranked by ARWU-GRAS and PageRank. We then collected relevant descriptive statistics of the so constructed new variables to assess the extent to which PageRank departs from ARWU-GRAS in recognising academic reputation.  provides, for the five subjects under analysis, means, standard deviations, medians, and a collection of key percentile values for the differences (absolute value) in position in ARWU-GRAS and PageRank of all the institutions. We have included the median as well as percentiles 75 and 90 which will point to the behavior of the ranking different for the 50%, 25% or 10% of the institutions, respectively. The results shown in  reveal large position swaps in both classifications (e.g., by computing the ratio of the the last column (P90) over the second one (N), we realize that 10% of the institutions in each subject suffer a rank change of about 30% of the total number of universities included in the sample), uncovering differences in both ranking methodologies in spite of the observed significant correlations between them.These differences highlight how PageRank is a useful protocol to capture reputation rather than impact: indeed, while citations set a common trend between ARWU-GRAS and PageRank results (hence the observed correlations), with PageRank it does not only matter how many citation an institution aggregates, but also the provenance of them.A finer analysis is presented for each single WoS Category in the next few sections, where we also show and discuss briefly the properties of each resulting citation network. B. Dentistry, Oral Surgery & MedicineThe institutional citation network for Dentistry, Oral Surgery & Medicine is relatively dense, compared with the other cases analysed further below: indeed, the in-degree centrality distribution of this particular network has a much fatter tail than those emerging from the other categories, as shown in . This is also consistent with the numbers reported in , where the citations-to-institutions ratio is the highest. This characteristic is intrinsically related with the citation habits of the disciplines within the field of the Health and Medical Sciences, where papers usually include more references as compared to other disciplines .This feature makes, in turn, the institutional network more interconnected, since more references per publication directly imply more affiliations cited. This is clearly visible by looking at the actual shape of the network analysed in : the network edges (i.e. citations from one institution to another) cover indeed the whole background space. Despite the overall large connectivity, a cluster of central hub institutions (in terms of in-degree centrality) can be easily detected in the network: these are the institutions receiving more citations from their peers and are the ones for which we show names in  When computing the PageRank, we find as the best scored the University of S\u00e3o Paulo, the University of Gothenburg and the University of Bern, respectively. These institutions are also among the central core of the citation network, meaning that those universities not only receive many citations, but they also do so from equally prestigious institutions. The list of the top 10 institutions for reputation measured via PageRank is given in Table VII, while the full list is provided in the Supplementary material. Importantly, out of these ten institutions, four of them (the University of Michigan, the University of Hong Kong, the Academic Center for Dentistry Amsterdam and Harvard University, respectively) are featured in the top 10 universities of the QS thematic ranking for Dentistry, in terms of the metric academic reputation.One can also compute the Spearman's correlation coefficient on total scores between QS and ARWU GRAS and PageRank, for the the top 20 institutions of the QS Subject ranking. By doing so, one sees that the Spearman coefficient between QS and ARWU is \u03c1 = 0.06, while between QS and PageRank is \u03c1 = 0.28. These facts suggest PageRank is actually capable of capturing the reputation of a given institution, as expected, and to go beyond ARWU GRAS results. But while the academic reputation score in the QS ranking is obtained by means of surveys, whose control in terms of significance and robustness is hard to attain, here we derived a similar score based only on bibliometric data. C. Business, FinanceThe second category we study is Business, Finance. This network is the second smallest in terms of publications and citations, suggesting there exist a fragmentation pattern in knowledge communication and sharing in this field. By looking at the actual shape of the network in , one can appreciate that 'peripheral' nodes are effectively little connected but that, at the same time, there is a densely inter-connected cluster of hub institutions at the center. The in-degree centrality distribution shown in  has indeed a fatter tail distribution in this case if compared with the Information Science & Library Science WoS category, for instance.The PageRank results are, once again, fairly interesting. In the top three position, we find the University of Pennsylvania, the New York University and Stanford University, respectively. The three of them belong to the central cluster of knowledge hub institutions. The list of top 10 institutions is given in , while the full rank is given in the Supplementary Materials. In the top 10 we find 6 institutions (Harvard University, and Stanford University, Massachusetts Institute of Technology, University of Chicago, University of Pennsylvania, and New York University, respectively) that are featured in the first 10 positions in the QS Accounting and Finance thematic ranking, for academic reputation. Also, considering the top 20 institutions featured in the QS Subject ranking, the Spearman's correlation coefficient between the QS scores and ARWU GRAS scores is \u03c1 = 0.59, while for the case of PageRank it equals \u03c1 = 0.6. Again, these findings are a strong indication that PageRank is indeed capable of capturing the reputation of an Academic institution based solely on bibliometric data and to go beyond ARWU results. FIG. 5.The institutional network of cross-citations in the Business, Finance WoS category. Edges are citations from a publication produced by an institution to those authored by another one (10% of the total edges are plotted). The node size is proportional to the number of publications. D. Information Science & Library ScienceWe next examined the case of Information Science & Library Science, which is of special interest for us, considering the focus of the present work. This field is the one with the fewer publications and citations with respect to the set of WoS Categories analysed in this paper. The corresponding network shown in  is indeed much sparser than the previous ones and the respective in-degree centrality distribution decays much faster than in the other cases (see ).In this case, there is no sharp cluster of central hubs of knowledge as in the previous cases, albeit some key nodes (in terms of in-degree centrality) can be identified in the core of the network (as, e.g., the Indiana University at Bloomington, the University of Amsterdam and the National University of Singapore). However, these are not as interconnected as the central core nodes in the Dentistry case, for instance. This finding TABLE VIII. The Top 10 PageRank institutions in Business, Finance. According to the PageRank metrics and definitions, these are the most 'reputable' academic institutions in this WoS category. The three columns show the position in the PageRank, the classification in the ARWU-GRAS (computed according to ) and the academic institution, respectively.PageRank ARWU-GRAS rank Institution to be very meaningful without much further checking: we find indeed all the 'usual suspects' to be in the top 10 of the ranking. It is noteworthy the compactness of the northern European group, University of Amsterdam-Leiden-KU Leuven: these universities are clearly a world reference and thus high in the PageRank because of their global impact. However, by frequently cross-citing each other, they produce a supplementary 'catalysing effect' which boosts their score towards the top of the ranking. E. TelecommunicationsThe next case study is the Telecommunications WoS category. This particular network is densely connected and it is hard to detect a central cluster of emerging institutions. A clear, emerging feature is, in this case, the massive presence of Chinese institutions. The top 3 institutions according to the PageRank metrics are the University of Texas at Austin, Tsinghua University and the University of Waterloo, respectively. It is worth noting in this case that not all the top PageRank institutions are the ones with the largest in-degree centrality: indeed, a large number of citations does not necessarily imply a high PageRank, meaning that the reputation of the citing institutions and not only the quantity of citations matters in PageRank.Albeit there is no QS reputation specific ranking corresponding to this category, the top institutions we found are all featured in the top positions of the ARWU-GRAS ranking of Telecommunication Engineering. The list of the top 10 institutions is given Table X, while the full results are provided in the Supplementary Materials. F. Veterinary SciencesThe last WoS category we analyse is Veterinary Sciences. Again, the resulting network is fairly connected: in this case as well it is possible to detect a central cluster of 'influencing' institutions, that -interestingly -belong to very different geographical regions. Among those, we may name for instance the University of California at Davis, the Royal Veterinary College and Ghent University.We computed the PageRank for the last time on this particular network and found the top three universities to be the University of California at Davis, the Royal Veterinary College and the University of Pennsylvania: therefore, some of the main hubs in this case do also correspond with the most reputable institutions in the field. The top 10 PageRank institutions in this category are listed in , while the full list is provided in the Supplementary Materials. We can again compare the results we obtain for the top listed institutions with the QS thematic ranking in Veterinary. Five of the institutions listed in  are found to be in the top 10 QS Veterinary when ranked for academic reputation (in particular, the first two are found to coincide), a fact that hints again that PageRank is indeed capable of capturing the reputation of a certain institution. Furthermore, when computing the Spearman's correlation coefficient \u03c1 on total scores between QS and ARWU GRAS and PageRank, for the the top 20 institutions of the QS Subject ranking, one finds \u03c1 = 0.16 between QS and ARWU and \u03c1 = 0.44 between QS and PageRank, again implying that PageRank results enables one to gain more information than merely using ARWU GRAS rankings. VI. CONCLUSION AND FURTHER WORKIn this paper, we have explored the use of a quantitive, and reliable proxy, the PageRank algorithm, to assess academic reputation through the analysis of citation patterns among universities. For the analysis we have selected five different Web of Science categories, corresponding to research subjects studied by well established international academic classifications.To support the soundness of our work, we have supplied compelling evidence about the close connection of the results of the PageRank algorithm with the scores on two rankings that handle academic reputation in two distinct ways: scores partially based on academic surveys, QS, or driven by publication and staff figures, ARWU-GRAS. These two academic rankings do have their shortcomings: for instance, ARWU suffers limitations due to aggregation methodologies and with the chosen criteria (6), while the return rate of the reputational surveys and reliability of the statistical data were questioned for QS . However, although those two ranking methodologies do not enjoy the favour of the whole academic community, we believe they -at least superficially -capture some features of academic excellence. Because in this work we are proposing a new framework for ranking academic institutions, we therefore felt necessary to compare our results with those two standards, despite the controversy they both stir.The fact that the PageRank algorithm operates with hard data obtained through the analysis of citation patterns among papers published in peer review publications, well rooted, therefore, in a sound and credible mechanism for recognising reputation among researchers and institutions, makes PageRank a very reasonable candidate to be used as a direct mean to assess academic reputation. We believe that the results from our paper provide a solid argument in favor of using PageRank scores based only on bibliometric instead of (or along with) estimations through surveys to measure academic reputation.The current study offers a new glimpse into the analysis of scientific reputation via PageRank that adds to the large body of literature in which network analysis and bibliometrics are combined. The work carried out in this study may be extended along several research lines, by exploiting a few Network Science techniques. In this sense, a few interesting research venues we are planning to explore are for instance related to the measurement of the 'boost' in Academic Prestige due to institutional self-citations (which is expected to enhance the PageRank score and, thus, to correlate positively with academic ranking scores), to the detection of communities (i.e. of groups of academic institutions cross-citing each other more frequently than expected) and, finally, to the relation of the above two properties to the geographical location of the analysed institutions.\n###\n"}
{"summary": "has benchmark: Benchmark SQuAD1.1/Benchmark SQuAD1.1/Benchmark SQuAD1.1/Benchmark SQuAD1.1 dev\nhas research problem: Question Answering/Question Answering/Question Answering\nhas model: Multi-Perspective Matching (single model)/Multi-Perspective Matching (ensemble)/MPCM\nsame as: https://en.wikipedia.org/wiki/Question_answering/https://en.wikipedia.org/wiki/Question_answering/https://en.wikipedia.org/wiki/Question_answering", "text": "#Properties\nhas benchmark, has research problem, has model, same as\n#Text\nPrevious machine comprehension (MC) datasets are either too small to train endto-end deep learning models, or not difficult enough to evaluate the ability of current MC techniques. The newly released SQuAD dataset alleviates these limitations, and gives us a chance to develop more realistic MC models. Based on this dataset, we propose a Multi-Perspective Context Matching (MPCM) model, which is an end-to-end system that directly predicts the answer beginning and ending points in a passage. Our model first adjusts each word-embedding vector in the passage by multiplying a relevancy weight computed against the question. Then, we encode the question and weighted passage by using bi-directional LSTMs. For each point in the passage, our model matches the context of this point against the encoded question from multiple perspectives and produces a matching vector. Given those matched vectors, we employ another bi-directional LSTM to aggregate all the information and predict the beginning and ending points. Experimental result on the test set of SQuAD shows that our model achieves a competitive result on the leaderboard. Task DefinitionGenerally, a MC instance involves a question, a passage containing the answer, and the correct answer span within the passage. To do well on this task, a MC model need to comprehend the question, reason among the passage, and then identify the answer span.  demonstrates three examples from SQuAD. Formally, we can represent the SQuAD dataset as a set of tuples (Q, P, A), where Q = (q 1 , ..., q i , ..., q M ) is the question with a length M , P = (p 1 , ..., p j , ..., p N ) is the passage with a length N , and A = (a b , a e ) is the answer span, a b and a e are the beginning and ending points and 1 \u2264 a b \u2264 a e \u2264 N . The MC task can be represented as estimating the conditional probability Pr (A|Q, P ) based on the training set, and  where A(P ) is a set of answer candidates from P . As the size of A(P ) is in the order of O(N 2 ), we make a simple independent assumption of predicting the beginning and endding points, and simplify the model aswhere Pr(a b |Q, P ) (or Pr(a e |Q, P )) is the probability of the a b -th (or a e -th) position (point) of P to be the beginning (or ending) point of the answer span. Multi-Perspective Context Matching ModelIn this section, we propose a Multi-Perspective Context Matching (MPCM) model to estimate probability distributions Pr(a b |Q, P ) and Pr(a e |Q, P ).  shows the architecture of our MPCM model. The predictions of Pr(a b |Q, P ) and Pr(a e |Q, P ) only differentiate at the last prediction layer. And all other layers below the prediction layer are shared.Given a pair of question Q and passage P , the MPCM model estimates probability distributions through the following six layers.Word Representation Layer. The goal of this layer is to represent each word in the question and passage with a d-dimensional vector. We construct the d-dimensional vector with two components: word embeddings and character-composed Word Representation LayerFilter Layer embeddings. The word embedding is a fixed vector for each individual word, which is pre-trained with GloVe  or word2vec . The character-composed embedding is calculated by feeding each character (also represented as a vector) within a word into a Long Short-Term Memory Network (LSTM) . The output of this layer is word vector sequences for question Q : [q 1 , ..., q M ], and passage P : Context Representation Layer Aggregation Layer softmax softmax Prediction LayerFilter Layer. In most cases, only a small piece of the passage is needed to answer the question (see examples in ). Therefore, we define the filter layer to filter out redundant information from the passage. First, we calculate a relevancy degree r j for each word p j in passage P . Inspired from Wang et al. (2016b), we compute the relevancy degree r i,j between each word pair q i \u2208 Q and p j \u2208 P by calculating the cosine similarity, and get the relevancy degree by r j = max i\u2208M r i,j . Second, we filter each word vector by p j = r j \u2022 p j , and pass p j to the next layer. The main idea is that if a word in the passage is more relevant to the question, more information of the word should be considered in the subsequent steps.Context Representation Layer. The purpose of this layer is to incorporate contextual informa-tion into the representation of each time step in the passage and the question. We utilize a bidirectional LSTM (BiLSTM) to encode contextual embeddings for each question word.Meanwhile, we apply the same BiLSTM to the passage:Multi-Perspective Context Matching Layer. This is the core layer within our MPCM model. The goal of this layer is to compare each contextual embedding of the passage with the question with multi-perspectives. We define those multiperspective matching functions in following two directions:First, dimensional weighted matchings withwhere v 1 and v 2 are two d-dimensional vectors, W \u2208 l\u00d7d is a trainable parameter, l is the number of perspectives, and the returned value m is a l-dimensional vector m = [m 1 , ..., m k , ..., m l ]. Each element m k \u2208 m is a matching value from the k-th perspective, and it is calculated by the cosine similarity between two weighted vectorswhere \u2022 is the elementwise multiplication, and W k is the k-th row of W , which controls the k-th perspective and assigns different weights to different dimensions of the d-dimensional space. Second, on the orthogonal direction of f m , we define three matching strategies to compare each contextual embedding of the passage with the question:(1) Full-Matching: each forward (or backward) contextual embedding of the passage is compared with the forward (or backward) representation of the entire question.(2) Maxpooling-Matching: each forward (or backward) contextual embedding of the passage is compared with every forward (or backward) contextual embeddings of the question, and only the maximum value is retained.(3) Meanpooling-Matching: This is similar to the Maxpooling-Matching, but we replace the max operation with the mean operation.Thus, the matching vector for each position of the passage is the concatenation of all the matching vectorsFor the examples in , the forward Full-Matching vector is extremely useful for question #1, because we only need to match the left context to the entire question. Similarly, the backward Full-Matching vector is very helpful for question #2. However, for question #3, we have to utilize the Maxpooling-Matching and Meanpooling-Matching strategies, because both the left and right contexts need to partially match the question.Aggregation Layer. This layer is employed to aggregate the matching vectors, so that each time step of the passages can interactive with its surrounding positions. We incorporate the matching vectors with a BiLSTM, and generate the aggregation vector for each time step.Prediction Layer. We predict the probability distributions of Pr(a b |Q, P ) and Pr(a e |Q, P ) separately with two different feed-forward neural networks (shown in , solid-lines for Pr(a b |Q, P ), dotted-lines for Pr(a e |Q, P )). We feed the aggregation vector of each time step into the feed-forward neural network individually, calculate a value for each time step, then normalize the values across the entire passage with sof tmax operation. Experiments Experiment SettingsWe evaluate our model with the SQuAD dataset. This dataset includes 87,599 training instances, 10,570 validation instances, and a large hidden test set 1 . We process the corpus with the tokenizer from Stanford CorNLP . To evaluate the experimental results, we employ two metrics: Exact Match (EM) and F1 score .To initialize the word embeddings in the word representation layer, we use the 300-dimensional GloVe word vectors pre-trained from the 840B Common Crawl corpus . For the out-of-vocabulary (OOV) words, we initialize the word embeddings randomly. We set the hidden size as 100 for all the LSTM layers, and set the number of perspectives l of our multiperspective matching function (Equation (5)) as 50. We apply dropout to every layers in , and set the dropout ratio as 0.2. To train the model, we minimize the cross entropy of the be-  ginning and end points, and use the ADAM optimizer  to update parameters. We set the learning rate as 0.0001. For decoding, we enforce the end point is equal or greater than the beginning point.  summarizes the performance of our models and other competing models. Our single MPCM model achieves the EM of 65.5, and the F1 score of 75.1. We also build an ensemble MPCM model by simply averaging the probability distributions of 5 models, where all the models have the same architecture but initialized with different seeds. With the help of the simple ensemble strategy, our MPCM model improves about 3% in term of EM, and 2% in term of F1 score. Comparing the performance of other models, our MPCM models achieve competitive results in both single and ensemble scenarios. Results on the Test Set Influence of the Multi-Perspective Matching FunctionIn this sub-section, we study the influence of our multi-perspective matching function in Eq.(5). We built a baseline model vanilla-cosine by replacing Eq.(5) with the vanilla cosine similarity function. We also varied the number of perspectives l among {1, 10, 30, 50}, and kept the other options un-   changed.  shows the performance on the dev set. We can see that, even if we only utilize one perspective, our multi-perspective matching function works better than the vanilla-cosine baseline. When increasing the number of perspectives, the performance improves significantly. Therefore, our multi-perspective matching function is really effective for matching vectors. Layer AblationIn this sub-section, we evaluate the effectiveness of various layers in our MPCM model. We built several layer ablation models by removing one layer at a time. For the Multi-Perspective Context Matching Layer, we cannot remove it entirely. Instead, we built three models (w/o Full-Matching, w/o Maxpooling-Matching, w/o Meanpooling-Matching) by eliminating each matching strategy individually.  shows the performance of all ablation models and our full MPCM model on the dev set. We can see that removing any components from the MPCM model decreases the performance significantly. Among all the layers, the Aggregation Layer is the most crucial layer. Among all the matching strategies, Maxpooling-Matching has the biggest effect. what is when how many what was which where what did in what what does what type  what are why how much what do what kind how did what year how long in which what were what has what can (44) what did what do what can what was what does where what type  which what is what are how much who how many what kind how long when in what in which what year (71) F1 EM : Performance for different question types. Result AnalysisTo better understand the behavior of our MPCM model, we conduct some analysis of the result on the dev set.  shows the performance changes based on the answer length. We can see that the performance drops when the answer length increases, and the EM drops faster than the F1 score. The phenomenon reveals that longer answers are harder to find, and it is easier to find the approximate answer region than identify the precise boundaries.  shows the performances of different types of questions. The numbers inside the brackets are the frequency of that question type on the dev set. We can see that the performances for \"when\", \"what year\", \"in what\", and \"in which\" questions are much higher than the others. The possible reason is that the temporal expressions are easier to detect for \"when\" and \"what year\" questions, and there is an explicit boundary word \"in\" for \"in what\" and \"in which\" questions. Our model works poorly for the \"how did\" question. Because \"how did\" questions usually require longer answers, and the answers could be any type of phrases.  visualizes the probability distributions produced by our MPCM model for an example question from the dev set, where the upper subfigure is the probabilities for the beginning point and the lower one is the probabilities for the ending point. We can see that our model assigns most mass of the probability to the correct beginning and ending points.To conduct the error analysis, we randomly select 50 incorrect questions from the dev set. We found that predictions for 16% questions are acceptable (even though they are not in the correct answer list) and 22% overlap with the correct answer. 14% of the questions require reasoning across multiple sentences, and most of the remaining questions require external knowledge or complex reasoning. ConclusionIn this work, we proposed the Multi-Perspective Context Matching (MPCM) model for machine comprehension task. Our model identifies the answer span by matching each time-step of the passage with the question from multiple perspectives, and predicts the beginning and ending points based on globally normalizing probability distributions. Ablation studies show that all aspects of matching inside the MPCM model are crucial. Experimental result on the test set of SQuAD shows that our model achieves a competitive result on the leaderboard.\n###\n"}
{"summary": "has benchmark: Benchmark TACRED/Evaluation Score 87:1/Benchmark SemEval-2010 Task 8/Benchmark TACRED\nhas research problem: Relation Extraction/Relation Extraction\nhas model: Alt et al. (2019)/TRE\nsame as: https://en.wikipedia.org/wiki/Relationship_extraction/https://en.wikipedia.org/wiki/Relationship_extraction", "text": "#Properties\nhas benchmark, has research problem, has model, same as\n#Text\nCurrent state-of-the-art relation extraction methods typically rely on a set of lexical, syntactic, and semantic features, explicitly computed in a pre-processing step. Training feature extraction models requires additional annotated language resources, which severely restricts the applicability and portability of relation extraction to novel languages. Similarly, pre-processing introduces an additional source of error. To address these limitations, we introduce TRE, a Transformer for Relation Extraction, extending the OpenAI Generative Pre-trained Transformer [Radford et al., 2018]. Unlike previous relation extraction models, TRE uses pre-trained deep language representations instead of explicit linguistic features to inform the relation classification and combines it with the self-attentive Transformer architecture to effectively model long-range dependencies between entity mentions. TRE allows us to learn implicit linguistic features solely from plain text corpora by unsupervised pre-training, before fine-tuning the learned language representations on the relation extraction task. TRE obtains a new state-of-the-art result on the TACRED and SemEval 2010 Task 8 datasets, achieving a test F1 of 67.4 and 87.1, respectively. Furthermore, we observe a significant increase in sample efficiency. With only 20% of the training examples, TRE matches the performance of our baselines and our model trained from scratch on 100% of the TACRED dataset. We open-source our experiments and code 1 . SentenceSubject Object Relation Mr. Scheider played the police chief of a resort town menaced by a shark. Scheider police chief per:titleThe measure included Aerolineas's domestic subsidiary, Austral.  : Relation extraction examples, taken from TACRED (1-3) and SemEval 2010 Task 8 (4-6). TACRED relation types mostly focus on named entities, whereas SemEval contains semantic relations between concepts. AerolineasHowever, relying on explicit linguistic features severely restricts the applicability and portability of relation extraction to novel languages. Explicitly computing such features requires large amounts of annotated, language-specific resources for training; many unavailable in non-English languages. Moreover, each feature extraction step introduces an additional source of error, possibly cascading over multiple steps. Deep language representations, on the other hand, have shown to be a very effective form of unsupervised pre-training, yielding contextualized features that capture linguistic properties and benefit downstream natural language understanding tasks, such as semantic role labeling, coreference resolution, and sentiment analysis . Similarly, fine-tuning pre-trained language representations on a target task has shown to yield state-of-the-art performance on a variety of tasks, such as semantic textual similarity, textual entailment, and question answering .In addition, classifying complex relations requires a considerable amount of annotated training examples, which are time-consuming and costly to acquire.  showed language model fine-tuning to be a sample efficient method that requires fewer labeled examples.Besides recurrent (RNN) and convolutional neural networks (CNN), the Transformer  is becoming a popular approach to learn deep language representations. Its self-attentive structure allows it to capture long-range dependencies efficiently; demonstrated by the recent success in machine translation ], text generation , and question answering .In this paper, we propose TRE: a Transformer based Relation Extraction model. Unlike previous methods, TRE uses deep language representations instead of explicit linguistic features to inform the relation classifier. Since language representations are learned by unsupervised language modeling, pre-training TRE only requires a plain text corpus instead of annotated language-specific resources. Fine-tuning TRE, and its representations, directly to the task minimizes explicit feature extraction, reducing the risk of error accumulation. Furthermore, an increased sample efficiency reduces the need for distant supervision methods , allowing for simpler model architectures without task-specific modifications.The contributions of our paper are as follows:\u2022 We describe TRE, a Transformer based relation extraction model that, unlike previous methods, relies on deep language representations instead of explicit linguistic features.\u2022 We are the first to demonstrate the importance of pre-trained language representations in relation extraction, by outperforming state-of-the-art methods on two supervised datasets, TACRED and SemEval 2010 Task 8.\u2022 We report detailed ablations, demonstrating that pre-trained language representations prevent overfitting and achieve better generalization in the presence of complex entity mentions. Similarly, we demonstrate a considerable increase in sample efficiency over baseline methods.\u2022 We make our trained models, experiments, and source code available to facilitate wider adoption and further research. TREThis section introduces TRE and its implementation. First, we cover the model architecture (Section 2.1) and input representation (Section 2.2), followed by the introduction of unsupervised pre-training of deep language model representations (Section 2.3). Finally, we present supervised fine-tuning on the relation extraction task (Section 2.4). Model ArchitectureTRE is a multi-layer Transformer-Decoder , a decoder-only variant of the original Transformer . As shown in , the model repeatedly encodes the given input representations over multiple layers (i.e., Transformer blocks), consisting of masked multi-headed self-attention followed by a position-wise feedforward operation:Where T = (t 1 , . . . , t k ) is a sequence of token indices in a sentence 2 . W e is the token embedding matrix, W p is the positional embedding matrix, L is the number of Transformer blocks, and h l is the state at layer l. Since the Transformer has no implicit notion of token positions, the first layer adds a learned positional embedding e p \u2208 R d to each token embedding e p t \u2208 R d at position p in the input sequence. The self-attentive architecture allows an output state h p l of a block to be informed by all input states h l\u22121 , which is key to efficiently model long-range dependencies. For language modeling, however, self-attention must be constrained (masked) not to attend to positions ahead of the current token. For a more exhaustive description of the architecture, we refer readers to  and the excellent guide \"The Annotated Transformer\" 3 . Block is applied at each of the L layers to produce states h 1 to h L . (right) Relation extraction requires a structured input for fine-tuning, with special delimiters to assign different meanings to parts of the input. Input RepresentationOur input representation (see also ) encodes each sentence as a sequence of tokens. To make use of sub-word information, we tokenize the input text using byte pair encoding (BPE) . The BPE algorithm creates a vocabulary of sub-word tokens, starting with single characters. Then, the algorithm iteratively merges the most frequently co-occurring tokens into a new token until a predefined vocabulary size is reached. For each token, we obtain its input representation by summing over the corresponding token embedding and positional embedding. While the model is pre-trained on plain text sentences, the relation extraction task requires a structured input, namely a sentence and relation arguments. To avoid taskspecific changes to the architecture, we adopt a traversal-style approach similar to . The structured, task-specific input is converted to an ordered sequence to be directly fed into the model without architectural changes.  provides a visual illustration of the input format. It starts with the tokens of both relation arguments a 1 and a 2 , separated by delimiters, followed by the token sequence of the sentence containing the relation mention, and ends with a special classification token. The classification token signals the model to generate a sentence representation for relation classification. Since our model processes the input left-to-right, we add the relation arguments to the beginning, to bias the attention mechanism towards their token representation while processing the sentence's token sequence. Unsupervised Pre-training of Language RepresentationsRelation extraction models benefit from efficient representations of long-term dependencies  and hierarchical relation types . Generative pretraining via a language model objective can be seen as an ideal task to learn deep language representations that capture important lexical, syntactic, and semantic features without supervision , before fine-tuning to the supervised task -in our case relation extraction.Given a corpus C = {c 1 , . . . , c n } of tokens c i , the language modeling objective maximizes the likelihoodwhere k is the context window considered for predicting the next token c i via the conditional probability P . TRE models the conditional probability by an output distribution over target tokens:where h L is the sequence of states after the final layer L, W e is the embedding matrix, and \u03b8 are the model parameters that are optimized by stochastic gradient descent. Supervised Fine-tuning on Relation ExtractionAfter pre-training with the objective in Eq. 2, the language model is fine-tuned on the relation extraction task. We assume a labeled dataset, where each instance consists of an input sequence of tokens x 1 , . . . , x m , the positions of both relation arguments a 1 and a 2 in the sequence of tokens, and a corresponding relation label r. The input sequence is fed to the pre-trained model to obtain the final state representation h L . To compute the output distribution P (r) over relation labels, a linear layer followed by a softmax is applied to the last state h m L , which represents a summary of the input sequence:During fine-tuning we optimize the following objective:According to , introducing language modeling as an auxiliary objective during fine-tuning improves generalization and leads to faster convergence. Therefore, we adopt a similar objective:where \u03bb is the language model weight, a scalar, weighting the contribution of the language model objective during fine-tuning. Experiment SetupWe run experiments on two supervised relation extraction datasets: The recently published TACRED dataset  and the SemEval dataset . We evaluate the PCNN implementation of Zeng et al.[2015] 4 on the two datasets and use it as a state-of-the-art baseline in our analysis section. In the following we describe our experimental setup.   ] is a standard benchmark for binary relation classification, and contains 8,000 sentences for training, and 2,717 for testing. Sentences are annotated with a pair of untyped nominals and one of 9 directed semantic relation types, such as Cause-Effect, Entity-Origin, as well as the undirected Other type to indicate no relation, resulting in 19 distinct types in total. We follow the official convention and report macro-averaged F1 scores with directionality taken into account by averaging over 5 independent runs. The dataset is publicly available 6 and we use the official scorer to compute our results. Datasets Pre-trainingSince pre-training is computationally expensive, and our main goal is to show its effectiveness by fine-tuning on the relation extraction task, we reuse the language model 7 published 4.  5.  6.  7.  by  for our experiments. The model was trained on the BooksCorpus , which contains around 7,000 unpublished books with a total of more than 800M words of different genres. It consists of L = 12 layers (blocks) with 12 attention heads and 768 dimensional states, and a feed-forward layer of 3072 dimensional states. We reuse the model's byte pair encoding vocabulary, containing 40,000 tokens, but extend it with task-specific ones (i.e., start, end, and delimiter tokens). Also, we use the learned positional embeddings with supported sequence lengths of up to 512 tokens. Entity MaskingWe employ four different entity masking strategies. Entity masking allows us to investigate the model performance while providing limited information about entities, in order to prevent overfitting and allow for better generalization to unseen entities. It also enables us to analyze the impact of entity type and role features on the model's performance. For the simplest masking strategy UNK, we replace all entity mentions with a special unknown token. For the NE strategy, we replace each entity mention with its named entity type.Similarly, GR substitutes a mention with its grammatical role (subject or object). NE + GR combines both strategies. Hyperparameter Settings and OptimizationDuring our experiments we found the hyperparameters for fine-tuning, reported in , to be very effective. Therefore, unless specified otherwise, we used the Adam optimization scheme [Kingma and Ba, 2015] with \u03b2 1 = 0.9, \u03b2 2 = 0.999, a batch size of 8, and a linear learning rate decay schedule with warm-up over 0.2% of training updates. We apply residual, and classifier dropout with a rate of 0.1. Also, we experimented with token dropout, but did not find that it improved performance. ResultsThis section presents our experimental results. We compare our TRE model to other works on the two benchmark datasets, demonstrating that it achieves state-of-the-art performance even without sophisticated linguistic features. We also provide results on model ablations and the effect of the proposed entity masking schemes. TACREDOn the TACRED dataset, TRE outperforms state-of-the-art single-model systems and achieves an F1 score of 67.4 . Compared to SemEval, we observe methods to perform better that are able to model complex syntactic and long-range dependencies, such as PA-LSTM  and C-GCN . Outperforming these methods highlights our model's ability to implicitly capture patterns similar to complex syntactic features, and also capture long-range dependencies. We would like to point out that the result was produced by the same \"entity masking\" strategy used in previous work . Similar to our NE + GR masking strategy, described in Section 3.3, we replace each entity mention by a special token; a combination of its named entity type and grammatical role. While we achieve state-of-theart results by providing only named entity information, unmasked entity mentions decrease the score to 62.8, indicating overfitting and, consequently, difficulties to generalize to specific entity types. In Section 5.3, we analyze the effect of entity masking on task performance in more detail.  : TACRED single-model test set performance. We selected the hyperparameters using the validation set, and report the test score of the run with the median validation score among 5 randomly initialized runs. \u2020 marks results reported in the corresponding papers. SemEvalOn the SemEval 2010 Task 8 dataset, the TRE model outperforms the best previously reported models, establishing a new state-of-the-art score of 87.1 F1 ( ). The result indicates that pre-training via a language modeling objective allows the model to implicitly capture useful linguistic properties for relation extraction, outperforming methods that rely on explicit lexical features (SVM , RNN ). Similarly, our model outperforms approaches that rely on explicit syntactic features such as the shortest dependency path and learned distributed representations of part-ofspeech and named entity tags (e.g., BCRNN , DRNN , CGCN ). Similar to , we observe a high correlation between entity mentions and relation labels. According to the authors, simplifying SemEval sentences in the training and validation set to just \"subject and object\", where \"subject\" and \"object\" are the actual entities, already achieves an F1 score of 65.1. To better evaluate our model's ability to  : SemEval single-model test set performance. \u2020 marks results reported in the corresponding papers. We report the mean and standard deviation across 5 randomly initialized runs.generalize beyond entity mentions, we substitute the entity mentions in the training set with a special unknown (UNK) token. The token simulates the presence of unseen entities and prevents overfitting to entity mentions that strongly correlate with specific relations. Our model achieves an F1 score of 79.1 (  : SemEval single-model test set performance with all entity mentions masked by an unknown (UNK) token. \u2020 marks results reported in the corresponding papers. Due to the small test set size, we report the mean and standard deviation across 5 randomly initialized runs.the previous state-of-the-art. The result suggests that pre-trained language representations improve our model's ability to generalize beyond the mention level when predicting the relation between two previously unseen entities. Analysis & Ablation StudiesAlthough we demonstrated strong empirical results, we have not yet isolated the contributions of specific parts of TRE. In this section, we perform ablation experiments to understand the relative importance of each model component, followed by experiments to validate our claim that pre-trained language representations capture linguistic properties useful to relation extraction and also improve sample efficiency. We report our results on the predefined TACRED validation set and randomly select 800 examples of the SemEval training set as a validation set. Effect of Pre-trainingPre-training affects two major parts of our model: language representations and byte pair embeddings. In , we first compare a model that was fine-tuned using pre-trained representations to one that used randomly initialized language representations. On both datasets we observe fine-tuning to considerably benefit from pre-trained language representations. For the SemEval dataset, the validation F1 score increases to 85.6 when using a pre-trained language model and no entity masking, compared to 75.6 without pre-training. We observe even more pronounced performance gains for the TACRED dataset, where using a pre-trained language model increases the validation F1 score by 20 to 63.3. With entity masking, performance gains are slightly lower, at +8 on the SemEval dataset and +9.4 (UNK) respectively +3.8 (NE+GR) on the TACRED dataset. The larger effect of pretraining when entity mentions are not masked suggests that pre-training has a regularizing effect, preventing overfitting to specific mentions. In addition, the contextualized features allow the model to better adapt to complex entities. Our observations are consistent with the results of , who observed that language model pre-training considerably improves text classification performance on small and medium-sized datasets, similar to ours.  : Ablation with and without masked entities for SemEval (left) and TACRED validation set (right). We report F1 scores over 5 independent runs. SemEvalIn addition, we train a model from scratch without pre-trained byte pair embeddings. We keep the vocabulary of sub-word tokens fixed and randomly initialize the embeddings. Again, we observe both datasets to benefit from pre-trained byte-pair embeddings. Because of its small size, SemEval benefits more from pre-trained embeddings, as these can not be learned reliably from the small corpus. This increases the risk of overfitting to entity mentions, which can be seen in the lower performance compared to UNK masking, where entity mentions are not available. For the TACRED dataset, model performance drops by approximately 3 \u2212 5% with and without entity masking when not using pre-trained byte pair embeddings. Which Information is captured by Language Representations?Undoubtedly, entity type information is crucial to relation extraction. This is confirmed by the superior performance on TACRED  when entity and grammatical role information is provided (NE+GR). The model achieves a validation F1 score of 68.0, compared to 63.3 without entity masking. Without pre-trained language representations, the model with NE+GR masking still manages to achieve a F1 score of 64.2. This suggests that pre-trained language representations capture features that are as informative as providing entity type and grammatical role information. This is also suggested by the work of , who show that a language model captures syntactic and semantic information useful for a variety of natural language processing tasks such as part-of-speech tagging and word sense disambiguation. Effect of Entity MaskingEntity masking, as described in Section 3.3, can be used to limit the information about entity mentions available to our model and it is valuable in multiple ways. It can be used to simulate different scenarios, such as the presence of unseen entities, to prevent overfitting to specific entity mentions, and to focus more on context.  shows F1 scores on the TACRED validation dataset for different entity masking strategies. As we saw previously, masking with entity and grammatical role information yields the best overall performance, yielding a F1 score of 68.0. We find that using different masking strategies mostly impacts the recall, while precision tends to remain high, with the exception of the UNK masking strategy.When applying the UNK masking strategy, which does not provide any information about the entity mention, the F1 score drops to 51.0. Using grammatical role information considerably increases performance to an F1 score of 56.1. This suggests that either the semantic role type is a very helpful feature, or its importance lies in the fact that it provides robust information on where each argument entity is positioned in the input sentence. When using NE masking, we observe a significant increase in recall, which intuitively suggests a better generalization ability of the model. Combining NE masking with grammatical role information yields only a minor gain in recall, which increases from 65.3% to 67.2%, while precision stays at 68.8%. Entity MaskingPrecision  : TACRED validation F1 scores for TACRED with different entity masking strategies. Sample EfficiencyWe expect a pre-trained language model to allow for a more sample efficient fine-tuning on the relation extraction task. To assess our model's sample efficiency, we used stratified subsampling splits of the TACRED training set with sampling ratios from 10% to 100%. We trained the previously presented model variants on each split, and evaluated them on the complete validation set using micro-averaged F1 scores, averaging the scores over 5 runs. The results are shown in . The best performing model uses a pre-trained language model combined with NE+GR masking, and performs consistently better than the other models. There is a steep performance increase in the first part of the curve, when only a small subset of the training examples is used. The model reaches an F1 score of more than 60 with only 20% of the training data, and continues to improve with more training data.The next best models are the TRE model without a pre-trained language model, and the TRE model without NE+GR masking. They perform very similar, which aligns well with our previous observations. The PCNN baseline performs well when masking is applied, but slightly drops in performance compared to the TRE models after 30% of training data, slowly approaching a performance plateau of around 61 F1 score. The PCNN baseline without masking performs worse, but improves steadily due to its low base score. The TRE model without a language model seems to overfit early and diminishes in performance with more than 70% training data. Interestingly, the performance of several models drops or stagnates after about 80% of the training data, which might indicate that these examples do not increase the models' regularization capabilities. ConclusionWe proposed TRE, a Transformer based relation extraction method that replaces explicit linguistic features, required by previous methods, with implicit features captured in pretrained language representations. We showed that our model outperformes the state-ofthe-art on two popular relation extraction datasets, TACRED and SemEval 2010 Task 8. We also found that pre-trained language representations drastically improve the sample efficiency of our approach. In our experiments we observed language representations to capture features very informative to the relation extraction task.While our results are strong, important future work is to further investigate the linguistic features that are captured by TRE. One question of interest is the extent of syntactic structure that is captured in language representations, compared to information provided by dependency parsing. Furthermore, our generic architecture enables us to integrate additional contextual information and background knowledge about entities, which could be used to further improve performance.Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. 2015 IEEE International Conference on Computer Vision (ICCV), pages 19-27, 2015.\n###\n"}
{"summary": "Dataset used: Wikipedia\nHas method: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\nhas research problem: Document similarity/Document classification\nhas methodology: Relevance model/All terms in top retrieve articles/Document rank & link frequency/Anchor texts in top retrieve Wikipedia documents\nHas implementation: enrich initial queries using semantic annotations in Wikipedia articles combined with phrases disambiguation\nhas publication year: 2019\nhas publication month: 6\nhas author: Kristina Toutanova/Kenton Lee/Ming-Wei Chang/Jacob Devlin\nhas doi: 10.18653/v1/n19-1423", "text": "#Properties\nDataset used, Has method, has research problem, has methodology, Has implementation, has publication year, has publication month, has author, has doi\n#Text\nMany digital libraries recommend literature to their users considering the similarity between a query document and their repository. However, they often fail to distinguish what is the relationship that makes two documents alike. In this paper, we model the problem of finding the relationship between two documents as a pairwise document classification task. To find the semantic relation between documents, we apply a series of techniques, such as GloVe, Paragraph-Vectors, BERT, and XLNet under different configurations (e.g., sequence length, vector concatenation scheme), including a Siamese architecture for the Transformer-based systems. We perform our experiments on a newly proposed dataset of 32,168 Wikipedia article pairs and Wikidata properties that define the semantic document relations. Our results show vanilla BERT as the best performing system with an F1-score of 0.93, which we manually examine to better understand its applicability to other domains. Our findings suggest that classifying semantic relations between documents is a solvable task and motivates the development of recommender systems based on the evaluated techniques. The discussions in this paper serve as first steps in the exploration of documents through SPARQL-like queries such that one could find documents that are similar in one aspect but dissimilar in another. Document Similarity & RecommendationsB\u00e4r et al.  discuss the notion of similarity between texts in the context of NLP. They express that while text similarity is present in many NLP tasks, the similarity is often ill-defined and used as an \"umbrella term covering quite different phenomena\". B\u00e4r et al.  formalize text similarity and suggest content, structure, and style as the major dimensions inherent to texts. With approximately 55% of publications using content-based filtering, it accounts for the majority of the LRS research . Structure and style are not actively being accounted for. Therefore, we focus only on the content.Giving its diversity and reach, Wikipedia had been used as a laboratory in which recommender system methodologies can be tested . In , we compared text-and link-based document similarity measures and found that both methods capture similarity differently. Link-based methods tend to retrieve documents from a broader context, while text-based methods are focused on specific terms and topics. Consequently, each similarity approach is suitable for different information needs, e.g., getting an overview of a topic or performing in-depth research. With the classification of semantic document relations, we intend to tailor recommendations depending on specific information needs. For example, we could provide either recommendations focusing on a particular relation class or diverse recommendations from multiple relation classes. Analogical QueriesAn analogy is a comparison between two or more elements in which their relation is used to illustrate an explanation. Moreover, analogical query solving in the form of \"A is to B as C is to ?\" is a fundamental aspect of human intelligence . Chan et al.  emphasize the importance of analogical query solving for scientific progress. They propose a semi-automated approach for finding analogies between research papers using expert and crowd annotators to segment the abstracts of papers into background, purpose, mechanism, and findings. Next, they encode the segments with GloVe  and Paragraph Vectors  and compute their similarity to determine whether papers are similar with respect to those segments. However, segmentation breaks the coherence of documents. Our method aims to find semantic relations between documents while maintaining their coherence intact.In the context of word embeddings, analogies are often illustrated using vector arithmetic, e.g., \u00ecw King \u2212 \u00ec w Queen = \u00ec w Man \u2212 \u00ec w Woman . Allen and Hospedales  give a mathematical description of analogies as linear relationships between word embeddings. Dai et al.  demonstrate that such analogies are also present in document embeddings. In their experiment, using Wikipedia articles, the nearest neighbor to the vector of \u00ec w LadyGaga \u2212 \u00ec w American + \u00ec w Japanese is the article on Ayumi Hamasaki, a famous Japanese singer that published an album called \"Poker Face\" in 1998 (like Lady Gaga in 2008). TransformersRecently, Transformer-based  neural language models introduced a shift from context-free word embeddings, like GloVe , to contextual embeddings as the ones used in BERT  and XL-Net . The Transformer architecture allowed the efficient unsupervised pretraining of language models and led to significant improvements in many NLP benchmarks . Reimers and Gurevych  proposed to combine BERT with a Siamese architecture  for semantic representations of sentences and their similarity . In prior work , we also utilized a Siamese BERT model to determine the discourse relations between text segments to generate a story for the segments. Moreover, BERT has successfully solved various document classification tasks . Akkalyoncu Yilmaz et al.  apply BERT to an information retrieval system for an end-to-end search over large document collections. Despite their success in NLP, Transformers have gained little attention in the recommender system community so far and are not even mentioned in a recently published survey . To our knowledge, Hassan et al.  are one of the first to use BERT to recommend research papers. As opposed to our work, Hassan et al. use BERT to encode only the paper titles as vectors and then generate recommendations using cosine similarity. In our experiments, we utilize the article text and learn the document relation using a multilayer perceptron (MLP). METHODOLOGYIn the following, we describe the dataset and investigated systems to facilitate the reproduction of our results. Data set & Use caseExisting datasets provide either classifications of single documents (e.g., topic ), relations between sentences or entities (e.g., natural language inference , word analogies , entity relation extraction ), or similarity between text pairs (i.e., binary classification ). Our task is defined as as multi-class classification of document pairs consisting of multiple sentences. Moreover, the learning characteristic in our task requires considerably larger dataset than  or . To the best of our knowledge, no established dataset fulfills these requirements.   is connected to the article 4 and item  of the German Empire through the property country of citizenship  . The Wikidata property acts as both, the relation of the Wikipedia article pair and the class label in the training data for this same pair of documents.  lists other examples to illustrate our scenario better.Given Wikipedia's nature as an encyclopedia, its use as the dataset has some shortcomings. Encyclopedic documents tend to describe a single entity, and their semantics can be seen as rather homogeneous in comparison to other literature forms. Nonetheless, we consider Wikipedia and Wikidata to be a suitable corpus to demonstrate our approach. Wikidata properties range from entityspecific relations (e.g., educated at) to abstract ones (e.g., facet of ). Wikipedia articles and their relations are, on average, more comprehensible than those in scientific literature , which contributes to the analysis of our results. Another aspect that supports our choice of Wikipedia and Wikidata is their open license copyright. Semantic RelationsAt the time of writing, Wikidata contained 7,091 properties 7 of which we selected the following nine for this research:\u2022 country of citizenship -seed is citizen of the target;\u2022 different from -item that is different from another item, with which it is often confused; \u2022 educated at -educational institution attended by seed;\u2022 employer -seed works or worked for target;\u2022 facet of -topic of which this item is an aspect, item that offers a broader perspective on the same topic; \u2022 has effect -the seed causes the target;1  2  3  4  5  6  7 \u2022 has quality -the entity has an inherent or distinguishing non-material characteristic; \u2022 opposite of -item that is the opposite of this item;\u2022 symptoms -possible symptoms of a medical condition.  lists the corresponding Wikidata PIDs, their quantity, and examples for each property. Besides the number of available Wikipedia article pairs, diversity was also a criterion in our selection. Diversity refers to the different semantic meanings of properties (e.g., country of citizenship, opposite of ). Similarly, the requirements to predict a relation between documents can also be diverse. While some relations are clearly expressed within the document text (e.g., for documents referencing people, their citizenship is often put in the first sentences), others will require a more comprehensive understanding of the article content. For instance, while floor as the opposite of ceiling is evident, this fact will most likely not be explicitly mentioned in the article text. Also, other relations like has effect or symptoms can require unwritten domain knowledge. The classification performance can also be affected by the type of the connected articles. For example, the relation class country of citizenship exclusively connects persons and countries. No other property uses such a combination. On the contrary, the relation classes educated at and employer, connect a person with an organization. Additionally, all relations are unidirectional, except for opposite of. Given the many aspects our relations are exploring, we expect significant differences in the classification performance. Data PreprocessingWe sampled 10,000 article pairs in total with a balanced class distribution over the nine properties. The relations were obtained trough the Wikidata SPARQL interface in December 2019. For each Wikipedia article in the sample, we also checked whether the article was connected to any other article but was not part of the initial sample and retrieved the missing relations. We removed all duplicated article pairs and multi-label relations. The main goal of this paper is to explore the multi-class classification problem, so we ensure that the same pair of documents did not share different labels. Wikidata provides data for multi-label relations, especially for hierarchical properties. However, only less than 1% of our sample data contained multi-label relations. For the sake of simplicity, we decided to remove them. This procedure generates 16,084 Wikipedia article pairs with an imbalanced class distribution ( ). The increase of samples is due to the retrieval of missing relations. The corresponding articles were converted to plain-text from the English Wikipedia dump of November 2019 using the Gensim API . Negative SamplingIn addition to the nine positive classes from Wikidata, we introduce a class named None that works as negative cases for our positive samples in the same proportion. The articles in the None category are randomly selected and do not share any relation with the positive ones. The resulting dataset contains 32,168 samples in total. SystemsThis paper evaluates six classifiers under different configurations, totaling 30 systems. We distinguish between three model categories: (i) document embeddings from word embeddings using the full document text (GloVe and Doc2vec), 3.5.1 Doc2vec. With word2vec, Mikolov et al.  introduced an algorithm to learn dense vector representations of words such that semantically similar words end up close to each other in the embedding space. Word2vec is widely applied in NLP tasks  but unable to represent entire documents. Paragraph Vectors  (also known as Doc2vec), extends word2vec to learn embeddings for word sequences of arbitrary length. In the following, we refer Paragraph Vectors as Doc2vec, since we employ the widely-used implementation of the Gensim  framework. We obtained a 200D document vectors \u00ec d for each Wikipedia article by training Doc2vec's distributed bag of words model (dbow) using both training and test data, and the default hyperparameters in Gensim  . The document vector size of 200 corresponds to the size of the GloVe word vectors (Section 3.5.2). The choice of dbow over the distributed memory training model is due to its results in semantic similarity tasks . It is important to mention that even though the embeddings model used both training and test sets, the latter was not used for training the classifier. AvgGloVe.GloVe  also produce dense embedding representations, but unlike word2vec, GloVe is a count-based method that uses global statistics to derive its word vectors. In GloVe, the co-occurrence matrix explores the ratio of the probabilities of words in a text to derive its semantic vectors. While we use the 200D pretrained word embedding model 9 , GloVe does not provide document vectors directly. To embed a Wikipedia article \u00ec d, we compute the weighted average over its word vectors \u00ec w i (AvgGloVe), whereby the number of occurrences of the word i in d defines the weight c i . Arora et al.  showed the weighted average of word vectors is effective and yields good results for representing documents.For both full-text methods, AvgGloVe and Doc2vec, we encode each document from our document pair (d s , d t ) independent from the classification task and concatenate their resulting vectors. The different concatenation variants tested in our experiments are discussed in Section 3.6. The resulting document pair vector is then used as an input to a fully-connected MLP, which classifies the document pair relation\u0177 = relThe dimension of the output of the last layer of all classifiers (\u0177), corresponds to the nine Wikidata properties ) and one additional dimension for the None class of negative samples (Section 3.4). The logistic sigmoid function is used to generate the probabilities for the multi-class classification. Vanilla Transformer. As the third model category, we employ two language models for deep contextual text representations based on the Transformer architecture , named BERT  and XLNet . The two Transformer models are originally designed to solve sequence pair classification. The base training task (i.e., next sentence prediction) for BERT and XLNet allows us to fine-tune them for the document pair classification task. The content of the document pair (i.e., title and text of d s and d t ) is tokenized, delimited with special tokens, i.e., [CLS] and [SEP] for BERT, <cls> and <sep> for XLNet, and then jointly fed trough the Transformer ). The Transformer output is used as the input to a single fully-connected linear layer with 512 units for the classification (prediction head). Regarding terminology, we refer to the two models as vanilla Transformer since their original architecture is unchanged.[CLS][SEP]  3.5.4 Siamese Transformer. We combine the two Transformers (BERT and XLNet) in a Siamese network architecture . In Siamese networks, two inputs are fed through identical sub-networks with shared weights (in this case, the Transformers), and then passed to a classifier or a similarity function. Reimers and Gurevych  have shown that Siamese BERT networks are suitable for text similarity tasks. For our experiment, both documents d s and d t are input to the Transformer sub-networks to derive two contextual document vectors . Next, the document vectors are concatenated and classified with a 2-layer MLP (2x512 units with ReLU activation), the same method applied by Doc2vec and AvgGlove. In contrast to Doc2vec and AvgGloVe, the document representations are neither fixed nor frozen, but continually learned during the training of the classifier. Different than , our implemented Siamese architecture is applied to a multi-class classification instead of a binary one.[CLS][SEP]Transformer Hyperparameters3.6.1 Sequence length. The vanilla and Siamese Transformer models based on BERT have a maximum sequence length of 512 tokens due to absolute positional embeddings. However, XLNet integrates the relative positional encoding, as proposed in Transformer-XL . Therefore, XLNet's architecture is, in theory, not bound to a maximum sequence length. However, a custom pretraining is out of scope for this research, and the publicly available pretrained models of XLNet have the same 512 token limit as BERT. It remains unknown how the length of the processed sequence affects the classification task. From , we know that the performance of similarity measures peaks at 450 words since the introduction section in Wikipedia articles presumably contains all essential information. Other sections might add only noise and make it harder to encode relevant semantic information from the articles. Thus, we evaluate the Transformers using 128, 256, and 512 tokens (Section 4.2). ImplementationAll experiments with Doc2vec and AvgGloVe can be run on CPU in less than 15 minutes using the Gensim  and Scikit-learn  framework. Before training the Doc2Vec model, Gensim preprocesses RESULTSOur results are divided in: overall, sequence length, concatenation, relation classes, and manual sample examination. These five subsections move from a high-level perspective to a detailed investigation of the main aspects that most contributed to our findings. OverallThe empirical results of the tested systems and hyperparameters are presented in . Vanilla BERT-512 yields the best micro average F1-Score with 0.933, followed by its 256 length size model, with 0.930. The second-best model is the vanilla XLNet-512 with 0.926 F1 and a statistically significant lower score compared to vanilla BERT-512 (95% confidence interval   an F1-score of 0.845 at its best configuration, Doc2vec is the worst performing model. In summary, we consider the results of Avg-GloVe and vanilla BERT as most promising for future application scenarios. We hypothesize that an F1-score of above 0.90 is already suitable enough for LRS. Especially, expert users would tolerate some misclassifications in favor of otherwise undiscoverable information. This would be the case for target documents that are considered to be dissimilar to the seed with existing methods but are found to have semantic relation with the help of our methods. Sequence LengthAs explained in Section 3.6, we are particularly interested in the effect of the sequence length on the Transformer models. To illustrate this effect,  shows the comparison of Siamese BERT, Siamese XLNet, vanilla BERT, and vanilla XLNet with respect to their sequence length (i.e., 128, 256, and 512). In this comparison, the Siamese models use the best performing concatenation method, which isOur findings reveal that longer sequences are related to better results. For all models, except Siamese XLNet, the highest F1-score is achieved with 512 tokens and the second-highest with 256 tokens. One could think this outcome is to be expected. However, in , the performance of text-and linkbased document similarity measures declines for Wikipedia articles with more than 450 words. When comparing Siamese with vanilla Transformers, the vanilla models work with only half of the sequence length to encode one document of the pair. In vanilla Transformers, the document pairs share the sequence length, while in Siamese Transformers each document has its own Transformer subnetwork (sequence length). For example, a vanilla 128-Transformer would use only 62 or 63 tokens of each document (three tokens are reserved for special tokens as  shows). Thus, the small performance difference within vanilla BERT with 512 tokens (0.933 F1), 256 tokens (0.930 F1), and 128 tokens (0.920 F1) is remarkable. Moreover, the performance differences should be considered relative to the higher computation expenses of longer sequences.S i a m e s e B E R T S i a m e s e X L N e t V a n i l l a B E R T V a n i l l a X L N e t ConcatenationAside from the sequence length, we also analyzed the different concatenation methods in AvgGloVe, Doc2vec, and the Siamese models ( ). All models achieve the highest F1-score when the concatenation with an element-wise difference and product is used. Furthermore, we confirmed the results of Reimers and Gurevych , i.e., the most crucial component is the element-wise difference |u \u2212v |. Only for Doc2vec the element-wise difference decreases the performance in comparison to the simple concatenation. However, this performance decrease is marginal and within standard deviation. In general, the element-wise difference measures the distance between the dimensions of the two document vectors and, thus, ensures that similar pairs are closer to each other than dissimilar pairs. This effect is evident for Siamese BERT and Siamese XLNet, for which the element-wise difference yields the most substantial performance improvement. On the contrary, the element-wise product adds only a small improvement to our models. Relation ClassesWe selected nine diverse Wikidata properties to explore how the systems would respond to the individual challenges of each property.  presents precision, recall, and F1-score of the best four systems for the different model categories. Each score is the mean over the 4-fold cross-validation (cf. as concatenation method, and all Transformer models (Siamese BERT, vanilla BERT, and vanilla XLNet) use the 512 sequence length. The best relation classes in terms of performance are country of citizenship, none (negative samples), and different from, whereas the classes employer, has quality, has effect yield the lowest scores. Given that the best performing classes are also over-represented in terms of sample count, the outcome suggests that other classes only need more training data. Still, the comparison of the employer class (389 test samples, vanilla BERT 0.740 F1) and facet of (336 test samples, vanilla BERT 0.911) reveals that the performance difference is also due to the diverse requirements of classes themselves. The superiority of vanilla BERT is also present in the classspecific evaluation scenario, although it is outperformed by vanilla XLNet for three relation classes with a small number of samples (has effect, has quality and opposite of ). In AvgGloVe, symptoms has the highest precision score, which is probably caused by Avg-GloVe being able to utilize the full-text of articles in contrast to the Transformer models. Medical articles, like Alcoholism (Example 9 in ), contain a section \"Signs and symptoms\" in which their symptoms are listed. However, such a section is not part of the 512 Transformer tokens. When comparing precision and recall for all classes, both scores are mostly balanced. There is only one striking exception for vanilla BERT. For employer, the precision score of 0.829 is higher than the recall of 0.653, while for educated at the opposite occurs, with a precision of 0.759 and recall of 0.900, but in a smaller magnitude. A reason for this outcome is that employer is often confused with educated at as  shows. 0.88 0.00 0.00 0.06 0.00 0.00 0.02 0.01 0.01 0.03 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.02 0.00 0.97 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.14 0.00 0.00 0.69 0.00 0.00 0.03 0.03 0.01 0.11 0.00 0.00 0.00 0.00 0.68 0.27 0.00 0.00 0.00 0.05 0.00 0.00 0.00 0.00 0.05 0.90 0.00 0.00 0.00 0.05 0.06 0.00 0.00 0.03 0.00 0.00 0.87 0.02 0.01 0.01 0.03 0.00 0.00 0.00 0.00 0.00 0.00 0.93 0.00 0.03 0.20 0.00 0.11 0.05 0.00 0.00 0.01 0.03 0.52 0.09 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0  The confusion matrix in  depicts which classes are most often confused with each other. The predicted classes are taken from the vanilla BERT-512 system, whereby the number of true and predicted classifications is normalized to make the different classes comparable. With 27% of the test sample, educated at and employer are the most mistaken relation classes in our experiments. We see this outcome because both relation classes connect persons and organizations, and we assume it is harder for the classifier to tell the relations apart. For instance, Albert Einstein could be employed or educated at ETH Zurich . The misclassification between different relations is also found in opposite of, has quality and has effect, which we conclude is because of similar reasons. In particular, the opposite of relation connects various types of articles. Manual Sample ExaminationTo validate our empirical findings, we manually examine the prediction from vanilla BERT-512 with a focus on errors  Our manual examination confirms the overall results. Most relations are correctly identified, while some relations are missing even if they are explicitly mentioned in the text. An analysis of the inner Transformer components  is a subject for future work. CONCLUSION AND FUTURE WORKThis paper introduces the pairwise document classification to determine semantic relations between documents as an underlying task to advance LRS and other information retrieval applications. We elaborate on why document similarity measures do not account for the heterogeneous semantics of extensive documents and argue that similarity needs a context which defines to what it relates.The task of finding semantic document relations is implemented as a multi-class classification of document pairs. We demonstrate the viability of this approach with a new proposed dataset of 32,168 Wikipedia article pairs and Wikidata properties that define semantic relations among these articles. In an empirical study, we implement six different models AvgGloVe, Doc2vec, Siamese BERT, Siamese XLNet, vanilla BERT and vanilla XLNet, and evaluate them under different settings regarding the concatenation method and sequence length ( ). Our evaluation indicates a sequence length of 512 tokens as the best performing sequence limit for the Siamese and vanilla Transformer models. In addition, we identify [u; v; |u \u2212 v |; u * v] as the best concatenation method for AvgGloVe, Doc2vec and the Siamese Transformer models. With the manual sample examination and our evaluation for different relation classes, we show the behavior of the classifiers when exposed to different input data and provide an analysis over different perspectives. Moreover, the manual analysis confirms our empirical results.Our findings suggest that pairwise document classification is a solvable task using existing techniques. Even abstract semantic relations, like facet of, yield a considerable high F1-score. This outcome motivates us to investigate the semantic relations between documents of other literature domains, primarily scientific papers. We envision a system that enables users to explore scientific literature in an analogical manner. For instance, users could retrieve other research papers with a similar methodology but different result. Analogies could be even found with programmatic and SPARQLlike queries. To develop such a system, the Open Research Knowledge Graph  could be utilized as the scientific equivalence of Wikidata, while research paper from any open digital library, e.g., arXiv, would correspond to Wikipedia articles. Lastly, the presented Wikipedia and Wikidata dataset also facilitate the evaluation of methods in terms of required training data. The estimation the necessary amount of data would a prerequisite for looking into other domains.\n###\n"}
{"text": "#Properties\nimplementation, metric, has research problem, evaluation, Approach type, Document type, Summary usage, Summary characteristics\n#Text\nWe propose a new, ambitious framework for abstractive summarization, which aims at selecting the content of a summary not from sentences, but from an abstract representation of the source documents. This abstract representation relies on the concept of Information Items (INIT), which we define as the smallest element of coherent information in a text or a sentence. Our framework differs from previous abstractive summarization models in requiring a semantic analysis of the text. We present a first attempt made at developing a system from this framework, along with evaluation results for it from TAC 2010. We also present related work, both from within and outside of the automatic summarization domain. Abstractive Summarization FrameworkOur proposed framework for fully abstractive summarization is illustrated in figure 1. This section discusses how each step could be accomplished. INIT RetrievalAn Information Item is the smallest element of coherent information in a text or a sentence. This intentionally vague definition leaves the implementation details to be decided based on resources available. The goal is to identify all entities in the text, their properties, predicates between them, and characteristics of the predicates. This seemingly unreachable goal, equivalent to machine reading, can be limited to the extent that we only need INITs to be precise and accurate enough to generate a summary from them.The implementation of INITs is critical, as everything will depend on the abstract information available. Semantic Role Labeling (SRL) and predicatelogic analysis of text are two potential candidates for developing INIT Retrieval. Word-sense disambiguation, co-reference resolution and an analysis of word similarity seem important as well to complement the semantic analysis of the text. INIT SelectionGiven an analysis of the source documents that leads to a list of INITs, we may now proceed to select content for the summary. Frequency-based models, such as those used for extractive summarization, could be applied to INIT selection instead of sentence selection. This would result in favoring the most frequently occurring entities, predicates, and properties.INIT selection could also easily be applied to tasks such as query-driven or guided summarization, in which the user information need is known and the summarization system attempts to address it. With smaller building blocks (INITs rather than sentences), it would be much easier to tailor summaries so that they include only relevant information. GenerationPlanning, summary planning in our case, provides the structure of the generated text. Most INITs do not lead to full sentences, and need to be combined into a sentence structure before being realized as text. Global decisions of the INIT selection step now lead to local decisions as to how to present the information to the reader, and in what order.Text generation patterns can be used, based on some knowledge about the topic or the information needs of the user. One could use heuristic rules with different priority levels or pre-generated summary scenarios, to help decide how to structure sentences and order the summary. We believe that machine learning could be used to learn good summary structures as well.Once the detailed planning is completed, the summary is realized with coherent syntax and punctuation. This phase may involve text-to-text generation, since the source documents' sentences provide a good starting point to generate sentences with varied and complex structures. The work of on sentence fusion shows an example of re-using the same syntactical structure of a source sentence to create a new one with a slightly different meaning. First Attempt at Abstractive SummarizationThe three-step plan that we laid down is very hard, and instead of tackling it head on, we decided to focus on certain aspects of it for now. We followed a simplified version of our framework, illustrated by the dashed line in . It defers the content selection step to the selection of generated short sentences, rather than actually doing it abstractly asOriginal Sentence The Cypriot airliner that crashed in Greece may have suffered a sudden loss of cabin pressure at high altitude, causing temperatures and oxygen levels to plummet and leaving everyone aboard suffocating and freezing to death, experts said Monday. Selected Generated Sentence as it appears in the summary 1. Last year, 25 bears died in greater Yellowstone area. planned. The summary planning has to occur after generation and selection, in a Summary Generation step not shown explicitly on the workflow.We have restricted our implementation of INITs to dated and located subject-verb-object(SVO) triples, thus relying purely on syntactical knowledge, rather than including the semantics required for our frame-work. Dates and locations receive a special treatment because we were interested in news summarization for this first attempt, and news articles are factual and give a lot of importance to date and location.We did not try to combine more than one INIT in the same sentence, relying instead on short, to-the-point sentences, with one INIT each. shows two examples of sentences that were generated from a source document sentence using the simplified abstractive summarization framework.At first glance, the simplified version of our approach for generating sentences may seem similar to sentence compression. However, it differs in three important ways from the definition of the task of compression usually cited :\u2022 Our generated sentences intend to cover only one item of information and not all the important information of the original sentence. \u2022 An input sentence may have several generated sentences associated to it, one for each of its INITs, where it normally has only one compressed sentence. \u2022 Generated sentences sometimes include words that do not appear in the original sentence (like 'some' in the second example), whereas sentence compression is usually limited to word deletion.3 Abstractive Summarization at TAC 2010Our first attempt at full abstractive summarization took place in the context of the TAC 2010 multidocument news summarization task. This section describes briefly each module of our system, while provides the implementation details. INIT RetrievalAn INIT is defined as a dated and located subjectverb-object triple, relying mostly on syntactical analyses from the MINIPAR parser and linguistic annotations from the GATE information extraction engine . Every verb encountered forms the basis\n###\n", "summary": " implementation: Implementation\n, has research problem: Automatic text summarization\n, Document type: Multiple documents\n, Summary characteristics: Extractive\n###"}
{"text": "#Properties\nMethod, dataset, has research problem, approach, deals with, uses similarity, Performance metric, Evidence, Uncertainty, Limitations\n#Text\nMembers of the academic community have increasingly turned to digital libraries to search for the latest work of their peers. On account of their role in the academic community, it is very important that these digital libraries collect citations in a consistent, accurate, and up-to-date manner, yet they do not correctly compile citations for myriads of authors for various reasons including authors with the same name, a problem known as the \"name ambiguity problem.\" This problem occurs when multiple authors share the same name and particularly when names are simplified as in cases where names merely contain the first initial and the last name. This paper proposes a reliable and accurate pair-wise similarities approach to disambiguate names using supervised classification on Web correlations and authorship correlations. This approach makes use of Web correlations among citations assuming citations that co-refer on publication lists on the Web should to refer to the same author. This approach also makes use of authorship correlations assuming citations with the same rare author name refer to the same author, and furthermore, citations with the same full names of authors or e-mail addresses likely refer to the same author. These two types of correlations are measured in our approach using pair-wise similarity metrics. In addition, a binary classifier, as part of supervised classification, is applied to label matching pairs of citations using pair-wise similarity metrics, and these labels are then used to group citations into different clusters such that each cluster represents an individual author. Results show our approach greatly improves upon the name disambiguation accuracy and performance of other proposed approaches, especially in some name clusters with high degree of ambiguity. View PDF1. John R. 2. Yi Wu, Ching-Yung Lin, Edward Y. Chang and John R. Smith, \"Multimodal information fusion for video concept detection,\" ICIP, pp. Our paper proposes an effective solution to the name ambiguity problem that uses two kinds of correlations among citations: Web Correlations and Authorship Correlations. Web correlations depend on authors' publications being listed on their publication lists or on their faculty's publication lists on the Web. If two citations co-refer to the same author on these publication lists, they likely refer to the same author. Since publication lists contain a lot of citations, Web correlations among citations is based on co-referring citations on these Web publication lists. Authorship correlation relies on two author identification strategies. First, authorship correlations give different priority to authors' names depending on the popularity of their names. Some authors' names are rarer than others. If two citations have the same co-authors and the author's name on these citations is rare, the name likely does not belong to two different people but to the same person, so these citations should be treated as having the same author. Second, authorship correlations treat citations with the same full name, e-mail, or other \"personal information\" as having the same author. To test the strength of our authorship correlations, the publication pages and articles for each citation were collected from digital libraries, and then the strength of the authorship correlation was measured based on whether the citations have the same author information or not. After calculating pair-wise similarities using Web and authorship correlations, a binary classifier was applied to label matching pairs, and then these labels were used to group the citations into several clusters such that each cluster represented a distinct author. The results demonstrated our approach has a significantly higher average disambiguation accuracy than those of other approaches, especially in some name clusters with a high degree of ambiguity. The results also demonstrated that using the personal information of authors significantly improves disambiguation of authors.The remainder of this paper is organized as follows. Section 2 reviews related works, and Section 3 describes our approach for pair-wise disambiguation. In Section 4 Experiments, 5 Discussion, we discuss the experiment results. Finally, we draw conclusions in Section 6. 2. 3. 4.classifier is then used to predict relations among citations with ambiguous names. These predictions are then used to group citations into clusters. used two binary classifiers, the hybrid Naive Bayes and Support Vector Machine (SVM), to disambiguate authors in the DBLP. Our previous work, used the SVM binary classifier to disambiguate names. conducted name disambiguation on the Medline database using the random forest model. Unsupervised clustering requires building similarity functions to integrate pair-wise similarities, whereas supervised classification requires striking a balance between quality and quantity of training and testing data.This paper proposes a reliable and accurate pair-wise similarities approach to name disambiguation using supervised classification on Web correlations and authorship correlations. Proposed approachOur approach improves on the performance of author name disambiguation of previous approaches in two ways. First, this approach makes use of Web correlations among citations on the assumption citations that co-refer on publication lists on the Web should to refer to the same author. Second, this approach makes use of authorship correlations on the assumption that citations with the same rare author name refer to the same author. Authorship correlations also assume citations with the same full names of authors or e-mail addresses likely refer to the same author. These two types of correlations are measured in our approach using pair-wise similarity metrics. In addition, a binary classifier, as part of supervised classification, is applied to label which pairs of citations match using pair-wise similarity metrics, and these labels are used to group citations into different clusters such that each cluster represents an individual author. Our approach is as follows:Use Web and authorship correlations to generate pairs of citations. The similarity scores are then calculated between any two citations in the dataset using the proposed similarity metrics.Create two datasets: training data and testing data. The training data is used to create a binary classifier.Every citation is paired with another and labeled matching or non-matching. Matching pairs of citations are pairs that have the same author, and vice versa.Apply the binary classifier to label matching pairs in the testing data.Group the citations into the appropriate clusters using the labels. Pairs of\n###\n", "summary": " Method: Web and authorship correlations\n, dataset: DBLP\n, has research problem: Author name disambiguation\n, approach: Semi-Supervised Learning\n, deals with: Homonyms problem/Synonyms problem\n, uses similarity: Cosine/Modified sigmoid function/Name popularity measure\n, Performance metric: F1/Pairwise Precision/Recall\n, Evidence: Authors/Title words/Venues\n, Uncertainty: F\n, Limitations: Inherently slow as using web info\n###"}
{"text": "#Properties\nHas value, Global Mean Sea level Rise Projection, has lower limit for likely range, has upper limit for likely range, has  start of period, has end of period, climate scenario, has research problem, has unit\n#Text\nThere is a growing awareness that uncertainties surrounding future sea-level projections may be much larger than typically perceived. Recently published projections appear widely divergent and highly sensitive to non-trivial model choices. Moreover, the West Antarctic ice sheet (WAIS) may be much less stable than previous believed, enabling a rapid disintegration. Here, we present a set of probabilistic sea-level projections that approximates the deeply uncertain WAIS contributions. The projections aim to inform robust decisions by clarifying the sensitivity to non-trivial or controversial assumptions. We show that the deeply uncertain WAIS contribution can dominate other uncertainties within decades. These deep uncertainties call for the development of robust adaptive strategies. These decision-making needs, in turn, require mission-oriented basic science, for example about potential signposts and the maximum rate of WAIS-induced sea-level changes. Future sea-level rise poses nontrivial risks for many coastal communities 1, 2. Managing these risks often relies on consensus projections like those provided by the IPCC 3. Yet, there is a growing awareness that the surrounding uncertainties may be much larger than typically perceived 4. Recently published sea-level projections appear widely divergent and highly sensitive to non-trivial model choices 4. Moreover, the West Antarctic ice sheet (WAIS) may be much less stable than previously believed, enabling a rapid disintegration 5, 6. In response, some agencies have already announced to update their projections accordingly 7, 8. The construction of sea-level projections is often largely motivated by scientific considerations, such as gaining a better understanding of the underlying physics 2, 9. In this process, the translation from input data to model projections and full uncertainty estimates involves a wide range of non-trivial model choices and assumptions that can result in large discrepancies between different uncertainty estimates 4. For example, many studies consider a high level of model detail indispensable for reliable projections 3 , whereas semi-empirical modeling approaches 10-12 trade complexity for the ability to calibrate the model. Semi-empirical modeling approaches often rely on strong assumptions about the prior parameter distributions, what mechanisms to include, and how to interpret and represent the data-model discrepancies. These modeling choices can be nontrivial and the associated uncertainties hard to quantify 13. On the other hand, projections based on multi-model ensembles (implicitly) focus on structural uncertainty which requires strong assumptions on which part of the overall uncertainty is covered 4. Decision makers often prefer \"robust\" over optimal decisions when faced with \"deep\" uncertainty 14-18. Deep uncertainty refers to a situation when experts cannot agree upon or are not willing to provide probabilistic uncertainty ranges 15. In the context of decision-making, robustness has many different definitions that usually involve trading some optimality for relative insensitivity to deviations from the model assumptions or relatively good performance over a wide range of futures 15-18. Here we present sea-level projections to inform the design of robust strategies to cope with the deep uncertainties surrounding sea-level change, i.e. \"solutions capable of withstanding from deviations of the conditions for which they are designed\" 17. This notion of \"robustness\" deviates from scientific robustness that builds on arguably well understood physics and empirical/robust evidence 19-21 , which may lead to overconfident uncertainty ranges 2, 4 and getting surprised by new insights and data 9 . Our sea-level projections are constructed to support robust decision frameworks by i) being explicit about the relevant uncertainties, both shallow and deep; ii) communicating plausible ranges of sea-level rise, including the deep uncertainties surrounding future climate forcings and potential WAIS collapse; and iii) tending to err on the side of underconfident versus overconfident when possible.Model design. We design the projections to be probabilistic where reasonable and explicit about deep uncertainties (e.g. resulting from non-trivial model choices) when needed. Robust decision frameworks often apply plausible rather than probabilistic ranges to represent and communicate uncertainties . In the case of sea-level projections, the bounding of the plausible range usually involves both a probabilistic interpretation of the surrounding uncertainties and estimates of which probabilities are still relevant. For example, a full disintegration of the major ice sheets is often not taken into account because the probabilities of this occurring are considered too small to be relevant . What probability is relevant is highly dependent on the decision context and therefore it makes sense to be explicit about the probabilities. Moreover, probabilities are the easiest and most unambiguous way to communicate uncertainties .Our projections are designed to highlight the relatively large deep uncertainties, notably those resulting from future climate forcings and those surrounding potential WAIS collapse (even though representations of deep uncertainty often implicitly encompass probabilistic interpretations). The future climate forcing is, to a large extent, controlled by future human decisions.The probability of a WAIS collapse is potentially much larger than previously thought due to the combined effects of Marine Ice Sheet Instability (MISI), ice cliff failure and hydrofracturing . The discovery of this new mechanism puts earlier expert elicitations in a different light as it is unclear if those were based on this combined effect. One approach when faced with deeply uncertain model structures and priors is to present a potential WAIS collapse as deeply uncertain by means of a plausible range. We stress that this range is not meant to represent an implicit probabilistic projection of the WAIS contribution to sea-level rise.We merge some small deep uncertainties into the probabilistic part of the projections. According to \"\u2026 a larger risk lies in sampling too narrow a range (thus ignoring potentially important vulnerabilities) rather than too wide a range which, at worst, will sample extreme states of the world in which all alternatives fail\". Thus, in the context of informing robust decision making, it can be preferable to be slightly under-than slightly overconfident. To minimize the risk of producing overconfident projections we only use observational data with relatively uncontroversial and well-defined error structure.Model setup. We use a relatively simple (39 free physical and statistical parameters), but a mechanistically motivated model framework to link transient sea-level rise to radiative concentration pathways applying sub-models for the global climate, thermal expansion (TE), and contributions of the\n###\n", "summary": " Has value: 0.2\n, Global Mean Sea level Rise Projection: Global Mean Sea Level Rise Projections\n, has  start of period: 2000\n, has end of period: 2050\n, climate scenario: RCP2.6\n, has research problem: Global Mean Sea Level Rise Projections\n, has unit: m\n###"}
{"text": "#Properties\nHas value, Global Mean Sea level Rise Projection, has lower limit for likely range, has upper limit for likely range, has  start of period, has end of period, climate scenario, has research problem, has unit\n#Text\nStrategies to manage the risks posed by future sea-level rise hinge on a sound characterization of the inherent uncertainties. One of the major uncertainties is the possible rapid disintegration of large fractions of the Antarctic ice sheet in response to rising global temperatures. This could potentially lead to several meters of sea-level rise during the next few centuries. Previous studies have typically been silent on two coupled questions: (i) What are probabilistic estimates of this Bfast dynamic^contribution to sea-level rise? (ii) What are the implications for strategies to manage coastal flooding risks? Here, we present probabilistic hindcasts and projections of sea-level rise to 2100. The fast dynamic mechanism is approximated by a simple parameterization, designed to allow for a careful quantification of the uncertainty in its contribution to sea-level rise. We estimate that global temperature increases ranging from 1.9 to 3.1\u00b0C coincide with fast Antarctic disintegration, and these contributions account for sea-level rise of 21-74 cm this century (5-95% range, Representative Concentration Pathway 8.5). We use a simple cost-benefit analysis of coastal defense to demonstrate in a didactic exercise how neglecting this mechanism and associated uncertainty can (i) lead to strategies which fall sizably short of protection targets and (ii) increase the expected net costs. Keywords Sea-level projections. West Antarctic ice sheet. Coastal flood defense. Climate impacts. Cliff failure and hydrofracturing Methods Sea-level riseWe employ and expand upon a model framework that has been previously applied for probabilistic projections of sea-level rise . This model has recently been made available as the building blocks for Relevant Ice and Climate Knowledge (BRICK) model v0.2 to simulate global mean surface temperature, ocean heat uptake, global mean sea level and its contributions from the Antarctic ice sheet, Greenland ice sheet, thermal expansion, and glaciers and small ice caps . BRICK uses a semi-empirical modeling approach, combining a platform of previously published models. The model is described in greater detail by , so we only provide an overview here.Global mean surface temperature and ocean heat uptake are simulated by the Diffusion-Ocean-Energy balance CLIMate model DOECLIM . DOECLIM is a zero-dimensional energy balance model coupled to a three-layer, one-dimensional diffusive ocean model. The input required to force DOECLIM is the radiative forcing time series (W m \u22122 ), which is provided based on the work of , with some updates and extensions, as described in greater detail in previous studies using DOECLIM . We use a 1-year time step, and the output global mean surface temperature couples to sealevel sub-models representing individual major sea-level contributions. All sea levels are presented relative to their 1986-2005 mean.The Greenland ice sheet is represented by the Simple Ice Sheet Model for Projecting Large Ensembles (SIMPLE; . SIMPLE first estimates an equilibrium Greenland ice sheet volume (V eq,GIS ), given an anomaly in global mean temperature (T g ), as well as the e-folding timescale of the ice sheet volume as it exponentially relaxes towards this equilibrium volume (\u03c4 GIS ).In Eqs. 1 and 2, t represents time (years), c GIS is the equilibrium ice sheet volume sensitivity to temperature (m SLE\u00b0C \u22121 ), b GIS is the equilibrium ice sheet volume for zero temperature anomaly (m SLE), \u03b1 GIS is the temperature sensitivity of the efolding ice sheet response timescale (\u00b0C \u22121 year \u22121 ), and \u03b2 GIS is the equilibrium response timescale (year \u22121 ). These quantities are uncertain model parameters, which we estimate as described in and briefly in Sect. 2.3. The change in Greenland ice sheet volume (V GIS ) can then be written asWe make the assumption that all GIS volume lost makes its way into the oceans. The contribution to sea level from glaciers and small ice caps (GSIC) is represented by the GSIC sub-model of the Model for Assessment of Greenhouse-Gas-Induced Climate Change (MAGICC) . The GSIC sea-level contribution (S GSIC ) is parameterized asIn Eq. 4, the uncertain model parameters are \u03b2 0 , the GSIC mass balance sensitivity to global temperature anomalies (m\u00b0C \u22121 year \u22121 ); V 0,GSIC , the initial GSIC volume susceptible to melt (m SLE); and n, the area-to-volume scaling parameter (unitless). These parameters are estimated as in . T eq,GSIC is taken equal to \u22120.15\u00b0C .Our parameterization for sea-level rise due to thermal expansion was originally formulated for global sea level by and adapted for thermal expansion by . First, an equilibrium thermal expansion is calculated (S eq,TE ), given the anomaly in global mean temperature:a TE , the sensitivity of this equilibrium thermal expansion to temperature changes (m\u00b0C \u22121 ), and b TE , the equilibrium thermal expansion for zero temperature anomaly (m SLE), are estimated as uncertain model parameters . The thermal expansion contribution to global mean sea level is modeled as an exponential relaxation towards S eq,TE :where \u03c4 TE is the e-folding timescale of the thermal expansion response, and the quantity 1/\u03c4 TE is estimated as a model parameter where B tot (m 3 year \u22121 ) represents the total rate of accumulation of Antarctic ice sheet mass and F (m 3 year \u22121 ) is the ice volume flux across the grounding line. T (\u00b0C) is the Antarctic surface temperature reduced to sea level (as the Antarctic surface is largely above sea level), S is sea level (m), and R is the Antarctic ice sheet radius (m). The interested reader is directed to and for more information about the DAIS model. Antarctic ice sheet fast dynamic parameterizationThe original DAIS model includes a parameterization for dynamic ice loss over the grounding line as it retreats due to subsurface ocean warming (F in Eq. 7 above; Shaffer 2014). This ice flux depends on the Antarctic ice sheet geometry, the water depth, and water temperature. This misses the critical link between rising global temperatures and the sudden, fast ceasing of buttressing ice shelves due to processes such as hydrofracturing and ice cliff failure , which may substantially speed up the dynamic outflow . We form an explicit link between global surface temperatures and these\n###\n", "summary": " Has value: 0.55\n, Global Mean Sea level Rise Projection: Global Mean Sea Level Rise Projections\n, has  start of period: 2000\n, has end of period: 2100\n, climate scenario: RCP2.6\n, has research problem: Global Mean Sea Level Rise Projections\n, has unit: m\n###"}
{"summary": "has benchmark: Benchmark ImageNet/Benchmark ImageNet\nhas research problem: Image Classification/Image Classification\nhas model: BBG (ResNet-18)/BBG (ResNet-34)\nsame as: https://en.wikipedia.org/wiki/Contextual_image_classification/https://en.wikipedia.org/wiki/Contextual_image_classification", "text": "#Properties\nhas benchmark, has research problem, has model, same as\n#Text\nBinary neural networks have attracted numerous attention in recent years. However, mainly due to the information loss stemming from the biased binarization, how to preserve the accuracy of networks still remains a critical issue. In this paper, we attempt to maintain the information propagated in the forward process and propose a Balanced Binary Neural Networks with Gated Residual (BBG for short). First, a weight balanced binarization is introduced and thus the informative binary weights can capture more information contained in the activations. Second, for binary activations, a gated residual is further appended to compensate their information loss during the forward process, with a slight overhead. Both techniques can be wrapped as a generic network module that supports various network architectures for different tasks including classification and detection. The experimental results show that BBG-Net performs remarkably well across various network architectures such as VGG, ResNet and SSD with the superior performance over state-of-the-art methods. THE PROPOSED METHOD Maximizing Entropy with Balanced Binary WeightsIn Binary Neural Networks, the most important training parameter is the non-differentiable discrete binary weights. This discrete property of binary weights brings troublesome problems to the training of the network. To preserve the information of binary weights, we propose to maximize entropy of binary weight in the training process. Directly optimize the above regularization is hard. Instead, we approximate the optimal solution for it by making the expectation of binary weights to be zero to, i.e. w b 1 = 0.Hence, we first center the real-valued weights and then quantize them into binary codes. More specifically, we use a proxy parameter v \u2208 R d\u00d7d to get w and then quantize it into binary codes w b . In the convolutional layer, we express the weight w in terms of the proxy parameter v using:After balanced weight normalization, we can perform binary quantization on the floating-point weight w. Subsequently, the forward pass and backward pass of binary weights are as follows:where sign(\u2022) is sign function that outputs +1 for positive numbers and \u22121 otherwise and E(| \u2022 |) calculates the mean of absolute value. In our balanced weight quantization, the parameter updating is completed based on the proxy parameters v, and thus the gradient signal can back-propagate through the normalization process. In the whole process, v and w are floating-point, and we update v on training process and only w b is needed on inference. Reconstructing Information Flow with Gated ResidualIn the binarization of activation, we first clip the value range of activations x into [0, 1] and use a round function to binarize activations. Therefore, the forward pass and backward pass for binary activations are as follows:where I 0<x<1 means if elements of x is in the range of [0, 1], then it is 1, otherwise 0. Binarizing activations results in much larger loss of precision than binary weights. All activation values of different channels are quantized to 0 or 1, without considering the differences among the channels. The quantization error caused by binary layers is accumulated layer by layer. To address this problem, we further propose a new module named gated residual to reconstruct information flow in channel-wise manner, during the forward propagation. Our gated residual employs the floating-point activations to reduce quantization error and recalibrate features.Subsequently, we propose a new designed layer with gated weights s = [s 1 , ..., s c ] \u2208 R c that learn the channel attention information of floating-point input feature map x = [x 1 , ..., x c ] \u2208 R c\u00d7h\u00d7w in a binary convolution layer (c, h, w means channels, height and width respectively). The operation on the ith channel of the input feature map x i is defined as follows:Based on the gated residual, the output feature map y \u2208 R c\u00d7h\u00d7w can be recalibrated, enhancing the representation power of the activations. The operation in the gated module can be written in the following form:where F(x) means the operation in the main path including activation binarization, balanced convolution and BatchNorm in total.With gated residual layer, the overall structure of gated module is shown in the . We initialize weight of gated residual with 1 which is the same with identity shortcut of vanilla ResNet. In training process, the gated residual learns to distinguish the channels and eliminate the less useful channels. Beside reconstructing the information flow in the forward process, this path also acts as an auxiliary gradient path for activations in the backward propagation. Usually, the STE used in the backward pass to approximate discrete binary functions results in severe gradient mismatch problem. Fortunately, with learnable gated weights, our gated residual can also reconstruct gradient in the following way:In terms of computational complexity and memory limitation in the nature of the binary networks, the additional operations for designing a new module need to be as small as possible. The structure of HighwayNet  requires a fullprecision weights that is as large as the weight in the convolutional layer, which is unacceptable. Compared to SENet , SE module is a correction to the output after the convolution layer, and we argue that it is better to make use of the unquantified information. Similarly, our FLOPs is only c \u00d7 w \u00d7 h and is much smaller than the SE module. When the number of channels increases and the reduction decreases, the amount of FLOPs required by the SE module will be much more. EXPERIMENTSIn this section, to verify the effectiveness of our proposed BBG-Net, we conduct experiments on both image classification and object detection task. Datasets and Implementation DetailsDatasets. In our experiments, we adopt three common image classification datasets: CIFAR-10/100, and ILSVRC-2012 ImageNet. We also evaluate our proposed method on the object detection task with a standard detection benchmark, Pascal VOC datasets named VOC2007, and VOC2012.Network Architectures and Setup.We conduct our experiments on popular and powerful network architectures ResNet  and Single Shot Detector(SSD ). For fair comparison with the existing methods, we respectively choose ResNet-20, ResNet-18 as the baseline model on CIFAR-10/100 and ImageNet datasets. And we verify SSD with different backbones, i.e. VGG-16  and ResNet-34 in Pascal VOC datasets. As for hyper-parameter, we mostly follow the same setup in the original papers and all our models are trained from scratch. Note that we do not quantize the first and last layer and we also do not quantize down-sample layers as suggested by many previous work . And we only quantize backbone network in SSD. Ablation StudyNow we study how our balanced weight quantization and gated residual module affects the network's performance. In , we report the results of ResNet-20 on CIFAR-10, with and without balanced quantization or gated residual. In the performance comparison from the first two rows, the network with balanced quantization can obtain 0.6% accuracy than that without this operation. From the whole table, we can easily observe that, balanced weights or gated residual brings accuracy improvement and together they work even better with 1.2% accuracy improvement. It reveals that our proposed method faithfully helps pursue a highly accurate binary network. Comparison with the State-of-the-ArtCIFAR-10/100 Dataset. In ResNet-20 on CIFAR-10/100 datasets, we further compare four different kinds of modules in . As shown in , in CIFAR-10 datasets, the accuracy improvement caused by Gated or module is less than 1% while in a more challenging CIFAR-100 dataset, it adds up to over 2%. Through the whole experiments, we can conclude that Vanilla Gated and Gated show superiority ResNet-18 on ImageNet Dataset.In , we compare our method with many binarization methods of recent years including XNOR-Net , DoReFa-Net , Bireal-Net , PCNN , and it further reveals stability of our proposed BBG-Net on larger datasets. Our method significantly outperforms all the other methods, with 1.3% performance gain over the state-of-the-art ResNetE .We also improve the accuracy of binary networks by exploring network width and resolution in a simple but effective way. In the calculation of FLOPs, the binary layer is divided by 64 following Bireal-Net . In the network width experiments, we expand all channels of the original ResNet-18 by 2\u00d7 and 3\u00d7. Compared with the full-precision networks, our 3\u00d7 width models can achieve almost the same accuracy. We also compare with ABC-Net where 5/3 means 5 binary bases for weight and 3 bases for activations, our methods consistently perform better than ABC-Net by a large margin. In resolution exploration, we employ a simple strategy that we remove the max pooling layer after the first convolution, which makes all hidden layers have 2\u00d7 feature maps compared with the original ones. Compared to CircConv  which employs four times binary weights, we have better results with 1.6% accuracy improvement, while we have slightly higher FLOPs but the same memory. Our resolution accuracy is 2.3% higher while the FLOPs is only 1.1\u00d7 of the FLOPs of DenseNet-28 . BENN  ensembles 6 standard binary ResNet-18 which has nearly three times FLOPs of our resolution while the accuracy declines by 2%.SSD on Pascal VOC Dataset. In object detection task, we compare our method with XNOR-Net , TBN , and BDN  which includes DenseNet37 and DenseNet-45.In the comparison of ResNet-34 as its backbone, we outperform XNOR and TBN by 6.9% and 2.5%. It proves that our solution using 1-bit can preserve the stronger feature representation and maintain the better generalization ability than ternary neural networks. As for VGG-16, we only need half of FLOPs of binary DenseNet-45 to achieve slightly higher results and our model outperforms DenseNet-37 by 2% with one thirds fewer FLOPs. The accuracy of our VGG-16 is only 5.8% less than full-precision counterparts. The significant performance gain further demonstrates that our method can better preserve the information propagated in the network and help extract the most discriminative features for detection task. Deploying EfficiencyFinally, we implement our method on mobile devices using a framework named daBNN . The mobile device we use is Rasberry Pi 3B, which has a 1.2GHz 64-bit quad-core ARM Cortex-A53. As shown in , we implement DoReFa which binarizes downsample layers while we do not, the difference in inference time is only 2ms which can be ignored. Our proposed method can run 5.8\u00d7 faster than full-precision counterparts. CONCLUSIONBinarization methods for establishing portable neural networks are urgently required so that these networks with massive parameters and complex architectures can be launched efficiently. In this work, we proposed a novel Balanced Binary Neural Network with Gated Residual, namely BBG-Net. Experiments conducted on benchmark datasets and architectures demonstrate the effectiveness of the proposed balanced weight quantization and gated residual for learning binary neural networks with higher performance but lower memory and computation consumption than the state-of-the-art methods.\n###\n"}
{"text": "#Properties\nimplementation, Developer, Input format, programming language, Attributes, has research problem, Keyword, Filter, Sampling, Aggregation, Incremental, Disk, Domain, Application type\n#Text\nOntologies become increasingly important as a means to structure and organize information. This requires methods and tools that enable not only ontology experts but also other user groups to work with ontologies and related data. We have developed VOWL, a comprehensive and well-specified visual language for the user-oriented representation of ontologies, and conducted a comparative study on an initial version of VOWL. Based upon results from that study, as well as an extensive review of other ontology visualizations, we have reworked many parts of VOWL. In this paper, we present the new version VOWL 2 and describe how the initial definitions were used to systematically redefine the visual notation. Besides the novelties of the visual language, which is based on a well-defined set of graphical primitives and an abstract color scheme, we briefly describe two implementations of VOWL 2. To gather some insight into the user experience with the new version of VOWL, we have conducted a qualitative user study. We report on the study and its results, which confirmed that not only the general ideas of VOWL but also most of our enhancements for VOWL 2 can be well understood by casual ontology users. Graph Visualizations of OntologiesMany approaches visualize ontologies as graphs, which is a natural way to depict the structure of the concepts and relationships in a domain of knowledge. The graphs are often rendered in force-directed or hierarchical layouts, resulting in appealing visualizations. However, only few visualizations show complete ontologies, but most focus on certain aspects. For instance, OWLViz , OntoTrack , and KC-Viz depict only the class hierarchy of ontologies. OWLPropViz , OntoGraf , and FlexViz represent different types of property relations, but do not show datatype properties and property characteristics required to fully understand ontologies.A smaller number of approaches provide more comprehensive graph visualizations that represent all key elements of ontologies. Unfortunately, the different ontology elements are often hard to distinguish in the visualizations. For instance, TGViz and NavigOWL use very simple graph visualizations where all nodes and links look the same except for their color. This is different in GrOWL and SOVA , which define more elaborated notations using different symbols, colors, and node shapes. However, as the notations of both GrOWL and SOVA rely symbols from description logic and abbreviations, they are not perfectly suited for casual users. Furthermore, the visualizations created with GrOWL and SOVA are characterized by a large number of crossing edges which has a negative impact on the readability.Other graph visualizations focus on specific tasks. The RelFinder , for instance, visualizes relationships between individuals described by ontologies and makes these relationships interactively explorable. GLOW uses a radial tree layout and hierarchical edge bundles to depict relationships within ontologies . Both approaches provide some insight into links between certain classes and individuals, but they do not give an overview of the complete ontology. Ontology Visualizations Based on Specific Diagram TypesThere are also a number of works that use other types of diagrams than graph visualizations to represent ontologies. For instance, Jambalaya and OWL-VisMod use treemaps to depict the class hierarchy of ontologies. Jambalaya additionally provides a nested graph visualization called SHriMP that allows to split up the class hierarchy into different views . CropCircles is a related visualization technique that visualizes the class hierarchy of ontologies with the goal to support the identification of \"undermodeled\" ontology parts . All these approaches visualize once again mainly the class hierarchy, without considering other property relations.Cluster Maps use a visualization technique that is based on nested circles and has also been successfully applied to ontologies . Instead of showing the class hierarchy, Cluster Maps visualize individuals grouped by the classes they are instances of. Similar techniques are used in VisCover and OOBIAN Insight that additionally provide a number of interactive filtering capabilities. Another related approach is gFacet , where individuals are grouped by their classes and can be filtered by selecting linked individuals or data values. While using appealing visualizations, these tools show only a selection of classes along with their instances but do not provide complete visualizations of ontologies.A powerful type of diagram related to OWL and often reused to visualize ontologies is the class diagram of the Unified Modeling Language (UML) . Precise mappings between OWL and UML class diagrams are specified in the Ontology Definition Metamodel (ODM) , among others. A major drawback of such attempts is that they require some knowledge about UML class diagrams. Although many people with an IT background are familiar with these types of diagrams, people from other domains have difficulties interpreting them correctly, as we also found in the aforementioned comparative study . VOWL 2: Visual Notation for OWL OntologiesBased upon our review of related work and the comparative evaluation , we decided to retain numerous traits of the initial VOWL version (VOWL 1). As already mentioned, graphs seem to be a natural and intuitive way to represent the structure of ontologies, which is confirmed by many of the related work reported above. VOWL is based on a mapping of OWL elements to graphical depictions that are combined into a graph representing the ontology. For VOWL 2, we have reworked these mappings and taken into account the exact semantics of all definitions from OWL that were considered. In particular, we have broken down the components of VOWL to a set of basic building blocks consisting of shapes and colors that express specific aspects of the OWL elements (datatype or object properties, different class and property characteristics, etc.), also considering possible combinations thereof.VOWL 1 included an integrated view that would display the TBox of an ontology along with information from the ABox. Comments from the initial user study on VOWL 1, however, led us to conclude that concerns about the scalability of the integrated view were justified. Even with few instances per class, additional information, such as property values of instances, would be difficult to show without creating lots of clutter. Therefore, VOWL 2 focuses on displaying primarily the TBox and only optionally integrates\n###\n", "summary": " implementation: VOWL 2\n, has research problem: Graph-based visualization systems\n, Keyword: F\n, Filter: F\n, Sampling: F\n, Aggregation: F\n, Incremental: F\n, Disk: F\n, Domain: Ontology\n, Application type: Web\n###"}
{"text": "#Properties\nimplementation, metric, has research problem, evaluation, Approach type, Document type, Summary usage, Summary characteristics\n#Text\nIn this paper, a novel summarization method that uses nonnegative matrix factorization (NMF) and the clus tering method is introduced to extract meaningful sentences relevant to a given query. The proposed method decomposes a sentence into the linear combination of sparse nonnegative semantic features so that it can represent a sentence as the sum of a few semantic features that are comprehensible intuitively. It can improve the quality of document summaries because it can avoid extracting those sentences whose similarities with the query are high but that are meaningless by using the similarity between the query and the semantic features. In addition, the proposed approach uses the clustering method to remove noise and avoid the biased inherent semantics of the documents being reflected in summaries. The method can ensure the coherence of summaries by using the rank score of sentences with respect to semantic features. The experimental results demonstrate that the proposed method has better performance than other methods that use the thesaurus, the latent semantic analysis (LSA), the K-means, and the NMF. Sakurai and Utsumi proposed a query-based summarization method using a thesaurus. Their method generates the core part of the summary from the most relevant document to the query using a thesaurus, and then the additional part of the summary, which elaborates upon the query, from the other documents. Their method works well for long summaries, while its performance is not satisfactory for short summaries.proposed a query-based document summarization method using NMF. proposed a multidocument summarization method based on clustering using NMF. This method clusters the sentences and extracts sentences using the cosine similarity measure between a topic and semantic features. This method improves the quality of summaries and avoids the topic being deflected in the sentence structure by clustering sentences and removing noise, but it may also extract more or less similar but meaningless sentences from documents and does not consider the coherence of the extracted sentences. proposed a multidocument summarization method using weighted NMF and clustering methods . Nonnegative Matrix FactorizationIn this paper, we define the matrix notation as follows. Let X * j be the j' th column vector of matrix X, X i* be the i' th row vector, and X ij be the element of the i' th row and the j' th column. Nonnegative matrix factorization (NMF) is to decompose a given m 3 n matrix A into a nonnegative semantic feature matrix (NSFM) W and a nonnegative semantic variable matrix (NSVM) H as shown in Equation (1):We use the objective function that minimizes the Euclidean distance between each column of A and its approximation \u00c3<WH, which was proposed by Lee and Seung . As an objective function, the Frobenius norm is used :where r are usually chosen to be smaller than m or n so that the total size of W and H is smaller than that of matrix A. A column vector corresponding to the j' th sentence, A *j , can be represented as a linear combination of the semantic feature vectors W * l and the semantic variable H l j as follows:Example (1). shows some of the sentences extracted from a document on the topic \"Native American reservation system-pros and cons\" in the Document Understanding Conference (DUC) data set . We generate the term-by-sentence matrix A by preprocessing a set of sentences in . Matrix A is composed of 541 terms and 54 sentences. illustrates cases of applying NMF to matrix A. shows an interpretation of basis vectors and feature vectors. There are no negative values and many zero values in . That is, semantic feature vectors obtained by using NMF are sparse so that NMF can obtain semantic features having a small range of semantics. This indicates that a method that uses NMF has better power to identify subtopics of documents than do methods that use decomposition approaches, such as principal components analysis (PCA) and vector quantization (VQ) . Besides, the semantic feature vectors in intuitively make more sense because NMF represents a sentence as the linear combination of a few intuitive semantic feature vectors having nonnegative values. Comparison of Latent Semantic Analysis and Nonnegative Matrix FactorizationLatent semantic analysis (LSA) is a decomposition method using singular value decomposition (SVD). This method decomposes matrix A into three matrices, U, D, and V T :where U is an m 3 n orthonormal matrix of eigenvectors of AA T (left singular vectors), and V is an n 3 n orthonormal matrix of eigenvectors of A T A (right singular vectors). D 5 diag(s1, s2 ..., sn) is an n 3 n diagonal matrix whose diagonal elements are nonnegative eigenvalues sorted in a descending order. \u0169 is a m 3 u matrix, wherewhereFinally, r < n.In the method using LSA, the i'th column vector A *i of matrix A is the weight vector of the i'th sentence, and is represented as the linear combination of the left eigenvectors U *j , which are semantic feature vectors, as shown in Equation . That is, the weight of the j'th semantic vector U *j corresponding to the sentence vectorExample . We illustrate an example using LSA and NMF. In LSA, let r be 3. LSA decomposes matrix A into U, D, and V, as shown in . shows an example of sentence representation using LSA. The column vector A *3 corresponding to the third sentence is represented as a linear combination of feature vectors U *j and their weights s j V ij . In NMF, let r be 2, the number of repetitions 50, and the tolerance 0.001. When the initial elements of the W and H matrices are 0.5, the nonnegative matrix A is decomposed into two nonnegative matrices, W and H, as shown in . shows an example of sentence representation using NMF. The column vector A *3 corresponding to the third sentence is represented as a linear combination of the semantic feature vectors W *l and the semantic variable column vector H *3 .Example . We analyzed examples\n###\n", "summary": " implementation: Implementation\n, metric: ROUGE 1/ROUGE 2/ROUGE L/ROUGE SU4/ROUGE W\n, has research problem: Automatic text summarization\n, evaluation: Evaluation\n, Approach type: Graph based\n, Document type: Multiple documents\n, Summary usage: Query based\n, Summary characteristics: Extractive\n###"}
{"text": "#Properties\nimplementation, dataset, has research problem, evaluation, Task, Test questions, Train questions, Question language, Language, On, Question analysis task, Phrase mapping task, Disambiguation task, Query construction task\n#Text\nWith the advent of Big Data concept, a lot of attention has been paid to structuring and giving semantic to this data. Knowledge bases like DBPedia play an important role to achieve this goal. Question answering systems are common approach to address expressivity and usability of information extraction from knowledge bases. Recent researches focused only on monolingual QA systems while cross-lingual setting has still so many barriers. In this paper we introduce a new cross-lingual approach using a unified semantic space among languages. After keyword extraction, entity linking and answer type detection, we use cross lingual semantic similarity to extract the answer from knowledge base via relation selection and type matching. We have evaluated our approach on Persian and Spanish which are typologically different languages. Our experiments are on DBPedia. The results are promising for both languages. MethodIn this section we introduce our proposed approach to deal with cross lingual QA over KBs. Any given question passes through four stages in a pipeline including 1. Keyword Extraction, 2. Keyword Type Detection, 3. Entity Linking & Ontology Type Extraction, and finally 4. Answer Extraction. We have employed QALD-5 as our dataset for training and testing, and DBPedia 2014 as our KB to extract answers from. We first briefly describe preparing dataset and then explain each of above stages in detail. Preparing DatasetQALD-5 is a multilingual QA dataset over DBPedia 2014 for QALD task at CLEF 2015. It contains 300 training questions in 7 languages with annotated keywords and queries to extract answers from DB-Pedia and 50 questions as test set. To add Persian translation to these dataset, the questions were translated to Persian by a language expert outside development team. To annotate keywords of each Persian questions we have used majority voting among 5 annotators. Each word has tagged as B, I or O. Also we have augmented this dataset with answer type tag. Each keyword has tagged as type detector or neutral. We have chosen these tags through majority voting among 5 annotators. Keyword ExtractionIn the first stage, the input question have to be analysed to extract content words which we call them keywords. A MaxEnt Markov Model is used in order to extract these keywords through sequence labelling. The features used to train the model are: 1. Unigram, bigram and trigram of POS tags, 2. Chunk tag, 3. Position of the word in question, 4. IDF of word in corpus 1 , 5. Exact match with entity labels in KB and 6. Babelfy tag . Keyword Type DetectionIn the second stage each keyword is classified as 1. Type Detector, 2. Grounded Entity or 3. Neutral. To do that we have utilized an SVM classifier with RBF kernel because it has the best performance in 10-fold cross validation in our experiments. The following features are used to train the SVM classifier: 1. Number of words in keyword, 2. POS tags of words in keyword, 3. Position of the first word of the keyword in question, 4. Average IDF of words of keywords in corpus, 5. Exact match with entity labels, 6. Babelfy tag, and 7. Match of translation with ontology types 2 . Entity Linking & Ontology Type ExtractionWhen the keywords that must be linked to some entities in the KB or refer to some types in the KB ontology have been found, we should link each of which to its appropriate entity or type. English and then search in KB ontology. Only string similarity between translated keyword and ontology types is used. Answer ExtractionIn the last stage using extracted entities, ontology types and keywords classified as Neutral in subsection 3.3, we search in KB graph. All entities in 2-hop vicinity of the found entities in KB whose types are different from extracted ontology types are pruned. If there are entities of desired types with different path labels to the found entities, the cross lingual semantic similarity model contributed by Camacho-Collados (2015) is used to select the most similar relation with the keywords of tagged as Neutral. We have used unified vectores constructed according to BabelNet synsets.To deal with aggregation questions, we have extracted all questions from the training data with atribute aggregation = \"true\" and then grouped them in four types:1. Sort 2. Count 3. Regular expresion 4. Time For each of these groups, POS tags and words which are common among all members of that group have been extracted and their frequencies have been calculated and normalized. For a given question four scores related to each agregation type is caclulated using following formula:is the number of occurance of * in the question and S( * ) is the normalized score obtained for * using agregation training questions. For each type which have some score greater than a threshold, agregation of that type is operated on the final result of the answer extracted. The threshold are calculated using all training questions. ExperimentsTo evaluate our approach we have conducted experiments on Persian and Spanish. we have used DBPedia 2014 as the KB that answers must be extracted from it. We have tested our system on QALD-5 test set. It contains 49 questions in both languages. 63.0 As a baseline we translate each question to English using Google Translate. shows the result of our approach for both Persian and Spanish questions compared with results of the baseline. Errors in translating of named entities in fully translating a question is one of main sources of errors in baseline with proportion of 64%.We have compared the performance of the monolingual version of our approach with the best participant of QALD-5 challenge. shows the results. Despite less annotation cost for training the model compared with Xser, our system improved F 1 by 2.2%.Since our proposed approach consist of a pipeline of pre-processing, we have evaluated internal stages of our system. shows the results for each stage. The reported accuracies are average accuracy over 10-fold cross validation.We have also evaluated the influence of calculating semantic similarity using unified vectors on accuracy of our method. Semantic\n###\n", "summary": " implementation: UTQA\n, has research problem: Question answering systems\n, Question analysis task: POS learned\n, Phrase mapping task: Distributional Semantics/Knowledge base labels/Redirects/String similarity\n, Disambiguation task: Local disambiguation\n###"}
{"text": "#Properties\nHas value, Global Mean Sea level Rise Projection, has lower limit for likely range, has upper limit for likely range, has  start of period, has end of period, climate scenario, has research problem, has unit\n#Text\nStrategies to manage the risks posed by future sea-level rise hinge on a sound characterization of the inherent uncertainties. One of the major uncertainties is the possible rapid disintegration of large fractions of the Antarctic ice sheet in response to rising global temperatures. This could potentially lead to several meters of sea-level rise during the next few centuries. Previous studies have typically been silent on two coupled questions: (i) What are probabilistic estimates of this Bfast dynamic^contribution to sea-level rise? (ii) What are the implications for strategies to manage coastal flooding risks? Here, we present probabilistic hindcasts and projections of sea-level rise to 2100. The fast dynamic mechanism is approximated by a simple parameterization, designed to allow for a careful quantification of the uncertainty in its contribution to sea-level rise. We estimate that global temperature increases ranging from 1.9 to 3.1\u00b0C coincide with fast Antarctic disintegration, and these contributions account for sea-level rise of 21-74 cm this century (5-95% range, Representative Concentration Pathway 8.5). We use a simple cost-benefit analysis of coastal defense to demonstrate in a didactic exercise how neglecting this mechanism and associated uncertainty can (i) lead to strategies which fall sizably short of protection targets and (ii) increase the expected net costs. Keywords Sea-level projections. West Antarctic ice sheet. Coastal flood defense. Climate impacts. Cliff failure and hydrofracturing Methods Sea-level riseWe employ and expand upon a model framework that has been previously applied for probabilistic projections of sea-level rise . This model has recently been made available as the building blocks for Relevant Ice and Climate Knowledge (BRICK) model v0.2 to simulate global mean surface temperature, ocean heat uptake, global mean sea level and its contributions from the Antarctic ice sheet, Greenland ice sheet, thermal expansion, and glaciers and small ice caps . BRICK uses a semi-empirical modeling approach, combining a platform of previously published models. The model is described in greater detail by , so we only provide an overview here.Global mean surface temperature and ocean heat uptake are simulated by the Diffusion-Ocean-Energy balance CLIMate model DOECLIM . DOECLIM is a zero-dimensional energy balance model coupled to a three-layer, one-dimensional diffusive ocean model. The input required to force DOECLIM is the radiative forcing time series (W m \u22122 ), which is provided based on the work of , with some updates and extensions, as described in greater detail in previous studies using DOECLIM . We use a 1-year time step, and the output global mean surface temperature couples to sealevel sub-models representing individual major sea-level contributions. All sea levels are presented relative to their 1986-2005 mean.The Greenland ice sheet is represented by the Simple Ice Sheet Model for Projecting Large Ensembles (SIMPLE; . SIMPLE first estimates an equilibrium Greenland ice sheet volume (V eq,GIS ), given an anomaly in global mean temperature (T g ), as well as the e-folding timescale of the ice sheet volume as it exponentially relaxes towards this equilibrium volume (\u03c4 GIS ).In Eqs. 1 and 2, t represents time (years), c GIS is the equilibrium ice sheet volume sensitivity to temperature (m SLE\u00b0C \u22121 ), b GIS is the equilibrium ice sheet volume for zero temperature anomaly (m SLE), \u03b1 GIS is the temperature sensitivity of the efolding ice sheet response timescale (\u00b0C \u22121 year \u22121 ), and \u03b2 GIS is the equilibrium response timescale (year \u22121 ). These quantities are uncertain model parameters, which we estimate as described in and briefly in Sect. 2.3. The change in Greenland ice sheet volume (V GIS ) can then be written asWe make the assumption that all GIS volume lost makes its way into the oceans. The contribution to sea level from glaciers and small ice caps (GSIC) is represented by the GSIC sub-model of the Model for Assessment of Greenhouse-Gas-Induced Climate Change (MAGICC) . The GSIC sea-level contribution (S GSIC ) is parameterized asIn Eq. 4, the uncertain model parameters are \u03b2 0 , the GSIC mass balance sensitivity to global temperature anomalies (m\u00b0C \u22121 year \u22121 ); V 0,GSIC , the initial GSIC volume susceptible to melt (m SLE); and n, the area-to-volume scaling parameter (unitless). These parameters are estimated as in . T eq,GSIC is taken equal to \u22120.15\u00b0C .Our parameterization for sea-level rise due to thermal expansion was originally formulated for global sea level by and adapted for thermal expansion by . First, an equilibrium thermal expansion is calculated (S eq,TE ), given the anomaly in global mean temperature:a TE , the sensitivity of this equilibrium thermal expansion to temperature changes (m\u00b0C \u22121 ), and b TE , the equilibrium thermal expansion for zero temperature anomaly (m SLE), are estimated as uncertain model parameters . The thermal expansion contribution to global mean sea level is modeled as an exponential relaxation towards S eq,TE :where \u03c4 TE is the e-folding timescale of the thermal expansion response, and the quantity 1/\u03c4 TE is estimated as a model parameter where B tot (m 3 year \u22121 ) represents the total rate of accumulation of Antarctic ice sheet mass and F (m 3 year \u22121 ) is the ice volume flux across the grounding line. T (\u00b0C) is the Antarctic surface temperature reduced to sea level (as the Antarctic surface is largely above sea level), S is sea level (m), and R is the Antarctic ice sheet radius (m). The interested reader is directed to and for more information about the DAIS model. Antarctic ice sheet fast dynamic parameterizationThe original DAIS model includes a parameterization for dynamic ice loss over the grounding line as it retreats due to subsurface ocean warming (F in Eq. 7 above; Shaffer 2014). This ice flux depends on the Antarctic ice sheet geometry, the water depth, and water temperature. This misses the critical link between rising global temperatures and the sudden, fast ceasing of buttressing ice shelves due to processes such as hydrofracturing and ice cliff failure , which may substantially speed up the dynamic outflow . We form an explicit link between global surface temperatures and these\n###\n", "summary": " Has value: 0.28\n, Global Mean Sea level Rise Projection: Global Mean Sea Level Rise Projections\n, has  start of period: 2000\n, has end of period: 2050\n, climate scenario: RCP4.5\n, has research problem: Global Mean Sea Level Rise Projections\n, has unit: m\n###"}
{"text": "#Properties\nHas method, Has result, related to, has GenBank accession number, has source, has location, reagent, has kit, has study design, number of persons tested, open invitation, random sample, number of persons with viral genome sequenced, Virus names in GISAID, EPI accession numbers, number of sequence variants, GISAID URL, according to external source (not stated in paper), has nucleotide substitution, compared to, has NCBI Reference Sequence accession number, has company, has SARS-CoV-2 reference genome , differs to the NC_045512 reference genome, software for consensus sequence generation, have nucleotide differences, has international travel, has countries, has software, has amino acid change, due to, Participants' characteristics (of which viral agents were sequenced), patient characteristics, has research problem, website, has number, has length, sequencing platform, read length in base pairs (paired-end*) , has application, Study date, has age, has gender, has illness, date of illness onset, date of hospital admission, has symptom, receives therapy, has drug, has technique, RNeasy Plus Universal Mini kit, has date \n#Text\nArticle Methods Extended Data Fig. 9 | Recombination events in WHCV. The sequence similarity plot of WHCV, SARS-like CoVs and bat SARS-like CoVs reveals putative recombination events. Emerging infectious diseases, such as severe acute respiratory syndrome (SARS) and Zika virus disease, present a major threat to public health . Despite intense research efforts, how, when and where new diseases appear are still a source of considerable uncertainty. A severe respiratory disease was recently reported in Wuhan, Hubei province, China. As of 25 January 2020, at least 1,975 cases had been reported since the first patient was hospitalized on 12 December 2019. Epidemiological investigations have suggested that the outbreak was associated with a seafood market in Wuhan. Here we study a single patient who was a worker at the market and who was admitted to the Central Hospital of Wuhan on 26 December 2019 while experiencing a severe respiratory syndrome that included fever, dizziness and a cough. Metagenomic RNA sequencing of a sample of bronchoalveolar lavage fluid from the patient identified a new RNA virus strain from the family Coronaviridae, which is designated here 'WH-Human 1' coronavirus (and has also been referred to as '2019-nCoV'). Phylogenetic analysis of the complete viral genome revealed that the virus was most closely related (89.1% nucleotide similarity) to a group of SARS-like coronaviruses (genus Betacoronavirus, subgenus Sarbecovirus) that had previously been found in bats in China . This outbreak highlights the ongoing ability of viral spill-over from animals to cause severe disease in humans.The patient studied was a 41-year-old man with no history of hepatitis, tuberculosis or diabetes. He was admitted to and hospitalized in the Central Hospital of Wuhan on 26 December 2019, 6 days after the onset of disease. The patient reported fever, chest tightness, unproductive cough, pain and weakness for 1 week on presentation . Physical examination of cardiovascular, abdominal and neurological characteristics was that these were normal. Mild lymphopoenia (defined as less than 9 \u00d7 10 5 cells per ml) was observed, but white blood cell and blood platelet counts were normal in a complete blood count test. Elevated levels of C-reactive protein (41.4 mg l \u22121 of blood; reference range, 0-6 mg l \u22121 ) were observed and the levels of aspartate aminotransferase, lactic dehydrogenase and creatine kinase were slightly elevated in blood chemistry tests. The patient had mild hypoxaemia with oxygen levels of 67 mm Hg as determined by an arterial blood gas test. On the first day of admission (day 6 after the onset of disease), chest radiographs were abnormal with air-space shadowing such as ground-glass opacities, focal consolidation and patchy consolidation in both lungs (Extended Data ). Computedtomography scans of the chest revealed bilateral focal consolidation, lobar consolidation and patchy consolidation, especially in the lower lung (Extended Data . A chest radiograph revealed a bilateral diffuse patchy and fuzzy shadow on day 5 after admission (day 11 after the onset of disease) (Extended Data ). Preliminary aetiological investigations excluded the presence of influenza virus, Chlamydia pneumoniae and Mycoplasma pneumoniae using commercial pathogen antigen-detection kits, and this was confirmed by PCR. Other common respiratory pathogens, including human adenoviruses, also tested negative by quantitative PCR (qPCR) (Extended Data . Although a combination of antibiotic, antiviral and glucocorticoid therapy was administered, the patient exhibited respiratory failure and was given high-flow non-invasive ventilation. The condition of the patient did not improve after 3 days of treatment and he was admitted to the intensive care unit. The patient was transferred to another hospital in Wuhan for further treatment 6 days after admission.Epidemiological investigations by the Wuhan Center for Disease Control and Prevention revealed that the patient worked at a local indoor seafood market. Notably, in addition to fish and shellfish, a variety of live wild animals-including hedgehogs, badgers, snakes and birds (turtledoves)-were available for sale in the market before the outbreak began, as well as animal carcasses and animal meat. No bats were available for sale. While the patient might have had contact with wild animals at the market, he recalled no exposure to live poultry.To investigate the possible aetiological agents associated with this disease, we collected bronchoalveolar lavage fluid (BALF) and performed deep meta-transcriptomic sequencing. The clinical specimen was handled in a biosafety level 3 laboratory at Shanghai Public Health Clinical Center. Total RNA was extracted from 200 \u03bcl of BALF and a meta-transcriptomic library was constructed for pairend (150-bp reads) sequencing using an Illumina MiniSeq as previously described . In total, we generated 56,565,928 sequence reads that were de novo-assembled and screened for potential aetiological agents. Of the 384,096 contigs assembled by Megahit 9 , the longest (30,474 nucleotides (nt)) had a high abundance and was closely related to a bat SARS-like coronavirus (CoV) isolate-bat SL-CoVZC45 (GenBank accession number MG772933)-that had previously been sampled in China, with a nucleotide identity of 89.1% ). The genome sequence of this virus, as well as its termini, were determined and confirmed by reverse-transcription PCR (RT-PCR) and 5\u2032/3\u2032 rapid amplification of cDNA ends (RACE), respectively. This virus strain was designated as WH-Human 1 coronavirus (WHCV) (and has also been referred to as '2019-nCoV') and its whole genome sequence (29,903 nt) has been assigned GenBank accession number MN908947. Remapping the RNA-sequencing data to the complete genome of WHCV resulted in an assembly of 123,613 reads, providing 99.99% genome coverage at a mean depth of 6.04\u00d7 (range, 0.01-78.84\u00d7) (Extended Data ). The viral load in the BALF sample was estimated by qPCR to be 3.95 \u00d7 10 8 copies per ml (Extended Data .The viral genome organization of WHCV was determined by sequence alignment to two representative members of the genus Betacoronavirus: a coronavirus associated with humans (SARS-CoV Tor2, GenBank accession number AY274119) and a coronavirus associated with bats (bat SL-CoVZC45, GenBank accession number MG772933). The untranslational regions and open-reading frame (ORF) of WHCV were mapped on the basis of this sequence alignment and ORF prediction. The WHCV viral genome was similar to these two coronaviruses ( and Supplementary ). The order of genes (5\u2032 to\n###\n", "summary": " Has method: Agilent 2100 Bioanalyzer (Agilent Technologies)/Qbit/BLASTn/Bowtie2/Diamond BLASTx/Megahit (v.1.1.3)/RSEM program/Trimmomatic program/Trinity (v.2.5.1)/SMARTer Stranded Total RNA-Seq kit v.2 (TaKaRa)/bronchoalveolar lavage fluid (BALF)/Paired-end (150-bp reads) sequencing/RNA extraction/RNA quantity & quality check/data processing & analysis/library construction/sample collection/sequencing\n, Has result: negative/Severe acute respiratory syndrome coronavirus 2 isolate Wuhan-Hu-1/longest contig/number of contigs assembled by Megahit/number of sequence reads\n, related to: bat SL-CoVZC45\n, has GenBank accession number: MN908947/MG772933\n, has source: https://www.ncbi.nlm.nih.gov/nuccore/MG772933/https://www.ncbi.nlm.nih.gov/nuccore/MN908947\n, has location: \"Wuhan\n, has company: Illumina\n, patient characteristics: patient\n, has research problem: identification of SARS-CoV-2 by Metagenomic RNA sequencing/sequencing of SARS-CoV-2 genome/unknown viral agent found in worker of Wuhan sea food market is SARS-CoV-2\n, website: http://www.illumina.com/\n, has number: \"384\n, has length: \"30\n, sequencing platform: MiniSeq platform (Illumina)\n, read length in base pairs (paired-end*) : 150-300*\n, has application: \"RNA-seq/complex genomes (human\n, has age: 41 years\n, has gender: male\n, has illness: bacterial test/date of hospital admission/date of illness onset/symptoms\n, date of illness onset: 20 December 2019\n, date of hospital admission: during admission\n, has symptom: chest tightness/cough/dizzy/dyspnoea/fever/sputum production/weakness\n, receives therapy: Antibiotic therapy/Antiviral therapy/Glucocorticoid therapy/Oxygen therapy\n, has drug: Cefoselis/Oseltamivir\n, has technique: mechanical ventilation\n, RNeasy Plus Universal Mini kit: has company\n, has date : during admission\n###"}
{"summary": "has benchmark: Benchmark WMT2016 Romanian-English/Benchmark WMT2014 English-German/Benchmark WMT2014 German-English/Benchmark WMT2016 English-Romanian/Benchmark WMT2016 Romanian-English/Benchmark WMT2014 English-German/Benchmark WMT2014 German-English/Benchmark WMT2016 English-Romanian\nhas research problem: Machine Translation/Machine Translation\nhas model: CMLM+LAT+1 iterations/CMLM+LAT+4 iterations\nsame as: https://en.wikipedia.org/wiki/Machine_translation/MT/https://en.wikipedia.org/wiki/Machine_translation/MT\nSub Problem: Open Machine Translation/Open Machine Translation", "text": "#Properties\nhas benchmark, has research problem, has model, same as, Sub Problem\n#Text\nIn this work, we introduce a novel local autoregressive translation (LAT) mechanism into non-autoregressive translation (NAT) models so as to capture local dependencies among target outputs. Specifically, for each target decoding position, instead of only one token, we predict a short sequence of tokens in an autoregressive way. We further design an efficient merging algorithm to align and merge the output pieces into one final output sequence. We integrate LAT into the conditional masked language model (CMLM; Ghazvininejad et al., 2019) and similarly adopt iterative decoding. Empirical results on five translation tasks show that compared with CMLM, our method achieves comparable or better performance with fewer decoding iterations, bringing a 2.5x speedup. Further analysis indicates that our method reduces repeated translations and performs better at longer sentences. CMLM with LAT ModelWe integrate our LAT mechanism into CMLM, which predicts the full target sequence based on the source and partial target sequence. We adopt a lightweight LSTM-based sequential decoder as the local translator upon the CMLM decoder outputs. For a target position i, the CMLM decoder produces a hidden vector pos i , based on which the local translator predicts a short sequence of tokens in an autoregressive way, i.e., t 1Here K is the number of location translation steps, which is set to 3 in our experiments to avoid affecting the speed much. DecodingDuring inference, a special token, sop (start of piece) is fed into the local translator to generate a short sequence based on the pos i . After generating the local pieces for all target positions in parallel, we adopt a simple algorithm to merge them into a full output sequence. This merging algorithm is described in detail in Section 3. We also perform iterative decoding following the same Mask-Predict strategy . In each iteration, we take the output sequence from the last iteration and mask a subset of tokens with low confidence scores by a special mask symbol. Then the masked sequence is fed together with the source sequence to the decoder for the next decoding iteration.Following , a special token LENGTH is added to the encoder, which is utilized to predict the initial target sequence length. Nevertheless, our algorithm can dynamically adjust the final output sequence and we find that our method is not sensitive to the choice of target length as long as it falls in a reasonable range. TrainingThe training procedure is similar to that of . Given a pair of source and target sequences S and T , we first sample a masking size from a uniform distribution from , where N is the target length. Then this size of tokens are randomly picked from the target sequence and replaced with the mask symbol. We refer to the set of masked tokens as T mask . Then for each target position, we adopt a teacher-forcing styled training scheme to collect the cross-entropy losses for predicting the corresponding groundtruth local sequences, the size of which is K = 3.Assume that we are at position i, we simply setup the ground-truth local sequencewhere T i denotes the i-th token in the full target ground-truth sequence. We include all tokens in our final loss, whether they are in T mask or not, but adopt different weights for the masked tokens that do not appear in the inputs. Therefore, our token prediction loss function is:Here, we adopt a weight \u03b1 for the tokens that are not masked in the target input, which is set as 0.1 so that the model could be trained more on the unseen tokens. Furthermore, we randomly delete certain positions (the number of deletion is randomly sampled from [1, 0.15*N ]) from the target inputs to encourage the model to learn insertion-styled operations. The final loss is the addition of the token prediction and the target length prediction loss. Merging AlgorithmIn decoding, the model generates local translation pieces for all decoding positions. We adopt a simple algorithm that incrementally builds the output through a piece-by-piece merging process. Our hypothesis is that if the local autoregressive translator is well-trained, then 1) the token sequence inside each piece is fluent and well-translated, 2) there are going to study here will study in the  overlaps between nearby pieces, acting as aligning points for merging. We first illustrate the core operation of merging two consecutive pieces of tokens. Algorithm 1 describes the procedure and  provides an example. Given two token pieces s1 and s2, we first use the Longest Common Subsequence (LCS) algorithm to find matched tokens (Line 1). If there is nothing that can be matched, then we simply do concatenation (Line 3), otherwise we solve the conflicts of the alternative spans by comparing their confidence scores (Line 9-14). Finally we can arrive at the merged output after resolving all conflicted spans.In the above procedure, we need to specify the score of a span. Through preliminary experiments, we find a simple but effective scheme. From the translation model, each token gets a model score of its log probability. For the score of a span, we average the scores of all the tokens inside. If the span is empty, we utilize a pre-defined value, which is empirically set to log 0.25. For aligned tokens, we choose the highest scores among them for later merging process (Line 16).With this core merging operation, we apply a left-to-right scan to merge all the pieces in a pieceby-piece fashion. For each merging operation, we only take the last K tokens of s1 and the first K tokens of s2, while other tokens are directly copied. This ensures that the merging will only be local, to mitigate the risk of wrongly aligned tokens. Here, K is again the local translation step size.Our merging algorithm can be directly applied at the end of each iteration in the iterative decoding. However, since the output length of the merging algorithm is not always the same as the number of input pieces, we further adopt a length adjustment procedure for intermediate iterations. Briefly speaking, we adjust the output length to the predicted length by adding or deleting certain amounts of special mask symbols. Please refer to the Ap-Algorithm 1: Merging two pieces.  pendix for more details.Although our merging algorithm is actually autoregressive, it does not include any neural network computations and thus can run efficiently. In addition to efficiency, our method also makes the decoding more flexible, since the final output is dynamically created through the merging algorithm. Experiments Experimental SetupWe evaluate our proposed method on five translation tasks, i.e., WMT'14 EN\u2194DE, WMT'16 EN\u2194RO and IWSLT'14 DE\u2192EN. Following previous works , we train a vanilla base transformer  on each dataset and use its translations as the training data. The BLEU score  is used to evaluate the translation quality. Latency, the average decoding time (ms) per sentence with batch size 1, is employed to measure the inference speed. All models' decoding speed is measured on a single NVIDIA TITAN RTX GPU.We follow most of the hyperparameters for the CMLM      LSTM-based neural network of size 512. Finally, we average 5 best checkpoints according to the validation loss as our final model. Please refer to the Appendix for more details of the settings. Main resultsThe main results are shown in . Compared with CMLM at the same number of decoding iterations (row 2 vs. 3 and row 4 vs. 5), LAT performs much better while keeping similar speed, especially when the iteration number is 1. Note that since our method is not sensitive to predicted length, we only take one length candidate from our length predictor instead of 5 as in CMLM. Furthermore, LAT with 4 iterations could achieve similar or better results than CMLM with 10 iterations (row 5 vs. 6) but have a nearly 2.5x decoding speedup. AnalysisOn local translation step. We also explore the effects of the number of local translation steps (K) on the IWSLT'14 DE-EN dataset. The results are shown in . Generally, with more local translation steps, there can be certain improvements on BLEU but with an extra cost at inference time.On repeated translation. We compute the ngram repeat rate (nrr, what percentage of n-grams are repeated by certain nearby n-grams) of different systems on WMT'14 EN-DE test set and the result is shown in . The nrr of CMLM with one iteration is much higher than other systems, showing that it suffers from a severe repeated translation problem. On the other hand, LAT can mitigate this problem thanks to the merging algorithm.On sentence length. We explore how various systems perform on sentences with various lengths. The WMT'14 EN-DE test set is split into 5 length buckets by target length.  show that LAT performs better than CMLM on longer sentences, which indicates the effectiveness of our methods at capturing certain target dependencies.  begin to explore nonautoregressive translation, the aim of which is to generate sequences in parallel. In order to mitigate multimodality issue, recent work mainly tries to narrow the gap between NAT and AT.  design a NAT model using CTC loss.  uses iteration decoding to refine translation. The conditional masked language model (CMLM)  predicts partial target tokens based on the source text and partially masked target sentence.  employs normalizing flows as the the latent variable to produce sequences.  designs an efficient approximation for CRF for NAT. Besides that, there are some works trying to improving the decoding speed of the autoregressive models. For example,  propose a semi-autoregressive translation model, which adopts locally non-autoregressive, but autoregressive decoding. And works mentioned in  use techniques such as knowledge distillation, block-sparse regularization to improve the decoding speed of autoregressive models. ConclusionIn this work, we incorporate a novel local autoregressive translation mechanism (LAT) into nonautoregressive translation, predicting multiple short sequences of tokens in parallel. With a simple and efficient merging algorithm, we integrate LAT into the conditional masked language model  and similarly adopt iterative decoding. We show that our method could achieve similar results to CMLM with less decoding iterations, which brings a 2.5x speedup. Moreover, analysis shows that LAT can reduce repeated translations and perform better at longer sentences. Appendices A PreprocessingWe follow the standard pre-processing procedure in prior works . All datasets are segmented into subwords through byte pair encoding (BPE) . The BPE code is learnt from the combination of source and target data for WMT datasets. For IWSLT, the bpe code is learned from the source and target data separately. B OptimizationWe sample weights from N (0, 0.02), initialize biases to zero, and set layer normalization parameters to \u03b2 = 0, \u03b3 = 1. For regularization, we use 0.3 dropout, 0.01 L2 weight decay, and smoothed cross-entropy loss with = 0.1. We train batches of 128k tokens using Adam (Kingma and Ba, 2015) with \u03b2 = (0.9, 0.999) and = 10 \u22126 . The learning rate warms up to a peak of 5 \u00d7 10 \u22124 within 10,000 steps, and then decays with the inverse square-root schedule. We train our models for 300k steps with batch size 128k  for WMT datasets. For the IWSLT dataset, we train our models for 50k steps with batch size 32k. C Model Parameter SizeThe averaged size of parameters for all models are shown in . These three kinds of models have similar number of parameters. LAT models have the most number of parameters due to the LSTM-based local translator. D Validation PerformanceThe performance of different models on translation tasks' validation sets is reported in the . We could find the similar trend to the performance on the test set. E Length Adjustment for Intermediate IterationsSince our merging algorithm produces the output dynamically, the output length is usually not the same as the number of input pieces. In iterative decoding, we find it helpful to adjust the output sequence's length to the input length in intermediate iterations. This is achieved by adding or deleting the special mask symbols. Notice that for the final iteration, we do not apply any adjustments and keep the merged output sequence as it is. For the length adjustment in the intermediate iterations, our goal is to adjust the output length of the merger (L out ) to be close to the input target length (L in ). If these two lengths are already equal or their relative difference is within a certain range (which is empirically set to 5%), we will do nothing. Otherwise, there can be two cases: 1) when L in is larger than L out , we further insert L in \u2212 L out mask tokens into the sequence; 2) otherwise, we try to delete L out \u2212L in mask tokens. Notice that the addition or deletion operations happen after the masking procedure for the next iteration.Here, we describe the addition case in detail. Suppose we need to further insert M masks into the output sequence, we decide the insertion places according to the position gaps. We adopt a simple position scheme for all the tokens. For each original token t j i (the j-th token in the i-th piece) in the input translation pieces, we set i + j as its position. For each token in the output sequence after merging, since it can originate from multiple input tokens through aligning, we take the averaged value of all its source input tokens' positions. We calculate the position gap between each pair of nearby unmasked tokens in the output sequence and maintain a priority queue for all these gaps. Then we insert M masks once at a time. For each time, we select the current maximal gap, insert a mask to that position, and subtract that gap by 1. The case for deletion would be similar but in the opposite direction: select the minimal gap, delete one mask if there are any, and increase that gap by 1. We will delete nothing if there are no masked tokens in the selected gap.\n###\n"}
{"summary": "has benchmark: Benchmark WikiText-103/Benchmark WikiText-103/Benchmark Penn Treebank (Character Level)/Benchmark enwik8\nhas research problem: Language Modelling/Language Modelling/Language Modelling\nhas model: Feedback Transformer (4 layers)/Feedback Transformer (8 layers)/Feedback Transformer", "text": "#Properties\nhas benchmark, has research problem, has model\n#Text\nTransformers have been successfully applied to sequential, auto-regressive tasks despite being feedforward networks. Unlike recurrent neural networks, Transformers use attention to capture temporal relations while processing input tokens in parallel. While this parallelization makes them computationally efficient, it restricts the model from fully exploiting the sequential nature of the input. The representation at a given layer can only access representations from lower layers, rather than the higher level representations already available. In this work, we propose the Feedback Transformer architecture that exposes all previous representations to all future representations, meaning the lowest representation of the current timestep is formed from the highest-level abstract representation of the past. We demonstrate on a variety of benchmarks in language modeling, machine translation, and reinforcement learning that the increased representation capacity can create small, shallow models with much stronger performance than comparable Transformers. METHODIn this section, we propose the Feedback Transformer, which provides capacity to build richer representations of each timestep t of a sequential modeling task. TRANSFORMER ARCHITECTURESWe briefly describe the Transformer . Each layer is composed of a multihead self-attention sublayer (Attn) followed by a feedforward sublayer (FF), and each sublayer is followed by an add-norm operation that combines a skip-connection  and layer normalization . The l-th layer of a Transformer processes an input sequence of vectorsinto a sequence of vectors of the same length. First, the self-attention sublayer computes a representation for each time step t by taking its related input vector x t along with its past context, {x l t\u2212\u03c4 , ..., x l t\u22121 }:t is used to form query vectors while its context is used to compute key and value vectors, forming a memory of the past information. Then the feedforward sublayer processes each vector z l t independently, i.e., x l+1 t = FF(z l t ). The Transformer layer transforms its input sequence into an output sequence X l+1 = FF(Attn(X l )).In practice, a block of steps {x l t\u2212M +1 , . . . , x l t } is computed in parallel during training, where M can be seen as the backpropagation through time (BPTT) length. This makes training Transformers efficient on hardware such as GPUs. However, to operate on sequences of unbounded length, Transformers require modifications such as caching and relative position embeddings . LIMITATIONS OF TRANSFORMERSPrevious work has analyzed the impact of several limitations of the Transformer architecture, such as the inability to track long sequences and process hierarchical inputs . In this work, we focus on two major limitations of Transformer architectures.Limited Access to Higher Level Representations. Layer by layer, Transformers build more abstract, high level representations of the input sequence. At each layer, the representations for the input sequence are treated in parallel. As a consequence, a Transformer does not leverage the highest level representations from the past to compute the current representation, even though these highest level representations have already been computed for autoregressive models.Maintaining a Belief State. Many sequential tasks require models to maintain an internal state for two main purposes. First, internal states act as memory for recalling past inputs, where Transformers excel because their internal state x l t is directly accessible to future steps through self-attention.The second role of an internal state is to act as a belief state that tracks the world state that is not directly observable in inputs. For example, when inputs are actions taken on a Markov Decision Process, an internal state can apply those changes to the current belief state and correctly predict the outcome. As a feedforward model, Transformer have inherent limitations in this area -only a fixed number of transformations can be applied to its internal states. Since both Attn and FF sublayers contain a fixed number of transformations and there are L layers of them, the total number of transformations between the input and output is limited by the depth. This means Transformers cannot maintain an internal state for long time if it has to be frequently updated. FEEDBACK TRANSFORMERWe propose to change the Transformer architecture by using the most abstract representations from the past directly as inputs for the current timestep. This means that the model does not form its representation in parallel, but sequentially token by token. More precisely, we replace the context inputs to attention modules with memory vectors that are computed over the past, i.e.,, where memory vectors m t are computed by summing the representations of all layers at time step t:where w l are learnable scalar parameters. Note these scalars are the only new parameters introduced by our change, with all else the same as the standard Transformer. Here l = 0 corresponds to token embeddings. The weighting of different layers by a softmax output gives the model more flexibility as it can average them or select one of them.This modification of the self-attention input adapts the computation of the Transformer from parallel to sequential, summarized in . Indeed, it provides the ability to formulate the representation x l t+1 based on past representations from any layer l , while in a standard Transformer this is only true for l < l. This change can be viewed as exposing all previous computations to all future computations, providing better representations of the input. Such capacity would allow much shallower models to capture the same level of abstraction as a deeper architecture. This has several practical advantages, as more shallow models have reduced memory footprint and increased decoding speed.An alternative view of such an architecture modification is providing the capacity for recursive computation -outputs from a sublayer can feed back to the same sublayer through the memory. The model can then maintain an internal state for unbounded time. This is a clear advantage over Transformers, in which a submodule never looks at its own output. While an RNN can also repeat its computation on its internal state, its internal state has a limited capacity determined by the number of layers and their hidden dimension. In contrast, the internal state of a Feedback Transformer is its whole memory, which can grow with the input length. This allows the model to keep track of a large number of things within its internal state.While our modification requires sequential computation, we significantly improve training speed by sharing the key and value projections W l k and W l v across all layers. This sharing reduces computation because we need to compute key and value vectors only once instead of computing them per layerFor the same reason, the memory footprint is smaller than a standard Transformer because only one set of k t , v t needs to be stored. To be more precise, the memory requirement for processing a single token is reduced from O(L \u00d7 T ) to O(T ), where L is the number of layers and T is the context size. Further, the reduced memory usage allows the batch size to be increased to recover some of the lost parallelism, which improves training speed. Thus, the Feedback Transformer is not much slower compared to the standard Transformer. Note that the same sharing of projections will not make the standard Transformer efficient because those projections are applied to different representations at each layer (the key and value vectors will not the same for all layers).Lastly, we note that the sequential nature of the Feedback Transformer does not affect the performance during generation where one needs to compute one step at a time anyway. The same is true for online reinforcement learning where the input must be processed sequentially even during training.  : Results on the Corridor task. The Transformer degrades as the memory size decreases, but the Feedback Transformer maintains performance. EXPERIMENTSWe explore different sequential input tasks in natural language processing and reinforcement learning. First, we demonstrate the downsides of the standard Transformer architecture on tasks where the Transformer performs poorly. We show that the Feedback Transformer is able to overcome challenges and retain long memory. Next, we highlight the strength of the Feedback architecture in building complex, high level representations even with shallow models. We demonstrate that the Feedback model can achieve significantly stronger results than Transformer models, an effect that is exaggerated as models get smaller. Finally, we compare the Feedback architecture to the Transformer architecture with other work on standard long-context language modeling tasks. In experiments on large datasets, we use the shared key-value projections to improve training time. Additional experimental details and results can be found in the appendix. LIMITATIONS OF TRANSFORMER: ILLUSTRATIVE TASKS LIMITED ACCESS TO LONG MEMORYFirst, we examine the Transformer's limited access to long memory on several simple, straightforward tasks that illustrate this. Unlike the standard Transformer, the Feedback architecture is able to remember information over many timesteps.Walking down a Corridor. In this reinforcement learning task, each agent is placed at the start of a long corridor with either a blue or green object. The agent must look at the object's color, walk down the corridor, and go through the corresponding colored door at the end. The only task is to remember the color and not become distracted by walking down the very long hallway. Results are shown in  and show that the performance of the Transformer degrades quickly as the memory size shrinks, but the Feedback Transformer maintains strong performance at all memory sizes.Copy and Reverse. We experiment next on two algorithmic tasks, copy and reverse . We train on sequences of length 40 consisting of integers 0 through 9, and test on sequences of length 400. Models read the input and then either copy or reverse, which requires memory over the sequence and the ability to track position, as well as generalization capability as the train and test settings are different lengths. We consider two variations of copying and reversing: either at the character level or at the sequence level. Results are shown in . The Feedback architecture has large improvements in accuracy, indicating improved memory and positional tracking.Counting. Finally, we experiment on a counting task, where models have a sequence of A's in a row, and must output the corresponding quantity of the letter B. The model must count the number of the A's to output the correct number of B's. We consider two settings: training on short sequences of lengths up to 50 and training on long sequences of lengths up to 1000. We show results in , where we demonstrate the Feedback model is much better at counting over long sequences. LIMITED STATE UPDATESThe complexity of the representations the Transformer is able to formulate is strictly dependent on the depth, as each layer of the Transformer allows for additional nonlinearity. The Transformer, then, can only update its state the same number of times as it has layers. We demonstrate that the Feedback Transformer does not have this limitation -in tasks where the model must carefully track and update its state, the Feedback architecture is able to update its state at each timestep.Random Walk. We consider a random walk in a small grid where actions are: go forward 1 step, left turn, and right turn. Given a history of actions and the agent's initial position, it is strictly possible to calculate the current position. The task is trivial because a human could write down the current location and direction and keep updating with each action. However, Transformers cannot do this because they lack a storage that can be updated with each input. Its hidden state can store this information, but with each update, that information has to go up one layer.An alternative approach to this task is to solve it all at once given a sequence of actions, which is feasible for Transformers since they can access all inputs with their attention. However, this approach is challenging because the effect of each action depends on the direction at that point and whether the agent is on the edges, which itself is not known yet. This can be seen in , where the Transformer struggles and only reaches 68% accuracy. In contrast, the Feedback Transformer achieves 100% accuracy, which indicates the ability to track state for a long period of time. Both models are trained on 10K sequences, each containing 100 random actions and positions.Algorithmic task. A more complex setting where tracking and updating of a state is crucial is code executions. A model needs keep track of all variable values and update them if necessary. To demonstrate this, we create a simple algorithmic task that consists of the following simple statements: assignments (e.g. x=5), increments and decrements (e.g. y--), conditionals (e.g. if x==4: y++), and print commands (e.g. print(x)). Each task consists of 100 randomly selected statements. We consider two settings with 3 and 5 different variables.Processing of each statement in parallel will not work because conditional statements cannot be executed without knowing the current variable value, which itself can depend on another conditional. As shown , Transformers cannot solve this task because every time a variable increment or decrement, its value can only be found one layer up in the model, and eventually will be lost. Doubling their layers from 4 to 8 does help little, bringing the accuracy to 47.4% on the 3 variable version and 29.1% on the 5 variable version, but their performance is far from perfect. A recurrent model like LSTM is capable of storing a variable value while updating it, thus perform well on the 3 variables version with an accuracy of 82.8%. However, its performance drop to 32.1% when there are more variables because it has to store all their values in a single vector. The Feedback Transformer does not have this bottleneck, and can access updated variable values from the lowest layer, so it gives strong performance on this task. ADVANTAGES OF FEEDBACK ARCHITECTUREWe examined two limitations of standard Transformers that we improve upon: limited memory span and limited ability to update state. In the Feedback model, we improve on these limitations and now analyze performance on practical tasks including translation and reinforcement learning. STRONG PERFORMANCE WITH SMALL, SHALLOW MODELSThe Feedback Transformer is able to create higher level, more abstract representations with fewer layers and less capacity, as a layer can use all of the most recently created representations of previous timesteps. We demonstrate on neural machine translation that the Feedback model performs much better than Transformers at small, shallow sizes. Note that for sequence to sequence, we use Feedback Transformers only in the decoder because the encoder inputs are available simultaneously. We evaluate the performance of the Feedback Transformer on the WMT14 En-De machine translation benchmark of 4.5 million pairs. We follow  and train on WMT16 using newstest2013 as dev and newstest2014 as test. We learn 32K joint byte pair encodings , generate with beam size 5, tuning a length penalty on the dev set. We average the last 10 checkpoints and apply compound splitting and compute tokenized BLEU.In  (left), we display results when making the model shallower only -layers are removed from a Feedback Transformer decoder compared to Transformers. As the decoder becomes shallow, the gap in performance between the two architectures widens. While the 1-layer Transformer model can only reach 27.3, the Feedback Transformer has 28.3 BLEU. Shallow decoders are critical to fast inference -reducing to 1-layer improves decoding speed by 4.2x, while only losing 1 BLEU with the Feedback architecture. Such results are useful for practical applications, where the speed of producing a translation is very important. We report decoding speed in tokens per second on 1 GPU.We further experiment with large encoder but shallow decoders. The Feedback Transformer achieves 29.0 BLEU with 12 layer encoder and 2 layer decoder. As the encoder is parallelized even during inference, the increased size of the encoder has negligible impact on decoding speed. To stabilize the training of deeper models, we use LayerDrop . LONG MEMORY TRACKS STATEWe apply Feedback to a reinforcement learning maze task that requires long memory to optimally solve because agents have limited vision. Note that in such reinforcement learning tasks, the models are trained online using A2C, so the input must be processed sequentially even during training time. Thus, the non-parallelized nature of the Feedback Transformer is not a drawback, and training Feedback Transformers is as fast as Transformers.The goal is to navigate a procedurally generated random maze where colored objects are placed. One of the colors will be randomly selected as a target, and the agent has to reach it for a reward and a new target. For optimal performance, the agent must remember the maze and object locations. In addition, the agent has turn actions like the Random Walk task, which makes it necessary to keep track of its location and orientation. As shown in , the Feedback Transformer converges to reach higher average reward, compared to Transformers. Results are shown averaged over 10 trials. COMPARISON TO OTHER ARCHITECTURESIn this section, we first, we compare Feedback to recurrent architectures such as LSTM, as well as hybrid RNN-Transformer architectures, and show that the Feedback is more powerful than recurrence alone. Next, we compare our construction of the Feedback Memory with other possible compositions. Lastly, we compare to other Transformer architectures on competitive benchmarks. Model Test Recurrent ArchitecturesDenseNMT  25.5 RNMT+  28.5 Hybrid ArchitecturesBiARN  28.9 SRU  28.4 Transformer ArchitecturesTransformer 28.4 Transformer  29.3 Feedback Transformer 29.5 COMPARISON TO RECURRENT ARCHITECTURESWe compare the Feedback Transformer architecture to recurrent architectures like LSTMs as well as hybrid RNN-Transformer architectures. In , we display that the Feedback Transformer has stronger performance than the Transformer, RNN, and RNN-Transformer hybrid model. We note that recurrent models address some limitations of Transformer architectures, but the Feedback mechanism goes beyond that. By allowing all past representations to be immediately available for the computation of future representations, Feedback is stronger than Recurrence alone -Recurrent models can only see representations from the previous layer (as depicted in ). MEMORY COMPOSITIONWe next investigate the importance of the specific memory mechanism of the Feedback architecture on char-PTB. The Feedback architecture uses all layers when creating the memory, motivated by providing access to the entire past of all computations, but other ways of creating the memory as possible. For example, Recurrent architectures have a different memory structure. In multi-layer RNNs, each layer has recurrent connections to the same layer, but not to higher layers. This is an advantage of Feedback architectures -even the highest level abstractions are immediately available.In , we examine the construction of the Feedback memory, comparing our choice of making all computation accessible with recurrent memory that can access all previous layers plus the same layer, and top-only memory that can attend only to the topmost layer. The Feedback Transformer has the best performance, closely matched by top-only memory. This indicates the importance of high level representations (see Appendix 6.4 for further analysis on this). Note that recurrence alone is not enough for good performance, and thus the Feedback memory provides richer representations beyond the capacity of recurrent networks. COMPARISON TO OTHER TRANSFORMER ARCHITECTURESWe examine the performance of Feedback Transformer on long context language modeling benchmarks. We use caching  and relative position embeddings. Mechanisms applied at inference time  can further improve all models, so we do not focus on these.Wikitext-103. We evaluate on word-level language modeling on Wikitext-103 . Our Feedback architecture takes 3.5 days to train, compared to the Transformer which takes 1.2 days. We train a small Feedback model, about half the size of Transformer-XL, and find that it can match the performance of Transformer-XL, as shown in . This indicates the additional representational capacity of Feedback memory. If we train a standard Transformer that is approximately the same size as our Feedback Transformer, we find it has worse performance Model Params TestBest Existing  -15.8 Trans-XL  257M 18.3Our Transformer 140M 19.9 Feedback Transformer 126M 18.3 : Results on WikiText-103. We report perplexity on test. Model Params TestBest Existing  277M 0.97 Trans-XL  277M 0.99 Feedback Transformer 77M 0.96  (19.9 PPL rather than 18.3). Further, mechanisms like the Routing Transformer can be added to the Feedback Transformer as well. We focus on starting with Transformer-XL as a baseline and showing we can match the performance with a much smaller model. Enwiki8. Finally, we test our model on character-level language modeling in Enwiki8 , containing 100M unprocessed bytes from Wikipedia. We train a relatively small 12-layer model, that is one third of the size of the Transformer-XL baseline. Since the task requires very long context, we use adaptive attention span . As shown in , the Feedback Transformer model achieves a new SOTA performance of 0.96 bit-per-byte despite its small size. TRAINING AND INFERENCE SPEEDFinally, we compare the training and inference speed of the Feedback Transformer with standard Transformer architectures. Results are shown in . The Feedback Transformer has faster inference, because the key-value projection sharing substantially reduces the memory footprint of the model and reduces computation. Further, shallow Feedback models perform well, so the batch size can be increased. In language modeling, for example, sharing key-value provides almost 3X inference speed improvement. The shallow model size provides the remaining 10% of speed improvement at inference time. Finally, note that for certain problems (such as in RL), the data must be processed strictly sequentially anyway and Feedback Transformer is not any slower. CONCLUSIONWe propose a novel reformulation of the Transformer that fully exploits sequential input -the increased representation power and recursive computation of the Feedback Transformer allows shallow and small models to have much stronger performance compared to a Transformer of the same size. This architecture addresses two fundamental limitations of Transformers as an autoregressive model -limited access to long memory and limited ability to update state. We demonstrate on a variety of tasks the advantages of the Feedback architecture to illustrate the strong performance of this straightforward modification. ADDITIONAL RESULTS REINFORCEMENT LEARNINGMaze Navigation Easy. We experiment with a slightly different version of the Maze Navigation task. Instead of an agent with forward, turn-left and turn-right actions, the agent has no orientation and there are only 4 movement actions corresponding to 4 cardinal directions. This makes navigation easier because the agent do not need to keep track of its orientation. Further, it is much easier to compute relative locations given a history of actions. This might explain why standard Transformers are not far behind Feedback Transformers in performance as shown in  (left). We also compare to LSTMs, which performs much worse. See Section 7.2 for more implementation details.Water Maze. We modify the Morris Water Maze task  to make it more challenging. The maze is defined by a goal position and a mapping of cell to ID -these remain fixed within an episode but change between episodes. The agent receives as an observation the cell IDs of its current location and the target cell. When the agent finds the target, it receives +1 reward and is randomly teleported. During the same episode, if the agent reaches a previously seen cell, it needs to remember how it reached the target from there to go back. Results are shown averaged over 10 trials (the reward is reported averaged over the last 500 episodes for each trial). As shown in  (right), the Feedback Transformer converges to higher average reward. IWSLT DE-ENWe additionally evaluate the Feedback Transformer on IWSLT De-En, a small machine translation dataset. We train a small Transformer model with 6 layers. For generation, we use beam size 5 without checkpoint averaging. Model quality is evaluated using tokenized BLEU. Results are shown in  (left) and show that for shallower models, the Feedback Transformer has better performance than the standard Transformer. SUMMARIZATION ON CNN-DAILYMAILWe evaluate on the CNN-Dailymail multi-sentence summarization benchmark of 280K news articles Hermann et al. (2015), modeling the first 400 words of the article . We evaluate using . and use 3-gram blocking and tune length .  (right) displays the performance of the Feedback Transformer as the decoder layers are reduced, making the model shallower only. For all model depths, the Feedback architecture maintains a consistent improvement in ROUGE compared to the standard Transformer. Compared to sentencelevel tasks such as translation, this summarization benchmark requires multi-sentence generation, and the increased capacity of the Feedback architecture is beneficial.  : Ablation results on char-PTB: instead of a weighted sum of all layers as Feedback memory, only a single layer is used as memory for all layers. We also include a setting where the average of all layers is used. ABLATION STUDIES ON LANGUAGE MODELSWe investigate which layer of a model has the best representation to be used as a Feedback memory. In Feedback Transformers, a weighted sum of all layers is used as the memory, and feeds to all layers. An alternative approach is to manually select one of the layers as the memory and let all layers attend to it. In , we explore this approach, using the same 6-layer char-PTB models as Section 4.3.2 (top-only memory there corresponds to using the last 6th layer as memory). We can see that representations from higher layers work better as memory, confirming our assumption of the importance of higher level representations. Simply averaging all layers together works reasonably well as well. Interestingly, when all layer attend to the first layer output, it works as good as the standard Transformer. The weighted sum approach matches the best performance because it can adopt to select any of the layers.Here we study how different techniques affect the model performance on WikiText-103. The results shown in  indicate:\u2022 Pre-normalization combined with higher learning rates helps the performance, particularly for the standard Transformer. \u2022 Increasing the context size with adaptive span further improves the performance for both models. \u2022 The technique of increasing the BPTT length during training for efficiency does not affect the final performance. \u2022 The gap between two model is consistent along those variations.Next, we examine the effect of the model depth on performance on char-PTB and WikiText-103 This time, we keep the total number of parameters constant and only vary the number of layers to  isolate the effect of depth. This is achieved by proportionally increasing the head dimension and the ReLU layer size when we decrease the number of layers. The results in  demonstrate that for the standard Transformer improves as the depth increase. In contrast, the Feedback architecture is much robust reduced depth, even achieving the best performance on char-PTB with only two layers.7 ADDITIONAL IMPLEMENTATION DETAILS RANDOM WALK TASK DETAILSWe provide additional details for the random walk toy task we explore. The agent starts at a fixed position of a 8 \u00d7 8 grid. Available actions are 1) move one step forward, 2) turn left and 3) turn right. At every time step, the agent randomly picks on of the three actions and executes it. An action would be ignored if it can't be executed like going out of the grid. After 100 actions, the agent is reset back to the initial position.The input to the model is a sequence of actions taken by the agent, and a special symbol if there was a reset. The output is a sequence of location symbols corresponding to the agent's location after each action. We generate 10k training episodes, totalling 1M tokens.We use the same setup as our language modeling experiments, except now the model predicts separate output tokens rather than a next token. We concatenate all the episodes and feed them to the model as a single sequence. The training is done with the negative-log-likelihood loss. See  for the hyperparameters used in the experiment. The attention span is set to 100, so that the models can attend to all the information they needs to solve the task.  : An example program from the algorithmic task with 3 variables. MAZE NAVIGATION DETAILSWe generate random 9 \u00d7 9 mazes using Kruskal's algorithm. Dead ends are eliminated by randomly removing one of the blocks surrounding them. We randomly place 8 target objects with different colors as shown in  (left). The agent is given a randomly selected color as a target. If the agent manages to reach the correct target, it gets a reward of +1 and a new target color is sampled. An episode ends after 200 steps. The observation includes the 3 \u00d7 3 area around the agent and target color.We train 2-layer Transformers with a hidden size 256 and 4 heads. We set the BPTT to 100 and the batch size to 1024. The reward discount rate is 0.99. The attention span is 200 so the agent can keep an entire episode in memory. All agents were trained using A2C with Adam with a learning rate of 0.0003 and a entropy cost of 0.0005. For the easy version of the task, we use RMSprop with a batch size of 128 and a learning rate of 0.0003. The RMSProp epsilon regularization parameter is set to 0.01 The LSTM model is a 3-layer LSTM with a hidden size of 256. WATER MAZE DETAILSThe water maze task we designed is depicted visually in . The grid size is 15 \u00d7 15.To help exploration, the agent can see if the goal is within a 3 \u00d7 3 area around it. An episode ends after 200 steps. We train for 500M steps (2.5M episodes). We use 2-layer Transformers with hidden size of 64 and 1 head. The attention span is 200 so the agent can put an entire episode in memory.All agents where trained using A2C with RMSprop with entropy cost of 0.0001, RMSProp epsilon regularisation parameter of 0.01, batch size of 64, and BPTT 200. Feedback Transformer and Transformer baseline were trained with a learning rate of 0.0003. LSTM model is a 2-layer LSTM with hidden size of 64. For LSTM model we used a learning rate of 0.0004. ALGORITHMIC TASK DETAILSIn this task, each program consists of 100 simple statements that should be sequentially executed.   3. Print. Output the value of a certain variable like print(y). Only this statement requires model to make a prediction. 4.Conditional. Execute the nested statement only if a variable has a certain value, e.g., if x==4: y--. Note that conditional and print statements cannot be nested.A program is generated by randomly choosing a statement one after another, but with the following conditions: a variable must be initialized before being used, and a variable value have to between 1 and 10. The training data contains 10k such programs concatenated with a special separator keyword. We generate two version the data with 3 and 5 different variables in them. An example program is shown in . We used the same hyperparameters as the random walk task as show in . MACHINE TRANSLATION AND SUMMARIZATIONWe detail the hyperparameters in . Summarization experiments are done with the Transformer base architecture size and WMT En-De experiments are done with the Transformer big architecture size. As IWSLT De-En is a smaller dataset, we use a smaller model. For all sequence to sequence experiments, only the decoder is modified to have the Feedback Transformer architecture. LANGUAGE MODELINGIn the language modeling experiments, we added several improvements on top of the original Transformer  to better adapt to unbounded sequences:\u2022 Hidden representation caching : Since the input to the model is an unbounded sequence and the model needs to process it in small blocks, hidden representations from previous blocks are kept in cache so that any token in the current block will the same context length regardless of its position in the block. \u2022 Relative position embedding : Relative position embeddings allow each token in a block to be processed in the same way regardless of its absolute position in the block. We found that adding shared embeddings to key vectors at every layer to be effective. \u2022 Adaptive attention span  Language modeling requires a model to have a very long attention span, which is computationally expensive. The adaptive span mechanism allows each attention head to learn different attention spans for efficiency. \u2022 Pre-normalization : We observed that pre-normalization makes training more stable for Transformers, which allowed us to use larger batch sizes for better parallelization.Dropouts are applied to attention and ReLU activations. In WikiText-103 models, additional dropouts are added to the embedding layer output and the last sublayer output.In , we present the hyperparameter values used for our experiments. We use the same hyperparameters for both Transformers and Feedback Transformers, and optimize them with Adam. The final performances are obtained by finetuning the models with a 10x smaller learning rate.Details on the char-PTB experiments We trained the models for 15k updates (or earlier if the validation loss stops decreasing), and funetined them for 1k steps. We varied the depth of the models while keeping the number of parameters constant. This is achieved by changing the FF size and the head dimension inverse proportionally to the depth.Details on the enwik8 experiments We used an adaptive span limited to 8192 tokens with a loss of 0.0000005. The training is done for 100k updates and another 10k steps is used for finetuning. The warming up BPTT length is used for speeding up the training, where the BPTT length is decreased to 64 for the first half of the training. Details for Training on WikiText-103We employed the adaptive input  and the adaptive softmax  techniques for reducing the number of parameters within word embeddings. The models are trained for 200k steps and the finetuned for additional 10k steps.While most of the models have a fixed attention span of 512, the best performance is achieved by extending the attention span to 2048 with adaptive span loss 0.00001.After training our models, we noticed that our tokenization method differed from others by omitting end-of-line (EOL) symbols. Since our dictionary already contained the EOL token, we were able finetune our trained models on the data with EOL tokens, rather than training them from scratch. This change alone brought about 1ppl improvement.\n###\n"}
{"summary": "contains: Results/Results/ProblemStatement/ProblemStatement/Methods/Methods/Evaluation Score {HAS_VALUE}/Discussion/Contribution/Contribution\nhas content: JarvisQA outperforms the other baselines in terms of precision, recall,and F1-score measure at the cost of higher execution time and memory requirements./we conclude that usual information retrieval techniques used in search engines are failing to find specific answers for questions posed by a user./Can a QA system retrieve answers from tabular representations ofscholarly knowledge?/What type of questions can be posed on tabular scholarly knowledge?/Tables have to be converted into coherent text snippets that represent the entirety of the information presented in the table/The component uses a pre-trainednatural language QA model. The model is a deep transformer, fine tuned onthe SQuADv2 dataset to perform the QA task./SinceJarvisQAutilizes a BERT based QA component, different componentscan perform differently, depending on the use case and scenario./We propose a system, calledJarvisQA, that answers Natural Language (NL)questions on tabular views of scholarly knowledge graphs, specifically tabularviews comprising research contribution information from scientific articles./Additionally, we created a set of questions that cover various types of infor-mation and facts that can be retrieved from those tables. The benchmark consistsof 80 questions in English.", "text": "#Properties\ncontains, has content\n#Text\nAnswering questions on scholarly knowledge comprising text and other artifacts is a vital part of any research life cycle. Querying scholarly knowledge and retrieving suitable answers is currently hardly possible due to the following primary reason: machine inactionable, ambiguous and unstructured content in publications. We present JarvisQA, a BERT based system to answer questions on tabular views of scholarly knowledge graphs. Such tables can be found in a variety of shapes in the scholarly literature (e.g., surveys, comparisons or results). Our system can retrieve direct answers to a variety of different questions asked on tabular data in articles. Furthermore, we present a preliminary dataset of related tables and a corresponding set of natural language questions. This dataset is used as a benchmark for our system and can be reused by others. Additionally, JarvisQA is evaluated on two datasets against other baselines and shows an improvement of two to three folds in performance compared to related methods. ApproachWe propose a system, called JarvisQA, that answers Natural Language (NL) questions on tabular views of scholarly knowledge graphs, specifically tabular views comprising research contribution information from scientific articles. Data and Questions CollectionIn order to evaluate our QA system we create the ORKG-QA benchmark, collected using the ORKG. The ORKG provides structured comparisons  of research contributions obtained from papers. The ORKG-QA benchmark comprises a dataset that integrates 13 tables, covering information spanning more than 100 academic publications. The data is collected through the ORKG API and the featured set of tables 5 , which can be exported in CSV format. The system has two main components. (a) Table2Text (T2T) component, which in turn has two functionalities: (1) to break the table into a set of triples < s, p, o > and (2) to compile the triples into an NL sentence. Component (b) is the engine of the QA system, where an NL QA (BERT based) system is employed to answer the input question using the text, by extracting features, finding candidate answers, and ranking them.Additionally, we created a set of questions that cover various types of information and facts that can be retrieved from those tables. The benchmark consists of 80 questions in English. The questions cover a variety of question types that can be asked in the context of tables in the scholarly literature. These types of questions include aggregation questions (e.g., min, average and most common), ask questions (i.e., true, false), answer listing questions, and questions that rely on combining information. In the ORKG-QA dataset  , 39% are normal questions addressing individual cells in tables, 20% are aggregation questions, 11% are questions for which the answer relates to other parts of the table, and the rest are questions of different types (i.e., listings, ask queries, empty answers).We also use the TabMCQ  QA dataset, specifically questions on the regents tables. TabMCQ was derived from multiple choice questions of 4th grade science exams and contains 39 tables and 3 745 related questions. While TabMCQ is not a scholarly dataset, but is to the best of our knowledge the closest one available. Since TabMCQ has only multiple-choice questions, we adapted the questions with only the correct choice. JarvisQA system architectureJarvisQA is designed with modularity in mind. Hence, the core QA components are replaceable with newer or more fine-tuned versions. TextPaper 1's semantic representation is \"ORKG\", its data type is \"Free Text\", and its scope is \"Summary\" ...architecture in more detail. Since we used a natural language QA system, we need a pre-processing step that transforms the table information into the textual description (representing only the information contained in the table not the entire raw text of the article). With the output of the \"Table2Text\" step and the input question, the NL QA system can reason over the question with the provided context (textual table description) and attempts to answer the question. We now discuss the individual components of the architecture in more detail.Table2Text (T2T) converter. Although JarvisQA operates on tabular data, the core QA engine processes textual contexts. To that end, tables have to be converted into coherent text snippets that represent the entirety of the information presented in the table. T2T component splits tables into its entries and converts entries into triples.  information about three publications, along with their triples and textual representations compiled by the T2T component. Furthermore, the T2T component enriches the textual description with aggregated information (i.e., max value of certain rows, most common value used within some columns). This enables the system to answer aggregation-type questions such as \"Which system has the maximum accuracy?\" and \"What is the most common method used among the papers?\".QA core engine. This component is the primary building block of JarvisQA.It is where reasoning over questions happens. The component uses a pre-trained natural language QA model. The model is a deep transformer, fine tuned on the SQuADv2 dataset to perform the QA task. The component is replaceable with any other similar transformer model (of different sizes and architectures).Our base implementation uses a fine tuned version of a BERT model and we evaluate our model using different model sizes and architectures. The model parameters are set: maximum sequence length to 512, document stride to 128, : Evaluation metrics used to experimentally benchmark JarvisQA against other baselines. Metric DefinitionGlobal Precision Ratio between correct answers retrieved in the top ranked position and the total number of questions. Global RecallRatio between the number of questions answered correctly at any position (here till the 10th retrieved answer) and the total number of questions. F1-ScoreHarmonic mean of global precision and global recall. Execution Time Elapsed time between asking a question and returning the answer. top k answers to 10, maximum answer length to 15, and the maximum question length to 64. As illustrated in , the QA engine extracts sets of features from the questions and the text (i.e., embeddings), then it finds a set of candidate answers and ranks them by confidence score. The benefits of such architecture are the flexibility in model choice, multilingualism, and reusability. Different transformer models can replace ours to support other languages, other datasets, and potentially other features. To accomplish these objectives, the system is built using the Transformers framework . Experimental StudyWe empirically study the behavior of JarvisQA in the context of scholarly tables against different baselines. The experimental setup consists of metrics and baselines.  lists the evaluation metrics for the performance measurements of the systems. Since a QA system can produce multiple answers and the correct answer can be any of the retrieved answers we use a metric that takes the position of the answer into account. As baselines we use the following two methods for answer generation:-Random: the answer is selected from all choices randomly.-Lucene 8 : is a platform for indexing, retrieving unstructured information, and used as a search engine. We index the triple-generated sentences by Lucene. For each question, the top answer produced by Lucene is regarded as the final answer.The evaluation was performed on an Ubuntu 18.04 machine with 128GB RAM and a 12 core Xeon processor. The implementation is mostly based on HuggingFace Transformers 9 , and is written in Python 3.7. The evaluation results : JarvisQA performance on the ORKG-QA benchmark dataset of tabular data. The evaluation metrics are precision, recall, and F1-score at k position. JarvisQA is compared against two baselines on the overall dataset and specific question types. The symbol (-) indicates that the performance metric showed no difference than the reported value for higher K values. The results suggest that JarvisQA outperforms the baselines by 2-3 folds. for precision, recall, and F1-score are reproducible while other metrics such as time and memory depend on the evaluation system hardware. However, the ratio of the difference between the baselines should be similar or at least show a similar trend. The code to reproduce the evaluation results and the presented results are available online. 10Experiment 1 -JarvisQA performance on the ORKG-QA benchmark.In order to evaluate the performance of JarvisQA, we run the system and other baselines on the ORKG-QA dataset at various k values (k denotes the position of the correct answer among all retrieved answers). For this experiment we evaluate k \u2208 {1, 3, 5, 10}. Moreover, the experiment was conducted on a specific subset of questions (based on types) to show the performance of the system for certain categories of questions. The tested question categories are: Normal : normal questions about a specific cell in the table with a direct answer; Aggregation: questions about aggregation tasks on top of the table; Related : questions that require retrieving the answer from another cell in the table; Similar : questions that address the table using similar properties (e.g., synonyms).  shows the performance of the baselines and our system on the ORKG-QA benchmark.The results show that JarvisQA performs better by 2-3 folds against Lucene, and Random baselines respectively. Experiment 2 -Different models of QA and their performance. We evaluate different types of QA models simultaneously to show the difference in performance metrics, execution time, and resource usage.  illustrates the : Performance comparison of different deep learning models on the task of question answering with different model sizes and architectures using the ORKG-QA benchmark dataset. The results suggest that different models perform differently on various question types, and generally the bigger the model the better it performs. For each question type, the best results are highlighted. Questions typePrecision @K Recall @K F1-Score @K difference in performance on the ORKG-QA benchmark dataset for different classes of questions and the overall dataset. JarvisQA's QA engine employs the BERT L/U/S2 model due to its execution time and overall higher accuracy at higher positions.Experiment 3 -Trade-offs between different performance metrics. We illustrate trade-offs between different dimensions of performance metrics for the JarvisQA approach compared to the baselines. We choose global precision, global recall, F1-score, in-memory size, and execution time as five different dimensions.  depicts the performance metrics trade-offs between our system and other baselines. JarvisQA achieves higher precision and recall while consuming considerably more time and memory than the other baselines.Experiment 4 -Performance on TabMCQ. We also show the performance of our system on the TabMCQ dataset against the ORKG-QA dataset. We see : Performance of the JarvisQA system. JarvisQA and the baselines are compared in terms of Global Precision, Global Recall, Global F1-Score, Inv.Time, Inv.Memory; higher values are better. JarvisQA improves Precision, Recall, and F1-Score by up to three times at the cost of execution time and memory consumption. System Dataset Precision @K Recall @K F1-Score @K #1 #3 #5 #10 #1 #3 #5 #10 #1 #3 #5 #10 the same trend in both datasets, that JarvisQA outperforms the baselines by many folds. TabMCQ is not directly related to scholarly knowledge. However, it shows that JarvisQA can generalize to related data and can answer questions about it.  presents the results of this experiment. Random ConclusionRetrieving answers from scientific literature is a complicated task. Manually answering questions on scholarly data is cumbersome, time consuming. Thus, an automatic method of answering questions posed on scientific content is needed. JarvisQA is a question answering system addressing scholarly data that is encoded in tables or sub-graphs representing table content. It can answer several types of questions on table content. Furthermore, our ORKG-QA benchmark is a starting point to collaborate on adding more data to better train, evaluate, and test QA systems designed for tabular views of scholarly knowledge. To conclude,JarvisQA addresses several open questions in current information retrieval in the scholarly communication domain, and contributes towards improved information retrieval on scholarly knowledge. It can help researchers, librarians, and ordinary users to inquire for answers with higher accuracy than traditional information retrieval methods.\n###\n"}
{"text": "#Properties\nimplementation, metric, has research problem, evaluation, Approach type, Document type, Summary usage, Summary characteristics\n#Text\nWe present a methodology for summarization of news about current events in the form of briefings that include appropriate background (historical) information. The system that we developed, SUMMONS, uses the output of systems developed for the DARPA Message Understanding Conferences to generate summaries of multiple documents on the same or related events, presenting similarities and differences, contradictions, and generalizations among sources of information. We describe the various components of the system, showing how information from multiple articles is combined, organized into a paragraph, and finally, realized as English sentences. A feature of our work is the extraction of descriptions of entities such as people and places for reuse to enhance a briefing. Summarization of Multiple ArticlesWith a few exceptions (cf. Section 2), all existing summarizers provide summaries of single articles by extracting sentences from them. If such systems were applied to a series of articles, they might be able to extract sentences that have words in common with the other articles, but they would be unable to indicate how sentences that were extracted from different articles were similar. Moreover, they would certainly not be able to indicate significant differences between articles. In contrast, our work focuses on processing of information from multiple sources to highlight agreements and contradictions as part of the summary. Summarization from Multiple SourcesGiven the omnipresence of on-line news services, one can expect that any interesting news event will be covered by several, if not most of them. If different sources present the same information, the user clearly only needs to have access to one of them. Practicall~ this assumption doesn't hold, as different sources provide updates from a different perspective and at different times. An intelligent summarizer's task, therefore, is to attain as much information from the multiple sources as possible, combine it, and present it in a concise form to the user. For example, if two sources of information report a different number of casualties in a particular incident, SUMMONS will report the contradiction and attribute the contradictory information to its sources, rather than select one of the contradictory pieces without the other. Symbolic Summarization through Text Understanding and GenerationAn inherent problem to summarizers based on sentence extraction is the lack of discourse-level fluency in the output. The extracted sentences fit together only in the case they are adjacent in the source document. Because SUMMONS uses language generation techniques to determine the content and wording of the summary based on information extracted from input articles, it has all necessary information to produce a fluent surface summary. Automatic Acquisition of Lexical Resources for GenerationWe show how the summary generated using symbolic techniques can be enhanced so that it includes descriptions of entities (such as people, places, or organizations) it contains. If a user tunes in to news on a given event several days after the first report, references to and descriptions of the event, people, and organizations involved may not be adequate. We collect such descriptions from on-line sources of past news and represent them using our generation formalism for reuse in later generation of summaries. Structure of the paperThe following section positions our research in the context of prior work in the area. Section 3 describes the system architecture that we have developed for the summarization task. The next two sections describe in more detail how a base summary is generated from multiple source articles and how the base summary is extended using descriptions extracted from on-line sources. Section 6 describes the current status of our system. We conclude this article in Sections 7 and 8 by describing some directions for future work in symbolic summarization of heterogeneous sources. Summarization through Sentence ExtractionTo allow summarization in arbitrary domains, researchers have traditionally applied statistical techniques . This approach can be better termed extraction rather than summarization, since it attempts to identify and extract key sentences from an article using statistical techniques that locate important phrases using various statistical measures. This has been successful in different domains and is, in fact, the approach used in recent commercial summarizers (Apple , Microsoft, and inXight). report that statistical summaries of individual news articles were rated lower by evaluators than summaries formed by simply using the lead sentence or two from the article. This follows the principle of the \"inverted pyramid\" in news writing, which puts the most salient information in the beginning of the article and leaves elaborations for later paragraphs, allowing editors to cut from the end of the text without compromising the readability of the remaining text. also notes that problems for this approach center around the fluency of the resulting summary. For example, extracted sentences may accidentally include pronouns that have no previous reference in the extracted text or, in the case of extracting several sentences, may result in incoherent text when the extracted sentences are not consecutive in the original text and do not naturally follow one another. Paice describes techniques for modifying the extracted text to replace unresolved references. Summaries that consist of sentences plucked from texts have been shown to be useful indicators of content, but they are often judged to be highly unreadable .A more recent approach uses a corpus of articles with summaries to train a statistical summarization system. During training, the system uses abstracts of existing articles to identify the features of sentences that are typically included in abstracts. In order to avoid problems noted by Paice, the system produces an itemized list of sentences from the article thus eliminating the implication that these sentences function together coherently as a full paragraph. As with the other statistical approaches, this work is aimed at summarization of single articles.Work presented at the 1997 ACL Workshop on Intelligent Scalable Text Summarization primarily focused on the use of sentence extraction. Alternatives to the use of frequency of key phrases included the identification and representation of lexical chains to find the major themes of an article followed by the extraction of one or two sentences per chain , training over the position of summary sentences\n###\n", "summary": " implementation: SUMMONS\n, has research problem: Automatic text summarization\n, Approach type: Algebraic\n, Document type: Multiple documents\n, Summary characteristics: Abstractive/Extractive\n###"}
