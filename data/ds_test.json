{"text": "#Properties\nHas value, Global Mean Sea level Rise Projection, has lower limit for likely range, has upper limit for likely range, has  start of period, has end of period, climate scenario, has research problem, has unit\n#Text\nMechanisms such as ice-shelf hydrofracturing and ice-cliff collapse may rapidly increase discharge from marine-based ice sheets. Here, we link a probabilistic framework for sea-level projections to a small ensemble of Antarctic ice-sheet (AIS) simulations incorporating these physical processes to explore their influence on global-mean sea-level (GMSL) and relative sea-level (RSL). We compare the new projections to past results using expert assessment and structured expert elicitation about AIS changes. Under high greenhouse gas emissions (Representative Concentration Pathway [RCP] 8.5), median projected 21st century GMSL rise increases from 79 to 146 cm. Without protective measures, revised median RSL projections would by 2100 submerge land currently home to 153 million people, an increase of 44 million. The use of a physical model, rather than simple parameterizations assuming constant acceleration of ice loss, increases forcing sensitivity: overlap between the central 90% of simulations for 2100 for RCP 8.5 (93-243 cm) and RCP 2.6 (26-98 cm) is minimal. By 2300, the gap between median GMSL estimates for RCP 8.5 and RCP 2.6 reaches >10 m, with median RSL projections for RCP 8.5 jeopardizing land now occupied by 950 million people (versus 167 million for RCP 2.6). The minimal correlation between the contribution of AIS to GMSL by 2050 and that in 2100 and beyond implies current sea-level observations cannot exclude future extreme outcomes. The sensitivity of post-2050 projections to deeply uncertain physics highlights the need for robust decision and adaptive management frameworks. Plain Language Summary Recent ice-sheet modeling papers have introduced new physical mechanisms-specifically the hydrofracturing of ice shelves and the collapse of ice cliffs-that can rapidly increase ice-sheet mass loss from a marine-based ice-sheet, as exists in much of Antarctica. This paper links new Antarctic model results into a sea-level rise projection framework to examine their influence on global and regional sea-level rise projections and their associated uncertainties, the potential impact of projected sea-level rise on areas currently occupied by human populations, and the implications of these projections for the ability to constrain future changes from present observations. Under a high greenhouse gas emission future, these new physical processes increase median projected 21st century GMSL rise from \u223c80 to \u223c150 cm. Revised median RSL projections for a high-emissions future would, without protective measures, by 2100 submerge land currently home to more than 153 million people. The use of a physical model indicates that emissions matter more for 21st century sea-level change than previous projections showed. Moreover, there is little correlation between the contribution of Antarctic to sea-level rise by 2050 and its contribution in 2100 and beyond, so current sea-level observations cannot exclude future extreme outcomes. Revised Antarctic ProjectionsIn this paper, we compare two sets of projections. The first, which we label K14, follows the original methodology of K14, extended in space and time. The second, which we label DP16, replaces the AIS projections of K14 with projections based on new physical modeling . These processes include the influence of surface meltwater, driven by summer temperatures above freezing and the increasing ratio of rain to snow in a warming climate, on the penetration into ice shelves of surface crevasses that can lead to hydrofracturing. Hence, in DP16, buttressing ice shelves can thin or be lost entirely due to sub-ice ocean warming, the extensive spread of surface meltwater, or a combination of the two. In places where thick, marine-terminating grounding lines have lost their buttressing ice shelves, a wastage rate of ice is applied locally at the tidewater grounding line in places where vertical ice cliffs are tall enough to produce stresses that exceed the yield strength of the ice (see DeConto for complete formulation).Three uncertain but key model parameters relate to (1) the rate of sub-ice shelf melt rates in response to warming ocean temperatures (OCFAC), (2) the sensitivity of crevasse penetration to meltwater input (hydrofracturing) (CREVLIQ), and (3) the maximum rate of cliff collapse (VCLIF). Because, as discussed below, there are no modern analogues to widespread ice-cliff failure, model performance cannot be adequately judged relative to Holocene or recent trends in ice-sheet behavior. Instead, the new model physics were tested relative to past episodes of ice sheet retreat during the Pliocene (\u223c3 Ma) and the Last Interglacial (LIG, \u223c125 ka), when Antarctic ocean and surface air temperatures were warmer than today . The three key parameters were varied systematically. From an initial 64 versions of the ice-sheet model, 29 were found to satisfy both Pliocene and LIG sea-level targets, with Antarctic contributions to GMSL ranging between 5-15 m (Pliocene) and 3.6-7.4 m (LIG). The range of oceanic melt rate model parameters passing the Pliocene and LIG sea-level tests are comparable to those determined from a large, 625-member ensemble of the last deglacial retreat of the WAIS using the same ice-sheet model ; however, the deglacial simulations do not provide guidance on hydrofracturing and ice-cliff physics, because the background climate was too cold to trigger those processes.One challenge of formulating a parameterization of ice-cliff physics is the lack of observations of marine-terminating ice without buttressing ice shelves and of sufficient thickness (\u223c1000 m) to allow subaerial ice cliffs tall enough (\u223c100 m) to drive structural collapse . The few calving fronts of this scale that exist today (e.g., Helheim and Jakobshavn Glaciers on Greenland, and Crane Glacier on the Antarctic Peninsula) are experiencing rates of calving and structural failure at the terminus, comparable to the seaward flow of the glaciers, on the order of \u223c2 to >12 km/yr. (e.g., . Unlike several major Antarctic outlet glaciers, these Greenland outlet glaciers are in relatively narrow (5-12 km wide), restricted fjords, with substantial m\u00e9lange (a mix of ice bergs and sea ice that can provide some supporting buttressing/back pressure at the terminus), and supportive, lateral shear along the fjord walls. Hence, using observed rates of cliff collapse to constrain the model physics representing these processes could lead to underestimates.In Antarctica, there is potential for much wider ice cliffs to form along vast stretches of the coastline if floating ice tongues and shelves are lost. For example,\n###\n", "summary": " Has value: 0.26\n, Global Mean Sea level Rise Projection: Global Mean Sea Level Rise Projections\n, has lower limit for likely range: 0.18\n, has upper limit for likely range: 0.36\n, has  start of period: 2000\n, has end of period: 2050\n, climate scenario: RCP4.5\n, has research problem: Global Mean Sea Level Rise Projections\n, has unit: m\n###"}
{"summary": "has benchmark: Benchmark Twitter/Benchmark Recipe/Benchmark Classic/Benchmark Amazon/Benchmark BBCSport/Benchmark Reuters-21578/Benchmark 20NEWS/Benchmark Ohsumed\nhas research problem: Text Classification", "text": "#Properties\nhas benchmark, has research problem\n#Text\nThe Word Mover's Distance (WMD) proposed by Kusner et al. is a distance between documents that takes advantage of semantic relations among words that are captured by their embeddings. This distance proved to be quite effective, obtaining state-of-art error rates for classification tasks, but is also impracticable for large collections/documents due to its computational complexity. For circumventing this problem, variants of WMD have been proposed. Among them, Relaxed Word Mover's Distance (RWMD) is one of the most successful due to its simplicity, effectiveness, and also because of its fast implementations. Relying on assumptions that are supported by empirical properties of the distances between embeddings, we propose an approach to speed up both WMD and RWMD. Experiments over 10 datasets suggest that our approach leads to a significant speed-up in document classification tasks while maintaining the same error rates. Our ContributionsTo achieve this goal, in contrast to other approaches available, we explore the properties of the application domain, more specifically the distribution of distances among word embeddings. Our key observation is that one can assume, without incurring a significant loss, that the set of distances between word embeddings is split into two sets: the set of distances between related words and the set of distances between non-related words, with the distances in the latter having the same value. We show that this assumption, which is supported by empirical data, can be used to: (i) obtain a more compact formulation for the transportation problem that is used to calculate WMD and its variants and (ii) dramatically reduce the memory required to cache the distances between embeddings, which is essential for the fast computation of RWMD for large vocabularies since the evaluation of the distance between a pair of words requires hundreds of operations for typical sizes of embeddings.By relying on the previous observation, we propose a simple approach for speeding up WMD and distances with a similar flavour. More concretely, we show how to derive new distances between docu-arXiv:1912.00509v2 [cs.CL] 8 May 2020 ments by applying our approach to both WMD and RWMD. The time and space complexities required to compute these distances depend on a parameter r that has to do with the number of related words. This parameter can be set to a small value which leads to complexity improvements over WMD and RWMD. In addition, experiments executed over 10 datasets, for two distinct tasks, suggest that these distances yield to test errors as good as those obtained by WMD/RWMD, with a significant gain in terms of execution time. Indeed, with regards to efficient implementations of RWMD, we obtained an average speed-up of almost 5 times for one task and 15 times for the other. Paper OrganizationThe paper is organized as follows. In Section 2, we introduce our notation and discuss some background that is important to the understanding of our work. In the next section, we develop our approach. In Section 4, we present our experimental study comparing the new distance with WMD and RWMD both in terms of test error and computational performance. Finally, in Section 5, we present our conclusion. NotationWe assume that we have a vocabulary of n words {1, . . . , n} and a collection of documents. Throughout the text, we need to refer to arbitrary documents D and D to explain existing distances and the new ones that we propose. Hence, unless otherwise stated, we assume that the set of distinct words of D and D are, respectively, {w1, . . . , w |D| } and {w 1 , . . . , w |D | }. Note that |D| (resp. |D |) is the number of distinct words of document D (resp. D ). Moreover, we use Di to denote the normalized frequency of wi, that is, the number of occurrences of wi in D over the total number of words in D. We use D j to refer to the normalized frequency of w j analogously. NoteWe use x(i) to denote the embedding of a word i in a vector space of dimension d and we use c(i, j) to denote the Euclidean distance between the embeddings of words i and j, that is, c(i, j) =xTo make sure that our notation is clearly understood, we present a simple example involving the following documents: D:John likes algorithms. Mary likes algorithms too. D : John also likes data structures.Ignoring the stopwords, and assuming that D and D are the only doc's in our collection, we have the following vocabulary {1 : \"John\", 2 : \"likes\", 3 : \"algorithms\",4 : \"Mary\", 5 : \"too\", 6 : \"also\", 7 : \"data\", Word Mover's DistanceBy exploring the behaviour of the Word Embeddings, Kusner et al.  define the distance between two documents as the minimum cost of converting the words of one document into the words of the other, where the cost c(wi, w j ) of transforming the word wi into word w j is given by the distance between the word embeddings of wi and w j .Formally, the WMD between documents D and D is defined as the value of the optimal solution of the following transportation problem::In the above formulation, T is the flow matrix. The variable Tij gives the amount of word wi that is transformed into word w j . The equation , for a fixed i, assures that each unit of word wi is transformed into a unit of a word in D while Equation , for each j, assures that the total units of words in D transformed into w j is D j .The WMD, although well founded, suffers from efficiency problems since solving the transportation problem on a complete bipartite graph is costly, requiring super cubic time using the best known minimum cost flow algorithms . Relaxed Word Mover's DistanceTo overcome the high computational cost of solving the transportation problem, Kusner et al.  propose the RWMD, a variation of WMD whose computation relies on optimally solving relaxations of the transportation problem. These relaxations are obtained by either ignoring the set of constraints (2) or the set (3). In fact, the RMWD between D and D can be calculated by evaluating the expressionwhere the left and the right terms in the maximum are the optimum values of the relaxations that ignore constraints (3) and (2), respectively.By examining the above equation we conclude that, given the costs c(wi, w j )'s, RWMD can be calculated for a pair of documents D and D in O(|D| \u00d7 |D |) time, which is a significant improvement over WMD. Therefore, RWMD's bottleneck is the computation of the costs c(wi, w j )'s since it requires O(|D|We also note that by performing a simple preprocessing step before applying equation we can obtain a tighter relaxation of WMD that corresponds to the OMR distance proposed in . The motivation is better handling cases in which D and D share many words.The preprocessing consists of first identifying pairs of words (wi, w j ), with wi = w j . Then, for each of these pairs we do the following: (i) we replace Di with its excess max{Di \u2212 D j , 0} and, if Di \u2264 D j , we remove index i from the range where the minimum iterates in the right term of the max; (ii) similarly, we replace D j with its excess max{D j \u2212 Di, 0} and, if D j \u2264 Di, we remove index j from the range where the minimum iterates in the left term of the max. The impact of this preprocessing is associating in equation the excesses of Di and D j with the second closest word to wi and w j , respectively. RWMD in linear timeIn , it is proposed an implementation that computes the RWMD between two documents D and D in O(|D| + |D |) time, improving upon the O(|D| \u2022 |D| \u2022 d) time required by the original proposal. This improvement, however, comes at the expense of some potentially costly preprocessing.To explain the implementation, denoted here by RWMD(L), let C be a collection of documents and let Ci(j) be the word, among those in the i-th document of C, that is closest to some given word j in the vocabulary. RWMD(L) builds, at the preprocessing phase, a matrix M with |C| rows and n columns, where the entry Mij stores the distance between Ci(j) and word j. To fill the row of M associated with document D we have to pay O(nHaving the matrix M available, it is possible to compute the RWMD between documents D and D in O(|D| + |D |) time. The reason is that the terms minjc(wi, w j ) and minic(wi, w j ) of (5) can be computed in O(1) time. The former is obtained by accessing the entry M r ,i , where r is the row corresponding to document D , while for the latter we need to access the entry Mr,j, where r is the row corresponding to document D.In addition to the time required to build matrix M , another potential problem of RWMD(L) is its space requirement since the matrix M can be prohibitively large when either the vocabulary or the collection of documents is huge.We note that in order to properly use the preprocesssing for equation  described in the previous section, we should store in matrix M the distance of w to its second closest word among those that belong D, when w \u2208 D. AN APPROACH FOR SPEEDING UP WMD AND ITS VARIANTSFor large vocabularies the methods discussed in the previous section may have to cope with an enormous amount of distances between embeddings, which may be a problem either in terms of memory requirements or in terms of running time. In fact, for applications (e.g. document classification via k-NN) that use the distance between the same pair of embeddings several times caching turns out to be crucial for achieving computational efficiency. However, when the vocabulary is large, caching becomes prohibitive. In this section, we show that we can significantly attenuate this problem by taking into account the distribution of distances among word embeddings. On the distances between word embeddingsHere we discuss the assumptions in which our approach and the new distances, derived from it, rely on. For that, we present examples of distances between word embeddings. These embeddings, used here and in the next sections, were made available by Google 2 . To obtain them, they trained the vectors with d = 300 using the Word2Vec template of Word Embeddings  on top of a Google News document base containing altogether about 100 billion words and 3 million tokens. We also refer to some datasets that will be detailed in our experimental section.From a semantic perspective, it is reasonable to consider that words are closely related to only a few other words in general. As the word embeddings were designed to simulate semantic relations, it is expected that they present a similar behaviour; that is, each vector should be close to a few other vectors and far away from the remaining ones.As an example, if the words are ranked according to their distances to the embedding corresponding to \"cat\" one should expect \"dog\" and \"rabbit\" preceding both \"moon\" and \"guitar\". However, it is not clear whether \"moon\" or \"guitar\" comes first in the ranking since neither of them has an obvious relation with \"cat\".  illustrates this behaviour by displaying the distances between the embedding for \"cat\" and the embeddings from the words of the Amazon dataset sorted by increasing order of distance. We note that there are few words with small distances while the vast majority has distance concentrated in the range [1.2, 1.4]. For checking whether this behaviour persists for other words, we computed all the distances between embeddings from the words of the Reuters dataset.  shows the distribution of these distances clustered in bins for better visualization. Once again, we observe a high concentration of the distances around the interval [1.2, 1.4], behaving similarly to a Normal distribution. Based on this discussion, we make the following assumptions:(i) Given a word w, the remaining words can be split into two groups:RELATED(w) and UNRELATED(w), with the former (latter) containing the words related (unrelated) with w;(ii) The distances from every word in UNRELATED(w) to w, for every w, is the same \"large\" value cmax.Although the number of related words may vary according to the word of reference, in order to make our approach simpler and thus, more practical, we assume that all words have the same number of related words and we use r to denote this number. The value of r can either be set manually or automatically estimated using a training set as we discuss in the experimental section. Algorithms exploiting distance assumptionsAlgorithms to compute distances with the flavour of WMD can benefit from our assumptions because, by using them, they just need to handle a much smaller set of distances between embeddings, that is, the set of distances between related words. As a result, caching distances becomes feasible even for large vocabularies, which prevent these methods of calculating the distance between the same pair of words more than once. In addition, the transportation problem in which WMD and related distances as RWMD rely on can be solved in a sparse bipartite graph rather than on a complete bipartite graph.In the next subsections we discuss how WMD and RWMD can be adapted to make use of our assumptions. These adaptations lead to new distances between documents, namely Rel-WMD and Rel-RWMD. We start with the explanation of a preprocessing phase that is required to calculate these new distances. Preprocessing PhaseIn this phase, we build a structure (cache) C that stores for each word w, from a vocabulary of n words, the r closest words to w as well as its distances to w.Choose a word i in the vocabulary. The procedure computes its Euclidean distance x(i)\u2212x(j) 2 to every other word j and add these distances, as well as the corresponding words, to a list Li. Next, it selects the r words that are closest to i in Li and adds them, with their distances, to cache C. This selection can be performed in expected linear time using QuickSelect . The distances that were not included in C are then added to a global accumulator A with the goal of calculating cmax. This procedure is repeated for every word i in the vocabulary and the value cmax is given by the average of all values added to A.The cache C requires O(n \u2022 r) space and its construction requires O(n 2 \u2022d) time, where the term d is due to the time required to compute the distance between a pair of embeddings.For large vocabularies the construction of the cache C, as above described, may be costly due to the O(n 2 \u2022 d) time complexity. This construction, however, can be optimized by clustering the embeddings and then considering only words in the same cluster to find the related words.We discuss this approach using the traditional k-means clustering algorithm . On the one hand, this algorithm allows the user to define the number of clusters k and it performs O(n Related Word Mover's DistanceThe Related Word Mover's Distance (Rel-WMD) between D and D is defined as the optimum value of the transportation problem given by equations (1)-(4), where the costs of the edges are as follows:For small values of parameter r many costs are equal to cmax. In this case, it is possible to replace the formulation given by (1)-  with an equivalent and more compact one. This new formulation is given by (7)-  and its key idea is using variable Ti,t to represent the number of units of word wi that is transformed into words that are at a distance cmax from wi. Thus, the single variable Ti,t replaces all variable Ti,j in the original formulation for which (wi, w j ) does not belong to cache C. Similarly Tt,j represents the number of units transformed into w j from words that are at a distance cmax of word w j . The underlying graph of this new formulation is much sparser (for small values of r) so that the transportation problem can be solved significantly faster.s.t.:Tt,j +Ti,j, Ti,t, Tt,j \u2265 0 for all i, jWe shall note that this formulation has been used before by  to speed up EMD  in the context of image retrieval. Related Relaxed Word Mover's DistanceThe Related Relaxed Word Mover 's Distance (Rel-RWMD) is a variation of the Rel-WMD, in which we drop constraints of the original formulation in order to obtain a relaxation that can be computed more efficiently. Rel-RWMD is to Rel-WMD as RWMD is to WMD.The Rel-RWMD can be computed using equation with a cost structure given by . Let R (wi) (resp. R(w j ) ) be the set of related words of wi (resp. w j ) that belong to D (resp. D). Thus, the Rel-RWMD between documents D and D is given byAlthough not explicit in the above equation, if R (wi) (resp. R(w j )) is empty then Di (resp. D j ) is multiplied by cmax.To efficiently evaluate the first term of the maximum in equation  we need to obtain for each w in D its related words that belong to D , that is, the set R (w). By storing D as a hash table we can find them in O(r) time. For that, it is enough to traverse the list of words related to w in cache C and for each word w in the list we use the hash table of D to verify whether w belongs to D . Since the second term in the maximum can be calculated analogously we conclude that the Rel-RWMD between D and D can be computed in O((|D| + |D |) \u2022 r) time, which is a significant improvement over the (|D| \u2022 |D |) time required by RWMD when the size of the documents is considerably larger than r.Finally, we mention that the linear time implementation of RWMD presented in Section 2.3.1 can also benefit from our assumptions. The first advantage is that the matrix M can be computed faster since, in order to fill the row associated with a document D, we just need to consider the words in the vocabulary that are related to D because for the other words the corresponding entries have value cmax. EXPERIMENTSTo evaluate our methods, we employ two tasks that involve the computation of distances between documents. The first one is the document classification task via k-Nearest Neighbors (k-NN) that was used to evaluate the WMD algorithm in . The second one is the task of identifying related pairs of documents, employed to evaluate the performance of paragraph vector .We compared in terms of test error and computational performance our new distance, Rel-RWMD, against WMD, RWMD, Cosine distance and Word Centroid Distance (WCD) . The WCD between two documents is given by the Euclidean distance between their centroids, where the centroid of a document D is defined as |D| i=1 Dix(wi). When reporting computational times we use RWMD(S) and RWMD(L) to distinguish between the Standard implementation of RWMD and the one that requires Linear time. Rel-RWMD(S) and Rel-RWMD(L) are used analogously. We note that for all RWMD's implementations the preprocessing described at the end of Section 2.3 is applied.Although we have also implemented/evaluated Rel-WMD, its results are omitted in the next sections since, in general, it is competitive with Rel-RWMD in terms of test error while being much slower.The methods were implemented in C++. The Eigen library  was used for matrix manipulation and Linear Algebra while the OR-Tools library  was used for the resolution of flow problems. All experiments were executed using a single core of an Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz, with 8 GB of RAM. The code and datasets are available in a GitHub repository 3 . Document classification via k-NNOur experimental setting follows Kusner et al. , where different distances are evaluated according to their performance when they are employed by the k-NN method to address document classification tasks.In order to classify a document D from some testing set, k-NN computes the distance of D to each document in the corresponding training set and then it returns the most frequent class among the k closest documents to D. As stated in , motivations for using this evaluation approach, based on k-NN , include its reproducibility and simplicity.We run k-NN using k = 19, and, in case of ties, k is divided by two until there are no more ties. This setting is slightly different from , where k is selected from the set {1, 3, . . . , 19} based on the lower error rate obtained.The parameter r that defines the number of related words was selected from the set S = {1, 2, 4, . . . , 128} using a 5-fold crossvalidation on top of the training set. Because we are prioritizing computational performance and, the smaller the r the faster the method, we choose the lowest r whose test error in the cross validation is at most 1% larger than the minimum one found among all the possibilities in the set S. Datasets descriptionWe used the following eight preprocessed datasets 4 provided by : For all datasets 70% is used for training and 30% for testing, respecting the partitions provided.  presents relevant statistics for each of these datasets.   presents the test errors obtained by the distances under consideration over the eight datasets. We averaged the results for the datasets   AMAZON, BBCSPORT, CLASSIC, RECIPE, and TWITTER following the 5 predefined train/test splits. The remaining datasets have only one split, and so the average is not necessary. Some observations are in order: clearly, WCD and Cosine presented the worst results. Among WMD, RWMD, and Rel-RWMD, there is a balance. The behaviour of WCD and Cosine, as well as the balance between WMD and RWMD, are compatible with the findings/conclusions reached in  while the results of Rel-RWMD suggest that our simplifying assumptions work very well. The values selected for r ranged from 2 (20NEWS and RECIPE) to 128 (AMAZON) with a median equal 19.5.   presents the running times in minutes for all distances and datasets examined. First, as expected, WCD and Cosine are the fastest distances since they run in linear time and their preprocessing phases are very cheap while WMD is the slowest distance since it has to solve a transportation problem optimally. We note that the times of Cosine were omitted due to the lack of space. ResultsIt is interesting to examine how the distances/implementations related to RWMD perform. RWMD(S), the original implementation of , is the slowest of them while Rel-RWMD(L) is the fastest one, being on average 4.7 times faster than RWMD(L), which is the second fastest. The main advantage of Rel-RWMD(L) over RWMD(L) is due to the time required to build the matrix M since Rel-RWMD(L) is, on average, 10 times faster. With regards to the time required to evaluate two doc's we can also observe a small advantage of Rel-RWMD(L) which is probably related to the sparsity of M . It is important to mention that the values in  do not include the time required to estimate the value of r. In fact, the execution of a 5-fold cross validation on the training set for each potential r incurs a high cost. However, in practice one can estimate the value of r using a much smaller set or, even better, set r to a small value, without estimating it, as suggested by the results of . This table presents the test errors for r = 1, 2, 16, 128 and also for the value estimated via cross validation. We observe that the test errors remain at the same level, in particular for r \u2265 16. The running times, though not presented, change very little as expected since the value of r has a small effect in the time complexity of the linear implementation of Rel-RWMD. Identifying related documentsFor the second task our experimental setting is inspired on Dai et al. , where representations/distances are evaluated according to their capacity of recognizing whether a document D 1 is more related to a document D 2 or to a document D 3 . For achieving this goal, testing sets are used which contain many triples of documents, namely triplets. In each triplet, only two documents are related and a given distance succeeds if its smallest value is achieved for the related pair. Datasets descriptionFor our experiments, we first downloaded the documents in the two testing sets of triplets 5 provided in . The first set uses papers from Arxiv while the second one uses articles from Wikipedia. Then, we preprocessed them to remove all non-alphanumeric characters and words contained in a list of stopwords due to its little semantic value. Finally, to represent the documents we just consider the words that have embeddings in the set that Google made available. It is important to note that we are not using the embeddings of [6] since they were not provided.  presents relevant statistics for each of the datasets. ResultsIn this experiment, in contrast to the previous one, each document has its distance evaluated a few times on average, indeed less than twice.5  quocle/triplets-data.tar.gz Thus, building the cache M required for the linear time implementations of RWMD does not pay off. In addition, its size would be huge, around 10 10 entries for Wikipedia as an example. Therefore, we only executed RWMD(S) and Rel-RWMD(S). By comparing the statistics of the datasets in , we observe that the number of tokens (word embeddings) of the latter is one order of magnitude higher than the former and, as a consequence, the preprocessing phase of Rel-RWMD becomes expensive, harming the performance gain achieved while computing the distances. Thus, following our approach, we cluster the embeddings before building the cache C. We run k-means using a limit of I = 5 iterations and setting k = 289 \u2248 415, 967/5 for Wikipedia and k = 229 \u2248 260, 640/5 for Arxiv. Moreover, motivated by the discussion/results of the previous section we used r = 16 for Rel-RWMD.  presents the test errors achieved by the different methods. We can observe a behaviour similar to the previous task. Once again, both Cosine and WCD achieve the largest test errors while the others display competitive results. The computational times (in minutes) are displayed in . Again WCD and Cosine are the fastest. The former is slower because it has to compute the centroids of the documents in its preprocessing phase while the latter does not. Among the others, Rel-RWMD(S) and RWMD(S), as expected, are much faster than WMD. For Wikipedia Rel-RWMD(S) is 3 times faster than RWMD(S) while for Arxiv Rel-RWMD(S) it is 27 times faster.By taking a more in-depth examination of the running times we can also observe that the time consumption of REL-RWMD(S) is highly concentrated on its preprocessing phase when the cache C is built. Having this structure available, it computes the distances, on average, 25 and 60 times faster than RWMD(S) for Wikipedia and Arxiv, respectively. CONCLUSIONIn this paper, we presented an approach to speed up the computation of WMD and its variants that relies on the properties of the distances between embeddings. The improvements in time and space complexities together with our experimental evaluation provide strong evidence that this approach should be employed if one is aiming to compute these distances efficiently.\n###\n"}
{"text": "#Properties\nHas value, Method, Time period, has beginning, has end, Lower confidence limit, Upper confidence limit, Located in, Basic reproduction number, Confidence interval (95%), has research problem, location, Approaches, same as, description, Has value, Method, Time period, has beginning, has end, Lower confidence limit, Upper confidence limit, Located in, Basic reproduction number, Confidence interval (95%), has research problem, location, Approaches, same as, description\n#Text\nAn ongoing outbreak of a novel coronavirus (2019-nCoV) pneumonia hit a major city in China, Wuhan, December 2019 and subsequently reached other provinces/regions of China and other countries. We present estimates of the basic reproduction number, R 0 , of 2019-nCoV in the early phase of the outbreak. Methods: Accounting for the impact of the variations in disease reporting rate, we modelled the epidemic curve of 2019-nCoV cases time series, in mainland China from January 10 to January 24, 2020, through the exponential growth. With the estimated intrinsic growth rate (g), we estimated R 0 by using the serial intervals (SI) of two other well-known coronavirus diseases, MERS and SARS, as approximations for the true unknown SI. Findings: The early outbreak data largely follows the exponential growth. We estimated that the mean R 0 ranges from 2.24 (95%CI: 1.96-2.55) to 3.58 (95%CI: 2.89-4.39) associated with 8-fold to 2-fold increase in the reporting rate. We demonstrated that changes in reporting rate substantially affect estimates of R 0. Conclusion: The mean estimate of R 0 for the 2019-nCoV ranges from 2.24 to 3.58, and is significantly larger than 1. Our findings indicate the potential of 2019-nCoV to cause outbreaks. MethodsWe obtained the number of 2019-nCoV cases time series data in mainland China released by Wuhan Municipal Health Commission, China and National Health Commission of China from January 10 to January 24, 2020 from (Wuhan Municipal Health Commission, China, 2020). All cases were laboratory confirmed following the case definition by the National Health Commission of China (National Health Commission of the People's Republic of China, 2020). Although the date of submission of this study is January 26, we choose to use data up to January 24. Note that the data of the most recent few days contain a number of infections that were infected outside Wuhan due to travel, and thus this part of the infections is excluded from the analysis.Although there were cases confirmed on or before January 16, the official diagnostic protocol was released by WHO on January 17 (World Health Organization, 2020b). To adjust the impact of this event, we considered a time-varying reporting rate that follows a linear increasing trend, motivated by the previous study . We assumed that the reporting rate, r(t), started increasing on January 17, and stopped at the maximal level on January 21. The reporting rate increase corresponds to accounts for the announcement on improving the 2019-nCoV surveillance of the Hubei provincial government (Hubei provincial government, 2020). The length of the reporting increasing part roughly equals the average of the incubation periods of two other well-known coronavirus diseases, i.e., the Middle East Respiratory Syndrome (MERS) and the Severe Acute Respiratory Syndrome (SARS), i.e., 5 days . Denoting the daily reported number of new cases by c(t) for the t-th day, then the adjusted cumulative number of cases, C(t), is C\u00f0t\u00de \u00bc P t t\u00bc0 c\u00f0t\u00de=r\u00f0t\u00de. Instead of finding the exact value of r(t), we calculated the fold change in r(t) that is defined by the ratio of r on January 10 over that on January 24 minus 1. We illustrated six scenarios with 0-(no change), 0.5-, 1-, 2-, 4-and 8-fold increase in reporting rate, see (a), (c), (e), (g), (i) and (k).Following previous studies , we modelled the epidemic curve obeying the exponential growth. The nonlinear least square (NLS) framework is adopted for data fitting and parameter estimation. The intrinsic growth rate (g) of the exponential growth was estimated, and the basic reproduction number could be obtained by R 0 = 1/M(\u00c0g) with 100% susceptibility for 2019-nCoV at this early stage. The function M(\u00c1) is the Laplace transform, i.e., the moment generating function, of the probability distribution for the serial interval (SI) of the disease , denoted by h(k) and k is the mean SI. Since the transmission chain of 2019-nCoV remains unclear, we adopted the SI information from SARS and MERS, which share a similar pathogen as 2019-nCoV. We modelled h(k) as Gamma distributions with a mean of 7.6 days and standard deviation (SD) of 3.4 days for MERS , and mean of 8.4 days and SD of 3.8 days for SARS  as well as their average, see the row heads in  for each scenario. ConclusionWe estimated the mean R 0 of 2019-nCoV ranging from 2.24 (95%CI: 1.96-2.55) to 3.58 (95%CI: 2.89-4.39) if the reporting effort has been increased by a factor of between 8-and 2-fold, respectively, after the diagnostic protocol released on January 17, 2020 and many medical supplies reached Wuhan.\n###\n", "summary": " Has value: 2.24\n, Method: \"Statistical exponential growth model method adopting serial interval from SARS (mean\u2009=\u20098.4 days\n, Time period: Time interval\n, has beginning: 2020-01-10\n, has end: 2020-01-24\n, Lower confidence limit: 1.96\n, Upper confidence limit: 2.55\n, Basic reproduction number: Basic reproduction number estimate value specification\n, Confidence interval (95%): Confidence interval (95%)\n, has research problem: Determination of the COVID-19 basic reproduction number\n, location: China\n, Approaches: Corresponding to 8-fold increase in the reporting rateR0\u2009=\u20091/M(\u2212\ud835\udc5f)\ud835\udc5f =intrinsic growth rateM\u2009=\u2009moment generating function\n, same as: https://en.wikipedia.org/wiki/COVID-19_pandemic\n, description: This research problem aims at determining the basic reproduction rate of COVID-19.\n, Has value: 3.58\n, Method: \"Statistical exponential growth model method adopting serial interval from SARS (mean\u2009=\u20098.4 days\n, Time period: Time interval\n, has beginning: 2020-01-10\n, has end: 2020-01-24\n, Lower confidence limit: 2.89\n, Upper confidence limit: 4.39\n, Basic reproduction number: Basic reproduction number estimate value specification\n, Confidence interval (95%): Confidence interval (95%)\n, has research problem: Determination of the COVID-19 basic reproduction number\n, location: China\n, Approaches: Corresponding to 2-fold increase in the reporting rateR0\u2009=\u20091/M(\u2212\ud835\udc5f)\ud835\udc5f =intrinsic growth rateM\u2009=\u2009moment generating function\n, same as: https://en.wikipedia.org/wiki/COVID-19_pandemic\n, description: This research problem aims at determining the basic reproduction rate of COVID-19.\n###"}
{"text": "#Properties\nMethod, dataset, has research problem, approach, deals with, uses similarity, Performance metric, Evidence, Uncertainty, Limitations\n#Text\nThis paper proposes a method for classifying true papers of a set of focal scientists and false papers of homonymous authors in bibliometric research processes. It directly addresses the issue of identifying papers that are not associated (''false'') with a given author. The proposed method has four steps: name and affiliation filtering, similarity score construction, author screening, and boosted trees classification. In this methodological paper we calculate error rates for our technique. Therefore, we needed to ascertain the correct attribution of each paper. To do this we constructed a small dataset of 4,253 papers allegedly belonging to a random sample of 100 authors. We apply the boosted trees algorithm to classify papers of authors with total false rate no higher than 30% (i.e. 3,862 papers of 91 authors). A one-run experiment achieves a testing misclassification error 0.55%, testing recall 99.84%, and testing precision 99.60%. A 50-run experiment shows that the median of testing classification error is 0.78% and mean 0.75%. Among the 90 authors in the testing set (one author only appeared in the training set), the algorithm successfully reduces the false rate to zero for 86 authors and misclassifies just one or two papers for each of the remaining four authors. Proposed approachThis study takes the second approach, namely the binary classification approach. It is motivated by our studies on research collaboration/co-authorship networks and attempts to identify a clean publication set for each focal scientist to allow reliable network analyses. The proposed method predicts whether a paper is a true paper of our focal scientist or a false paper of homonymous authors. The real world application of our method to 54,853 papers of 1,315 authors is described in Appendix 1. In this methodological paper we calculate error rates for our technique. Therefore, we needed to independently ascertain the correct attribution of each paper. To do this we constructed a small development dataset of 4,253 papers allegedly belonging to a random sample of 100 American scientists who are part of the larger study. The work described here cleans the development data set and assesses the performance of the algorithms against the independent human judgment of each paper's true status.Among these 100 authors, we observe several very common English and Chinese last names (such as Smith and Wang). As expected, even after author name and affiliation filtering, the resulting paper set still has a very high false rate. The term ''false rate'' of an author, throughout this paper, refers to the percentage of papers under his/her name that actually were written by somebody else.To create a cleaned and usable bibliometric data set for each focal author, we designed a four-step method: the first step is downloading paper records from WoS using a name and affiliation match. The second step constructs the pairwise cosine similarity matrices for all papers under each focal author name, and then uses Eigen-decomposition and averaging to construct a variety of numerical similarity scores to measure the distance between one paper and all other papers under the same name. The method assumes that papers written by the same author have characteristics in common while papers written by different authors are less likely to be similar to each other. Affiliation filtering in the first step yields a set of papers with a majority likely to be true, so we can assume that true papers will show a higher similarity to all other papers, while false papers will have lower similarity to all other papers. This difference makes possible classification. In the third step we eliminate a small number of authors with very high false rates. In the fourth step we use boosted trees for classifying true and false papers for the remaining authors.The effectiveness of the boosted trees classification relies on the assumption that the majority of papers under one author name are true ones, so we only use the boosted trees algorithm for classifying papers of authors with false rate less than 30%. Therefore, an author screening model (step 3) is constructed to decide whether each author is in the low false rate group. Authors in the low false rate group will be included and their papers will be automatically classified by boosted trees, while authors in the high false rate group are excluded and their papers have to be manually cleaned.The procedure of our method is summarized in . This paper will explain steps 1, 2, and 4 (name and affiliation filtering, similarity score construction, and boosted trees classification) first, and then introduce the motivation and design of step 3 (author screening). Name and affiliation filteringIn real-world evaluation practice, external databases concerning focal authors are typical available before collecting bibliometric data. In our context, we have the whole affiliation history information of each author available. In the data retrieval stage, we required a name match (first initial and last name match) and affiliation match (at least one of the author Choice of these features is based on previous literature and data availability in WoS. Affiliation is found to be of particular importance for name disambiguation, but not included here, because we have used affiliation filtering in the first step. Authors with the same name are more likely to be one person, if they coauthor with same coauthors. Coauthor name has been found to be effective for name disambiguation, so we use the ''names of authors'' information available in WoS. Based on the idea that papers written by the same author should share something similar, a variety of aspects have been used in previous methods. For example, same author may tend to cite the same literature, and references information is widely used in name disambiguation practice. We use ''cited journal'' but not cited references at the article level, because the former information is much cleaner and more ready for analysis in WoS and the similarity matrix will be too sparse if using cited reference at the article level. Furthermore, many studies try to extract content/meaning information from articles and then measure their content similarities, and features investigated include\n###\n", "summary": " Method: Boosted tree classification\n, dataset: Not standard\n, has research problem: Author name disambiguation\n, approach: Supervised learning\n, deals with: Homonyms problem\n, uses similarity: Cosine/Eigen Decomposition\n, Performance metric: Misclassification error rate/Precision/Recall\n, Evidence: Abstract/Author name/Keywords/Subject category/Title words/Venues\n, Uncertainty: F\n, Limitations: How to decide the splitting point and how to control the size of the tree.\n###"}
{"summary": "Has evaluation: true\nAlligned with FAIR principles: false\nOnline data availability: true\nTest related data: None\nhas research problem: goals/instructional designs/framework\nReuse of existing vocabularies: ContextOntology/GoalsOntology/ProcessOntology", "text": "#Properties\nHas evaluation, Alligned with FAIR principles, Online data availability, Test related data, has research problem, Reuse of existing vocabularies\n#Text\nDespite rapid progress, most of the educational technologies today lack a strong instructional design knowledge basis leading to questionable quality of instruction. In addition, a major challenge is to customize these educational technologies for a wide range of instructional designs. Ontologies are one of the pertinent mechanisms to represent instructional design in the literature. However, existing approaches do not support modeling of flexible instructional designs. To address this problem, in this paper, we propose an ontology based framework for systematic modeling of different aspects of instructional design knowledge based on domain patterns. As part of the framework, we present ontologies for modeling goals, instructional processes and instructional materials. We demonstrate the ontology framework by presenting instances of the ontology for the large scale case study of adult literacy in India (287 million learners spread across 22 Indian Languages), which requires creation of 1000 similar but varied eLearning Systems based on flexible instructional designs. The implemented framework is available at http://rice.iiit.ac.in and is transferred to National Literacy Mission of Government of India. This framework could be used for modeling instructional design knowledge of systems for skills, school education and beyond. MotivationJohn McCarthy has envisioned that an intelligent way of building systems should focus on the knowledge that is required to represent system's inputs and methods through which possible conclusions can be automatically derived from that knowledge . Newell has proposed the need to have a knowledge level focusing on specifying the world independent of symbol level that focuses on implementing the behaviour of the system .\"The Knowledge Principle: A system exhibits intelligent understanding and action at a high level of competence primarily because of the specific knowledge that it can bring to bear: the concepts, facts, , and heuristics about its domain of endeavor.\" -Lenat and Feigenbaum  Feigenbaum coined the term knowledge engineering and proposed knowledge base should be a fundamental basis that stores expertise of human experts in solving real-world problems . There are two primary directions of research related to knowledge engineering one in the field of Artificial Intelligence primarily for automatic reasoning and expert systems and in computer science to represent different aspects of the system. In this paper, we are interested in modeling knowledge in the domain of education to facilitate design of educational technologies 1 and eLearning Systems 2 . We are specifically interested in the following questions:-What is the knowledge that is required to facilitate the design of educational technologies for scale and variety? where scale is the number of systems to be developed and variety represents the different kinds of systems to be developed in education domain. -How do you concretely represent this knowledge?Towards answering these questions, the core contribution of this paper 3 is:-An ontology based framework to model instructional design knowledge to facilitate scale & variety during design of educational technologies -The proposed framework is evaluated by demonstrating ontologies for the large scale case study of adult literacy throughout the paperThe rest of the paper is as follows: A brief background of adult literacy case study is presented in Section 2. Existing work on ontologies for instructional design and for adult literacy are discussed in Section 3 as part of related work. A broad spectrum of ontologies and their development process is in Section 4. In Section 5, we present our ontology based framework for modeling instructional design. Within this section, we detail ontologies for modeling goals, instructional process and instructional material in sub sections 5.1, 5.2 and 5.3. We present a concrete instance of domain ontology for adult literacy as a instantiation of the ontology in Section 6.1. Finally, we end the paper with conclusions and future work in Section 7. Concrete Knowledge in Adult Literacy Case StudyWe briefly present a concrete example of knowledge from adult literacy case study as a first step for setting the context for rest of the paper. The IPCL approach suggests the use of eclectic method for teaching reading skills, comprehension, problem solving and facilitates learning through interpretation of contents in the context of life (Handbook for Developing IPCL Material, 2003)(Confintea VI: sixth international conference on adult education: final report, 2010). We consider \"context of life\" as learners' prior knowledge. Ecletic method primarily differs from traditional methods as it does not start with alphabets but instead uses familiar and known words to learners, decomposes them to syllables and phonemes as shown in [A] . These syllables and phonemes are further synthesized to form words and in the end, the alphabet is learnt as in  . The patterns of decomposition (top-down) focuses on cognitive abilities of learners whereas the patterns of composition (bottom-up) can facilitate reasoning of the subject knowledge. We have depicted examples of this process in  for the Telugu language based on the primer (instructional material). In this figure, a sentence in Telugu language \u0c32\u0c02 \u0c02 is first decomposed into two words namely \u0c32\u0c02, \u0c02 . Each of these words are further decomposed till the syllables are obtained. Similarly, the same sentence is also decomposed into phonemes representing the sounds of the sentence, words and syllables respectively.On the other hand, [A] shows the composition process which uses the syllables and phonemes to form words and sentences. Specifically, words \u0c15\u0c32, \u0c0a\u0c15 and are formed by composing the syllable \u0c15 and other relevant syllables and words \u0c05\u0c02\u0c26\u0c02, \u0c35 \u0c28 are formed from their respective syllables with the learnt syllable \u0c26 in red color. This hierarchy of decompositions and compositions forms the basis for learning new syllables and phonemes in a language as shown in [B]. Several compositions are possible V-Vowel+Vowel Modifier, C-Consonant+Vowel Modifier, C+V-Consonant+Vowel, Vowel Modifier, C+C-Consonant+Consonant, C+C+V-Consonant+Consonant+Vowel and there will be several constraints on these compositions as well. For example, only one modifier might be allowed after a consonant. Discussing that in detail is not within the purview of this paper. But this approach of learning alphabet from known words and sentences and later synthesizing new words from syllables and phonemes was empirically established by IPCL as a successful approach for adult literacy in India (Handbook for Developing . Most importantly, this approach works for the scale and variety of 22 Indian languages and varied instructional designs. This specific knowledge has been abstracted into instructional design knowledge and patterns . Generalizing from this specific knowledge, the requirements for knowledge representation in the context of this paper are as follows. The representation should be:-in synergy with instructional design -machine-processable -facilitate reuse and semi-automatic design of eLearning Systems -able to support sharing of knowledge between different applications and toolsIn the next section, we discuss related work for modeling instructional design knowledge in the context of this paper. Ontologies for Instructional DesignOntologies have gained immense importance in the last few decades as one of the widely used methods to represent and share knowledge in several domains such as software engineering , enterprise modeling , requirements engineering . These ontologies are of different kinds ranging from informal light weight ontologies to formal ontologies depending on the degree of formalism and the power of expressivity . Happel et al. have discussed the advantages of ontologies over conceptual models and meta-models  as follows:-Enable new and efficient way to information reusability. -Enable to extend easily.-Provide consensus on the understanding of domain knowledge.-Support better understanding of domain knowledge.-Define problem and solution domain knowledge separately.-Assist in analyzing the structure of domain knowledge.-Facilitate a machine to use the knowledge in an application.-Share common semantics among people and applications.Fensel attributes the popularity of ontologies is due to the promise of providing \"a shared and common understanding of a domain that can be communicated between people and application systems\" , which can be construed as representation, communication and automation needs for scale and variety in the design of educational technologies.In the domain of education, ontologies are extensively used   in a wide range of applications ranging from explicit representation of domain knowledge to automatic generation of personalized content . Mizoguchi and Bourdeau have identified four key requirements of instructional authoring systems (i) adaptivity (ii) explicit conceptualization (iii) standardization to facilitate reuse (iv) theory-awareness, and proposed knowledge and ontological engineering as a potential solution to cater to these requirements .One particular use of ontologies that is of interest to this paper is to model instructional design theories and learning designs  but during design of educational technologies. A 10year research effort has resulted in creating a comprehensive ontology covering instructional design knowledge for various instructional theories and adhering to learning design standards . SMAR-TIES is a scenario-based instructional authoring tool based on this ontology and advocates the design of educational technologies based on educational theories modeled as ontologies to facilitate quality of instruction. However, the inherent complexity of the ontology and SMARTIES tool made it tough for its practical usage .While focusing on quality of instruction is one aspect, using ontologies in education to facilitate reuse is another critical research direction that received significant attention in the literature . Devedzic explored the notion of ontologies for intelligent tutoring systems (ITS) based on inspiration from software patterns in 1999 . Ontologies to formalize learning object content models have been proposed in . To facilitate flexible content reuse, the Abstract Learning Object Content Model (ALOCoM) ontology and a set of supporting tools were proposed in . Amorim et al. have proposed a learning design ontology based on IMS LD specification through a set of 20 design and run time axioms . The basic premise of this ontology was to explicitly and precisely address the drawbacks of IMS LD specification . But isolated research on learning objects and learning designs have made reuse dif-ficult motivating the need for a bridge ontology focusing on context . A formal ontology was presented for representing instructional design methods and provides a rule catalogue to verify the conformance of ontologies for a particular instructional design theory . An ontology and a software framework focusing on competencies was discussed in . However, the creation of these ontologies is not based on domain-specific patterns , which is the case in this paper. Furthermore, the existing ontologies are not aimed at scale and variety, which is an essential goal of this paper.There has been a significant growth of research in ontologies in the past decade resulting in a variety of approaches for ontological engineering (N. F.     . In particular, ontologies have been used for representing different aspects of a specific domain using a wide range of different mechanisms . At the core of ontologies is the identification of concepts generally represented as a hierarchy of classes and sub-classes and different relationships between them. Each of the these classes typically have associated properties and can also have a set of constraints. Instances of these ontologies are called as individuals and represent a specific instance of a particular domain represented by the ontology. For example, an ontology for instructional design at a higher level can be defined using concepts like goals, process, content and so on and each of these sub-classes can be further defined. An instance of this ontology can be a specific instructional design for teaching a specific course. We discuss these ontologies in detail in the rest of the paper.Diversified needs emerging from different domains gave raise to a spectrum of ontology kinds   as shown in . These kinds of ontologies vary based on the degree of specification detail, formalism and expressiveness power as we move from one end to the other end of the spectrum. A detailed description of this spectrum is given in  . In essence, there are informal or lightweight ontologies on one end, primarily geared towards some sort of communication and on the other end, formal ontologies help in automated reasoning of knowledge . This paper falls in the middle and mostly uses OWL/XML Schemas to address the primary needs of communication and automation. They also provide a mechanism to use instructional design as a basis throughout the design of educational technologies. In addition, two key future research directions that motivate the need for ontologies are:-Design of personalized learning environments for a diversified range of learners, teachers and subjects in India and across the globe -Design of technologies that allow students to explicitly justify their answers through reasoning and provide a debugging environment through automated reasoning Additionally, scope is another critical factor that can be used to classify ontologies at different levels of granularity. A distinction is made between upper ontologies that describe general-purpose concepts and their relationships, domain ontologies that define domain-specific concepts, task ontologies that specify domain-specific activities and application ontologies that instantiate domain ontologies and integrate task ontologies for a particular application . Ontologies for Adult Literacy Instructional DesignIn this section, we discuss our notion of ontology for adult literacy instructional design in relation to existing literature. The term ontology is used in a variety of ways in the literature  . A commonly used definition of ontology in computer science comes from Gruber , where he defines an ontology as \"a formal, explicit specification of a shared conceptualization\". This definition was further characterized and explained by several researchers   as:-formal, the ontology should be represented using a formal language processable by machines and tools This characteristic will allow us to represent instructional design knowledge in machine-processable form to facilitate automation -explicit, by using different types of primitives and precisely stating different concepts and axioms defining the ontology Instructional design for adult literacy is embedded in IPCL and primers; ontologies will help in making it explicit -shared, the ontology is meant for a group of stakeholders within a community belonging to a specific domain or sub-domain To the best of our knowledge, we could not find any ontologies that are even remotely connected to adult literacy in India. But we searched the literature for various ontologies focusing on different kinds of educational knowledge and give a few examples here. An ontology for literacy was proposed in the context of intelligent tutoring systems way back in 1999 . We then looked into some upper ontologies and found an example curriculum ontology devised by BBC for the national curricula on UK focusing on three topics (Algebra, Geometry, Formula), level (KS1, KS2, KS3, GCSE) and different fields of study (Maths, English, Science) (BBC, 2016). A comprehensive ontology that models several learning theories is presented in  where the idea is to have solid pedagogical basis for intelligent tutoring systems. Heiyanthuduwage et al. have analyzed 14 ontologies developed by different institutions for learning design and proposed an OWL-2 learners profile . One of the earliest ontologies developed by Mizoguchi focuses on creating a task ontology to facilitate reuse of problem solving knowledge . We came across several ontologies focusing on particular kind of instructional design; for example, a mobile learning ontology was designed for abductive science inquiry style of instruction . An ontology for learning scenarios based on collaborative learning theories in  and one focusing on gamification is presented in ). There were other set of ontologies focusing on specific subject matter, like word problems in mathematics , software engineering body of knowledge . In addition to these kinds of ontologies, there are different kinds of ontologies developed for learning content , learning design based on IMS LD standard , a context ontology for bridging the gap between learning content and learning design . There were ontologies to represent learning object  and learning design repositories  to facilitate search and retrieval of learning resources on the web.However, none of these ontologies cater to the need of scale and variety inherent in the problem domain and are not driven by patterns motivating the need for our proposed work. In the next section, we will briefly discuss the approach for development of ontologies. Development of OntologiesOntology development has matured in the last few decades from being a research topic to even the discipline of ontology engineering with several approaches . Specifically, ontology development methods supported with tools became quite popular in the recent times and prot\u00e9g\u00e9 is one of the exemplar examples to illustrate this case. In their Ontology Development 101, Noy and McGuinness proposed an iterative approach for building ontologies consisting of several activities that need not be sequential (i) determine scope (ii) consider reuse (iii) enumerate terms (iv) define classes (iv) define properties (v) define constraints (vi) create instances. An important conclusion from their work is \"there is no single correct ontology for any domain. Ontology design is a creative process and no two ontologies designed by different people would be the same\" (N. .We use OWL 2, a W3C recommendation that refines and extends OWL, the Web Ontology Language for representing knowledge in the semantic web . OWL 2 is based on earlier version of OWL, extends RDF and is also compatible with XML. According to Krotzsch, OWL serves as a descriptive language for expressing expert knowledge in a formal way and as a logical language for drawing conclusions from that knowledge . Accordingly, OWL 2 allows ontology engineers to represent knowledge using various representations like RDF/XML, OWL 2 XML, Functional Syntax, Macnhester Syntax, Turtle as shown in  with each of the methods having different expressive power and reasoning abilities . The choice of ontology representation is primarily decided by ontology engineers based on the requirements and needs of the domain . We confine ourselves to the descriptive use of ontologies and use OWL/XML for representing ontologies in this paper. We see three major directions for developing ontologies from a synthesis of the literature (i) manually by expert(s) for specific purposes following a varied set of processes from light-weight to a rigorous standard process (ii) semi-automatic way of developing ontologies, where a part of the ontology is developed manually and a part is automatically retrieved using text mining, natural language processing and other machine learning techniques (iii) fully automatic development, where the ontologies are derived using ontology learning approaches.We follow a simple process for developing ontologies in this paper based on existing literature. The first step in the process is to determine the requirements from the ontology, which is driven by the set of eLearning Systems to be developed in our case. The next step is to figure out the scope of the ontology drawing a boundary for what is within and outside the scope. Once the scope is defined, the next step is to identify any existing ontologies that can be used for creating the ontology. There are several search engines like SWOOGLE 6 and ontology servers like OntoLingua 7 s for searching existing ontologies.We discuss the ontologies we adapted from existing literature in the next section. Once the suitable sources for ontologies are defined, the next step is to use a standard approach to identify the concepts, relationships between the concepts, define properties, constraints and instances using an appropriate representation language like OWL/RDF. An important distinction of this process from the standard ontology development methodologies is the use of patterns as one of the critical sources for building ontologies. The patterns themselves are discovered after extensive discussions with domain experts; rigorous analysis of literature and analyzing existing applications that are built in the domain. We have extensively discussed with domain experts from NLMA; analyzed literature on adult literacy and instructional design as a source for our patterns. We also analyzed several eLearning Systems developed by TCS for 9 Indian languages before creating the patterns. We use these patterns as one of the primary source for creating the ontologies. We also consider other literature from the instructional design space as input to our ontologies. The output of this entire exercise of conceptualization and implementation is a set of ontologies. The evaluation of this ontologies is carried out by developing a set of applications based on these ontologies and assessing whether the domain requirements have been met or not.  shows a part of how we devised scope for our instructional design ontology framework. We detail our framework in the next section. IDont -An Ontology Based Framework for Modeling Instructional DesignIs teaching science the same as teaching mathematics? Does teaching in a country like US and India same? Can we use the same method for teaching different kinds of learners? The answer for most of these questions is no. There have been tremendous efforts in trying to come up with several standards in the space of educational technologies such as SCORM for learning objects , IMS-LD for learning designs , IEEE LOM for learning objects  and enormous research , platforms and tools  surrounding these standards. However, despite significant progress, most of the promises in educational technologies seem to be unfulfilled . We summarize the following major pitfalls:-One-size-fits-all -There are hundreds of learning theories in the literature.Attempts towards coming up with a unified way of dealing with them turned quite complex denting the success of the initiatives. -End-to-end automation -Several attempts have been made to automate different aspects of education, ranging from modeling learning theories to automatically generating learning environments and this focus on end-toend automation turned futile in the most of the attempts . For instance, in the case of IMS-LD, even though not stated explicitly, this goal of end-of-end automation resulted in complex authoring ). -Administration and Management -While it is important to handle and ease the job of teachers in administration and management activities for which several Learning Management Systems were developed, linking the instructional design to LMS has increased further complexity with the existing approaches.We learn from these experiences and propose a framework for modeling instructional design using ontologies based on patterns. The design rationale for IDont is as follows:-Simplicity & Separation of concerns approach -We strongly advocate the separation of concerns principle  to model ontologies. The core idea is to have smaller multiple ontologies for different aspects of instructional design such that they can be adapted, extended and reused in other contexts.  shows how different aspects can be separated as components having explicit interfaces such that they can be connected with other aspects and customized for a specific learning situation. -Leverage and Reuse existing ontologies -When designing new ontologies, ontological engineering suggests the utilization, adaption and extension of existing ontologies (learning objects, learning designs). -Technology Design -The framework should support the creation of a platform and authoring tools to explicitly capture and model all the ontologies. -Extensibility and Customization -These are two major criteria for the framework as most of the times the ontologies have to be customized and extended for the specific domain. There should be a provision in the framework such that existing ontologies should be replaced with custom ontologies with minimal effect on the overall framework. -Iterative and Collaborative Approach -The design of this framework should follow an iterative approach and must consult different stakeholders (such as teachers, learners, instructional designers and so on) during the process. -Internationalization is required for both ontologies and tools that support the creation of ontology instances.The core premise of this framework is to systematically model instructional design using different aspects like context, goals, process, content, evaluation, environment. We distinguish between two kinds of instructional design knowledge, one is at a conceptual level that maps with existing learning methodologies and the other is at a technical level to facilitate semi-automation of eLearning Systems. In this context, the core idea of IDont is not to define complete ontologies but to point to several possible modular ontologies that are required for systematic modeling of instructional design. As such, most of the aspects of IDont are optional and can be configured based on specific purposes and learning situations.  shows an overview of the IDont framework. The key inputs come from a set of instructional design requirements that drive selection of appropriate instructional design models, which are captured as patterns in our approach. We do not specify the exact ontologies for instructional design but have placeholders for different aspects. With the advent of several ontology repositories, an instructional designer or ontology engineer can either extract the required ontologies for specific instructional design model from existing literature or create a new one. This generic instructional design stitched from existing or new ontologies can be customized with domain ontologies and is further realized by specific instances like ID1, ID2 ... IDn.   presents an overview of IDont framework for adult literacy. Even though we show several ontologies in the diagram, we focus our discussion on goals, process and content ontologies. We briefly explain the important ontologies of our framework as follows:A. ContextOntology -Context plays a significant role in IDont as it allows for modeling of various aspects related to a particular learning situation. The notion of learning context was proposed in LOCO ontology to bridge the gap between learning design and content consisting of domain specific information . However, in this framework, we articulate context in a broader view that encompasses several pointers to all other ontologies. This is a meta-ontology that essentially captures the basic information related to all other aspects of instructional design such that each of these aspects can be potentially (re)used. As shown in , ContextOntology has metadata associated with it along with context information related to various aspects of instructional design. ContextOntology specifies how a ProcessContext achieves goals using ContentContext delivered through EnvrionmentContext following EvaluationContext and performed by RolesContext.B. GoalsOntology -This ontology formalizes the notion of goals (which can be instructional goals, learning goals or even learning outcomes). The details of how it is defined are left to the specific instance. Some properties associated with goals are hasName, hasPriority, hasPrerequisites, hasEvaluation, isAchievedByProcess. The GoalsOntology points to the process through which these goals will be achieved, target competencies, the instructional material that is required and the evaluation to be performed. Consider the scenario of creating goals for K12 students, and goals are prescribed by education boards.Teachers can potentially reuse these goals if captured in the form of Goal-sOntology. As the evaluation related to these goals is separately captured, it can be reused as well. We prescribe to the idea of goal-driven instruction as part of our framework irrespective of instructional design models. We detail GoalsOntology in the latter part of this paper in Section 5.1.C. RolesOntology -The success of instructional design depends on several people who perform their roles in the process. Hence, it is important to capture the roles, their responsibilities and how the different aspects of instructional design should be adapted according to the needs of specific roles. The two most important roles are that of teacher and learner. The crucial knowledge about a learner is captured through learner profiles consisting of several attributes. Generic roles in instructional design are captured first and then modified based on specific learning situations. Mapping of goals with competencies of roles can also be done in this ontology. Most of the roles are further associated with teams and in that case the role of the team as well as individual persons is modeled separately along with roles performed by agents. This ontology should also have metrics to trace from goals to evaluation.D. ProcessOntology -The crux of IDont framework is the ProcessOntology that captures the instructional design process, and relates to all other ontologies and practically executes the process. In the literature, Learning Design is discussed heavily, in particular IMS Learning Design  and received criticism as well . Ontologies for modeling learning design are presented in . Based on our prior experience with adult literacy instructional design, IPCL and our future goal to introduce reasoning into educational technologies, we proposed the ProcessPattern -(play, act, scene and instruction) . Each lesson is organized as hierarchy of pasi with instructions where concrete activities are performed based on Merill's principles of instruction in this particular instance. This instruction actually points to ContentOntology and associates required content for the respective instruction. This nomenclature allows us to systematically capture the knowledge of instructional design process and potentially reduce technological effort. This hierarchy has similarities to IMS LD but has variations to align with patterns for adult literacy instruction. We will present the ProcessOntology in Section 5.2. E. ContentOntology -This ontology allows for modeling of instructional material in a particular learning situation. There is extensive research on ontologies for learning objects and we use the ALOCoM ontology  as base for content aspect of our framework. However, for adult literacy instructional design, we have used fcrmt (facts, cases, rules, models, and theories) structure  as discussed in Section 5.3. So the ContentType of ALOCoM also includes fcrmt to support reasoning. The ContentOntology is closely associated with other ontologies and strongly with the ProcessOntology.F. EvaluationOntology -What if the most common evaluations of instructional design are captured and an instructor can customize them based on his or her requirements? The main intent of this ontology is to capture evaluations as independent knowledge and link them with goals through Con-textOntology. This separation makes it easier to perform different kinds of evaluations for the same set of goals. This ontology captures the details of evaluation and has a direct relationship with GoalsOntology which is connected with ProcessOntology.G. EnvironmentOntology -The final execution of instructional design happens in an environment. Consider the scenario of a lesson to teach about shapes to K2 students. Considering that the students first listen about shapes in a ClassroomEnvrionment and if a teacher wants to use computers, then most of the other aspects of instructional design remain same except the environment which changes to ComputerEnvironment. Separating the environment from the rest of instructional design makes it easier to run the learning situation in different environments similar in spirit with software deployment.H. DomainOntology -This ontology mainly articulates and customizes key aspects of instructional design with respect to a specific domain and provides a domain-specific version of ontology. In particular, the various subontologies and properties of these ontologies will have detailed associations when mapped to a specific domain. For example, the ContentOntology will have strong co-relation and mapping with content in the domain. We present a domain-specific ontology for adult literacy in 6.1.There can be several other ontologies like ActivitiesOntology for capuring various activites that are suported in the instructional design, WorkflowOntology to model the tedious workflows in education, FeedbackOntology to capture continuous feedback of the instructional design, OrganizationOntology focusing on characteristics of the organization, ResourcesOntology, having pointers to specific resources like text, audio, video and so on. In our analysis of instructional design literature, we strongly see that it is virtually impossible to capture all kinds of instructional design models and any attempt towards it turns to be futile. However, the main intent of our framework is to use a separation of concerns approach to systematically capture various aspects of instructional design through ontologies.We discuss the specific ontologies that are developed as part of IDont framework. Our attempt is not to present complete ontologies but to design educational technologies in sync with instructional designs and for scale and variety. We also include several entities in the ontologies for future use rather than just current needs. We also rely on the practice of adapting existing ontologies and create new entities only if required (N. . We discuss some core ontologies of our framework in the coming sections and illustrate them through adult literacy case study. An ontology for modeling goalsThe primary goal of any instructional design is to find ways to support learners in achieving their learning goals . Based on the pattern discussed for goals in , we present an ontology for representing instructional goals in this section based on revised Bloom's taxonomy  .  shows a part of GoalsOntology 8 developed using prot\u00e9g\u00e9 9 tool from Stanford. The priority of the goal is described using GoalPriority, progress through GoalProgress, deadline through the property goalDeadline. An important sub-class is to classify the goal according to a taxonomy. The class GoalClassification is further divided into two classes BloomTaxonomy and ABCD.The BloomTaxonomy is further divided into KnowledgeDimension and Cog-nitiveProcessDimension as per revised Bloom's taxonomy. The knowledge can be classified as FacutalKnowledge, ConceptualKnowledge, ProceduralKnowledge and MetaCognitiveLevelKnowledge with increasing levels of higher order levels of thinking. This is in sync with the ContentOntology that will be discussed in next section 5.3. The CognitiveProcessDimension is the most commonly used way to classify goals as per Bloom's taxonomy. It has six levels Remember, Understand, Apply, Analyze, Evaluate, Create and each of them have several verbs specifying the activities.Several object properties are shown in  connecting different concepts in the ontology. Priority of the goal can be captured using goalPrior-ityLevel, competency through goalCompetencyLevel and goalKnowledgeLevel can have a range of values from the KnowledgeDimension and maps to the fcrmt pattern. Every goal should have a goalDeadline and its progress is monitered through goalProgress. A goal also has hasPrerequisites, previous-Goal and nextGoal. This ontology is connected to ProcessOntology through isAchievedByProcess, ContentOntology via usesContent, EvaluationOntology through hasEvaluation and runsInEnvironment.In addition, there are several data properties that are associated with the ontology. For example, goalDeadline stores the deadline as dateTime. The goal itself can be described using goalText, goalImage, goalAudio, goalVideo, goal-Metadata. These data properties store specific information that can be later used for (semi-)automatic generation. GoalGranularity is another critical class that is specific to our instructional design as we have a goals hierarchy akin to play, act, scene, instruction pattern. In addition to the standard concepts, the ontology also has concepts for GoalPattern consisting of properties shown in . For example, SourceOfPattern is a data property that specifies the source of the patterns, Trade-Offs specifies the issues that might occur using this pattern. In our case, we realized that if specifying goals requires so much of effort, the entire exercise will be a burden for teachers and instructional designers making it a futile effort in the end. Hence, we have minimal mandatory properties with scope for using extended properties only if required. The ProcessOntology 10 is a core ontology for specifying instructional process and is closely associated with several other ontologies. As shown in , the ontology is divided into three conceptual sections at a higher level (i) learning, focusing on concepts that map to the underlying learning methodologies (ii) metadata consists of information about the process in general (iii) user interface to declaratively specify a few aspects of the eLearning System. The ProcessOntology is based on ProcessPattern  and its primary purpose is to achieve goals specified in the GoalsOntology and is connected through hasAssociatedGoal property. These goals have to be achieved using content specified via ContentOntology connected through the object property usesContent. Similarly, usesEvaluation, performedbyRole, runsInEnvironment connect this ontology to Evalua-tionOntology, RolesOntology and EnvironmentOntology respectively. This ontology has several data properties like title, description, metadata, noOfPlays, noOfScenes, noOfInstructions. One important property is hasTimeLimit that specifies the time limit for an activity, instruction, scene, act, play. Guidelines is an important concept that we use for giving instructions to learners during their interaction with the eLearning System at different levels of granularity specified using PlayGuidelines, ActGuidelines, SceneGuidelines, Instruc-tionGuidelines, ActivityGuidelines. For example, a guideline from a teacher might be \"Everybody look at the screen and observe how the two syllables are combined together to form a new word\". Separating this information provides the flexibility to change guidelines. This can be specifically used to change medium of instruction in an eLearning System. A language like Hindi can be taught using Telugu as medium of instruction by changing the guidelines in the entire system. An ontology for modeling instructional processesThe base InstructionalDesignModel can be specified as MerrillModel or any other instructional design model from the literature. We use MerrillModel as it is based on first principles of instruction distilled from several instructional designs . Then each lesson is modeled using a set of plays (GenericPlay) that are divided into acts (GenericAct), which are further divided into scenes (GenericScene) and instructions (GenericInstruction). We have identified different kinds of acts for adult literacy instruction which include  We inferred these acts from adult literacy eLearning Systems that are tested on the field. There are different kinds of scenes SimilarSoundsScene, SimilarSyllablesScene, InspectingSyllableBankScene, Syl-lableFormationRulesScene, FamilarWordsScene, SyllableBannerScene, Form-ingWordsScene under each act. Each scene further has instructions which have direct activities for facilitating learning. Each instruction follows one or more principles and can have one or more activities. We specify Merrill's first principles of instruction using FirstPrinciples that is further divided into IntegrationPrinciple, ActivationPrinciple, DemonstrationPrinciple, ApplicationPrinciple, DemonstrationPrinciple. Activity is one of the most commonly used concept in the space of instructional design and we model that using GenericActivity. We incorporate two kinds of activities from the literature LearningActivity and SupportActivity. But we also model four kinds of additional activities StructureActivity, GuidanceActivity, CoachingActivity and ReflectionActivity to accommodate Merrill's inner circle of structure-guidancecoaching-reflection. Modeling these activities as concepts allows us to change these activities based on learner styles or instruction styles. In addition, In-terpretedActivity and MoniteredActivity help from evaluation perspective.The current ontology also has basic concepts for UserInterface such as An-imationStyle, ColorTheme, AnimationSpeed, Language, Background. The instances of these concepts will help in configuring the user interface of eLearning Systems for adult literacy based on specific requirements.One principle behind this ontology is not to use all the classes and properties but to further filter this ontology to the specific needs and use only a fragment of the ontology in order to reduce the burden on the teachers and instructional designers. For example, if a course has 1000 instructions in total, then specifying principles for all of these instructions might be a burden and an alternative could be to make this property optional at instructional level but mandatory at a scene or act or play level. The ContentOntology 11 ontology is primarily derived from existing literature on learning objects and specifically the ALOCoM ontology  along with the ContentPattern elaborated in . As shown in , this ontology includes four core concepts ContentType, ContentFragment, ContentObject, LearningObject. The raw data in the form of text, audio, animation, video are concepts in ContentFragment and Con-tentObject is an aggregation of several content fragments. This ontology is further refined in terms of ContentType, which includes Facts, Cases, Rules, Models, Theories, which form the CoreType. In ExtendedType, there are further concepts derived from the literature . Essentially, they capture learning objects at a higher level of abstraction. Another important concept is LearningObject which has the sub concepts of PlayObject, ActObject, SceneObject, InstructionObject. These concepts are connected to respective elements in ProcessOntology. An ontology for modeling instructional materialThere are other ontologies for specifying Roles, Evaluation, Environment that are part of instructional design ontology but defining those ontologies is beyond the scope of this work. The RolesOntology is an interesting one with roles like Teacher, Mentor, TeachingAssistant, Coach and so on and can be mapped to different kinds of activities in the ProcessOntology. As an example, learning styles and teaching styles may be used in the role of learner and teacher in RolesOntology. In the next section, we will briefly discuss a domain ontology for adult literacy. EvaluationThere exist several ways of evaluating ontologies in the literature . Broadly, these approaches can be classified as (i) manual, mainly driven by human interventions, either experts or users (ii) automated approaches and (iii) semi-automated approaches that fall in between. To reiterate, the goal of this paper is to facilitate scale and variety during design of educational technologies rather than improving quality of instruction through knowledge representation using ontologies. To this end, we presented an ontology based modeling framework in this paper rather than an ontology. As part of our evaluation, (i) we demonstrated our framework through adult literacy case study in detail throughout the paper (ii) we also present a domain ontology that is an instantiation of the proposed framework for adult literacy in this section (iii) the ontology framework was used as a base for creation of a platform  for generating eLearning Systems for adult literacy in India. This platform was further used to generate 9 eLearning Systems (Chimalakonda, 2017) but in principle can be used to create thousands of eLearning Systems. A Domain Ontology for Adult LiteracyA distinction has been made in the literature between domain and application ontologies . Domain ontologies are aimed towards defining concepts pertaining to a specific domain like \"adult literacy for Indian languages\" whereas application ontologies are further refined to specific needs of an application, in our case it can be an eLearning System for a particular language. According to Census 2011, there are about 29 languages spoken by more than a million people, 60 languages by more than 100,000 people and about 122 languages by more than 10,000 people speaking it in India. Of these, there are 22 official languages. A fundamental tenet of Indian languages is that they share a common phonetic base . This commonality across a family of Indian languages provides a shared domain that can be used for representing domain knowledge as an ontology. Based on this premise, NLMA has come up with IPCL, a uniform methodology as explained in Section ??, which is the base for creating instructional material for all Indian languages. In Indian languages, the term \"aksharas\" is used to refer to alphabet. This akshara refers to a sound that is formed of sounds of vowels and consonants . Being invariant of position is an interesting characteristic of akshara that holds for all Indian languages .  [left] shows an ontology of Indic scripts for literacy, primarily focusing on the structure of syllables in the language. The core concepts of the ontology include Syllable denoting the visual representation of an akshara or a fragment of it. This is further specialized into SimpleSyllable, CompositeSyllable and SpecialSyllable. Every concept in this ontology have two properties hasRepresentation and hasResources refering to Representation and Resources respectively. Representation has five other concepts to systematically capture a syllable. [right] shows the core structure of a syllable in Indian languages. The syllable itself is composed of CoreSymbol, LeftSymbol, Right-Symbol, TopSymbol, BottomSymbol. Each of these concepts store the respective visual fragments of the syllable at the relative positions. For example, a base consonant like \u0915 can be modified using any of the vowel modifiers from left \u093f\u25cc to give \u0915, adding the right symbol \u25cc\u0940 to \u0915 results in \u0915 . Similarly, adding top symbol \u25cc\u0947 gives \u0915\u0947 and bottom symbol \u25cc\u0941 results in \u0915 \u0941 . The composition of these symbols is represented as CompositeSymbol. The most important property of Indian languages is the broad choice of each of these symbols giving rise to an entire alphabet for a particular language. [right] shows how vowel modifiers can be applied on four sides of a base consonant to give a set of composite symbols in a language. For most of the Indian languages, the number of vowel modifiers is 12 eventhough there are a few languages where the number can be less. Similarly, the number of consonants will vary from 12 to 36 for different languages.The concept of Resources in the ontology helps in storing the data for respective syllables and phonemes in the form of Text, Audio, Image. We have observed that most of the eLearning Systems for Indian languages developed today rely on images for storing and displaying language aspects making it difficult to change the system. However, in our ontology we make an attempt to systematically separate different aspects to facilitate variety for a multitude of languages. The ontology also has the concept of Vowel, that is further divided into SimpleVowel and CompositeVowel. They are bound to Syllable through the property hasVowel. Similarly, Consonant class represents the consonants in a given language and can be either SimpleConsonant or CompositeConsonant. Most of the Indian languages have ConsonantConjuncts like \u0c15 ,\u0c15 ,\u0c15 ,\u0c15 ,\u0c15 in Telugu language, whcih are special symbols formed with a number of syllables. Modifier is the core class for representing different modifiers like VowelModifier and SpecialModifier. VowelModifier in general consists of several signs often called as dependent vowels \u25cc\u0901 , \u25cc\u0902 , \u25cc\u0903, \u25cc\u093c , \u25cc\u093e, \u093f\u25cc, \u25cc\u0940, \u25cc\u0941 , \u25cc\u0942 , \u25cc\u0943 , \u25cc\u0944 , \u25cc\u0945 , \u25cc\u0946 , \u25cc\u0947 , \u25cc\u0948 , \u25cc\u0949, \u25cc\u094a, \u25cc\u094b, \u25cc\u094c, \u25cc\u094d . In Devanagari, there are twelve signs which when composed with consonants give rise to a number of composite syllables and is often called Barakhadi because of 12 modifiers. In addition to these, there are several special modifiers specific to a language. We have modeled Phonemes in similar lines to Syllables through respective concepts. Numeral class denotes the representation of symbols \u0966 \u0967 \u0968 \u0969 \u096a \u096b \u096c \u096d \u096e \u096f in a particular language. We did not get into writing part in detail even though we have left scope for further extension of the ontology. This entire exercise of creating detailed ontologies for adult literacy is to systematically specify different parts such that they become source of variety to facilitate semi-automatic development of eLearning Systems.We summarize the conclusions and future directions of our work in the next section. Conclusions & Future WorkInstructional Design is one of the fundamental pillars of educational technologies and forms the basis for rest of the activities that drive effective instruction. Berger defines instructional design as a \"systematic development of instructional specifications using learning and instructional theory to ensure the quality of instruction\" . There are over 100+ instructional design theories in the literature catering to a diversified range of needs in education domain . In this paper, we motivated the need for modeling instructional design knowledge through ontologies to address scale and variety inherent in the domain. The key premise of the research presented in this paper is to systematically model different aspects of instructional design as modular ontologies such that these modular ontologies can be composed together to represent an instructional design. We also showed how changing any of these ontologies will result in a variant of the instructional design. We specifically presented an ontology for modeling goals, an ontology for modeling instructional process and an ontology for modeling instructional material. We demonstrated each of these ontologies through adult literacy case study which requires thousands of similar but distinct eLearning Systems to be developed. The systems that are developed based on these ontologies are made available at  and transferred to National Literacy Mission of Government of India.As pointed by Noy, \"there is no single correct ontology for any domain. Ontology design is a creative process and no two ontologies designed by different people would be the same\" (N. . This leads to a natural limitation of our approach as the proposed ontologies are only placeholders for different aspects of instructional design. We have extended and created several ontologies in our ontology framework. However, by definition, every domain can have several perspectives and hence several ontologies. In our ontology framework, we have introduced the notion of meta-ontologies for representing high level aspects of instructional design like process and content, which are then customized for specific cases of Merill's First Principles of Instruction and Bloom's taxonomy. With this context, the following are some potential directions for further research:-Ontologies for domains beyond adult literacy. The first direction of future work is to apply the proposed framework for school education and skill education. We are currently working on adapting our ontologies to model skill curriculum, specifically focusing on vocational skills. -Several ontologies other than goals, process, content that were introduced in the ontology framework have to be extended and created in full detail as a natural extension of the framework. -While we have built tools 12 that help in development of systems, the platform generates eLearning Systems specific to adult literacy in India. There is a need for tools that can generate tools to generate tools. -Creating collaborative, distributed and agile environments for domain and subject matter experts to create, share and disseminate their ontologies is a critical future step towards design of educational technologies for scale and variety.\n###\n"}
{"summary": "Has model: Integer programme (IP)/Integer program (IP)\nHas heuristic: Iterated beam search (IBS)\nhas research problem: Simple assembly line balancing problem (SALBP)\nhas system specification: Random-Access-Memory/Processor\nhas instance: Wee-Mag/Sawyer/Buxey/Source\nHas value: 2GB\nhas name: AMD Athlon 64 X2\nSource: https://assembly-line-balancing.de/", "text": "#Properties\nHas model, Has heuristic, has research problem, has system specification, has instance, Has value, has name, Source\n#Text\nThe simple assembly line balancing problem (SALBP) concerns the assignment of tasks with pre-defined processing times to work stations that are arranged in a line. Hereby, precedence constraints between the tasks must be respected. The optimization goal of the SALBP-2 version of the problem concerns the minimization of the so-called cycle time, that is, the time in which the tasks of each work station must be completed. In this work we propose to tackle this problem with an iterative search method based on beam search. The proposed algorithm is able to obtain optimal, respectively bestknown, solutions in 283 out of 302 test cases. Moreover, for 9 further test cases the algorithm is able to produce new best-known solutions. These numbers indicate that the proposed iterative beam search algorithm is currently a state-of-the-art method for the SALBP-2. The SALBP-2The SALBP-2 can technically be described as follows. An instance (T, G, m) consists of three components. T = {1, . . . , n} is a set of n tasks. Each task i \u2208 T has a pre-defined processing time t i > 0. Moreover, given is a precedence graph G = (T, A), which is a directed, acyclic graph with T as node set. Finally, m is the pre-defined number of work stations which are ordered from 1 to m. An arc l i,j \u2208 A indicates that i \u2208 T must be processed before j \u2208 T . Given a task j \u2208 T , P j \u2282 T denotes the set of tasks that must be processed before j. A feasible solution is obtained by assigning each task to exactly one work station such that the precedence constraints between the tasks are satisfied. The objective function consists in minimizing the so-called cycle time. The SALBP-2 can be expressed in the following way as an integer programming (IP) problem. min z(1) subject to:x is \u2208 {0, 1} \u2200i \u2208 T, s = 1, . . . , mz > 0  This IP model makes use of the following variables and constants: x is is a binary variable which is set to 1 if and only if task i \u2208 T is assigned to work station 1 \u2264 s \u2264 m. The objective function (1) minimizes the cycle time z > 0.  The constraints (2) ensure that each task i \u2208 T is assigned to a single work station 1 \u2264 s \u2264 m. Constraints (3) reflect the precedence relationships between the tasks. More specifically, if task j \u2208 T is assigned to a work station 1 \u2264 s \u2264 m, all tasks i \u2208 P j must be assigned to work stations 1 \u2264 s \u2032 \u2264 m with s \u2032 \u2264 s. The constraints (4) ensure that the sum of the processing times of the tasks assigned to a work station 1 \u2264 s \u2264 m do not exceed the cycle time z. Note that this model is not necessarily the most efficient IP model for solving the SALBP-2. An evaluation of several different models can be found in . Solution RepresentationThe following solution representation is used for the description of the algorithm as given in Section 3. A solution S is an ordered list S = S 1 , . . . , S m of m sets of tasks, where S i denotes the set of tasks that are assigned to the i-th work station. Abusing notation we henceforth call S i a work station. Note that for a solution S to be valid the following conditions must be fulfilled:. . , n} and m i=1 S i = \u2205, that is, each task is assigned to exactly one work station. 2.For each task j \u2208 S i it is must hold that P j \u2286 i k=1 S k . This ensures that the precedence constraints between the tasks are not violated. Reverse Problem InstancesThe reverse problem instance (T, G r , m) with respect to an original instance (T, G, m) is obtained by inverting the direction of all arcs of G. It is well-known from the literature  that tackling the reverse problem instance may lead an exact algorithm faster to an optimal solution, respectively, may provide a better heuristic solution when tackled with the same heuristic as the original problem instance. Moreover, a solution S r to the reverse problem instance (T, G r , m) can easily be converted into a solution S to the original problem instance (T, G, m) as follows:3 Iterative Beam SearchAs mentioned in the introduction, the basic component of our algorithm for the SALBP-2 consists of beam search (BS), which is an incomplete derivative of branch & bound . Initially BS has especially been used in the context of scheduling problems (see, for example, ). To date only very few applications to other types of problems exist (see, for example, ). In the following we briefly describe how one of the standard versions of BS works. The crucial aspect of BS is the parallel extension of partial solutions in several ways. At all times, the algorithm keeps a set B of at most k bw partial solutions, where B is the so-called beam, and k bw is known as the beam width. At each step, at most k ext feasible extensions of each partial solution in B are selected on the basis of greedy information. In general, this selection is done deterministically. At the end of each step, the algorithm creates a new beam B by choosing up to k bw partial solutions from the set of selected feasible extensions. For that purpose, BS algorithms determine-in the case of minimization-a lower bound value for each extension. Only the maximally k bw best extensions-with respect to these lower bound values-are included in B. Finally, if any complete solution was generated, the algorithm returns the best of those. Note that the underlying constructive heuristic that defines feasible extensions of partial solutions and the lower bound function for evaluating partial solutions are crucial for the working of BS. In the following we first present a description of the implementation of the BS component, before we describe the algorithmic scheme in which this BS component is used. The Beam Search ComponentThe BS component described in this section-see Algorithm 1 for the pseudo-code-is the main component of the proposed algorithm for the SALBP-2. The algorithm requires a problem instance (T, G, m), a fixed cycle time C, a beam width k bw , and a maximal number for all S \u2208 B do 10:11:13:if solution S \u2032 is complete (that is, all tasks are assigned) then14: B \u2190SelectSolutions(B ext ,k bw ) 23: end while 24: output: If B compl = \u2205 the output is true, otherwise false of extensions k ext as input. Given a fixed cycle time C and m (the number of work stations) BS tries to find at least one feasible solution. As mentioned before, the crucial aspect of BS is the extension of partial solutions in several possible ways. At each step the algorithm extends each partial solution from B in a maximum number of ways. More specifically, given a partial solution with l \u2212 1 < m work stations already filled, an extension is generated by assigning a set of so-far unassigned tasks to the next work station S l such that the given cycle time C is not surpassed and the precedence constraints between the tasks are respected (see lines 11-12 of Algorithm 1). The algorithm produces extensions in a (partially) probabilistic way rather than in the usual deterministic manner.  Each generated extension (partial solution) is either stored in set B compl in case it is a complete solution, or in set B ext otherwise (see lines 13-19 of Algorithm 1). However, a partial solution is only stored in set B ext if it uses at most m \u2212 1 work stations, and if its l-th work station is different to the l-th work station of all partial solutions that are already in B. Finally, BS creates a new beam B by selecting up to k bw solutions from set B ext of further extensible partial solutions (see line 22 of Algorithm 1). This is done in function SelectSolutions(B ext ,k bw ) on the basis of a lower bound function LB(\u2022). In the following we describe in detail the extension of partial solutions and the working of function SelectSolutions(B ext ,k bw ).Algorithm 2 Function ExtendPartialSolution(S \u2032 , l, C) of Algorithm 1 1: input: A partial solution S \u2032 , the index l of the work station to be filled, and the cycle timec rem := c rem \u2212 t j 8:Extending Partial Solutions. The generation of an extension of a partial solution S \u2032 with l \u2212 1 work stations already filled works as follows. Unassigned tasks are iteratively assigned to work station S \u2032 l until the sum of their processing times is such that no other task can be added to S \u2032 l without exceeding the given cycle time C. This procedure is pseudo-coded in Algorithm 2. At each step, T \u2032 denotes the set of so-far unassigned tasks that may be added to S \u2032 l without violating any constraints. The definition of this set of available tasks is given in line 3, respectively 8, of Algorithm 2.It remains to describe the implementation of function ChooseTask(T \u2032 , c rem ) of Algorithm 2. For that purpose let us first define the following subset of T \u2032 :This definition is such that T sat contains all tasks that saturate, in terms of processing time, the l-th work station S l . The choice of a task from T \u2032 is made on the basis of greedy information, that is, on the basis of values \u03b7 i > 0 that are assigned to all tasks i \u2208 T \u2032 by a greedy function. The first action for choosing a task from T \u2032 consists in flipping a coin for deciding if the choice is made deterministically, or probabilistically. In case of a deterministic choice, there are two possibilities. First, if T sat = \u2205, the best task from T sat is chosen, that is, the task with maximal greedy value among all tasks in T sat . Otherwise, we choose the task with maximal greedy value from T \u2032 . In case of a probabilistic decision, a task from T \u2032 is chosen on the basis of the following probability distribution:For completing the description of function ChooseTask(T \u2032 , c rem ), we must describe the definition of the greedy values \u03b7 i , \u2200i \u2208 T . In a first step a term \u03b3 i is defined as follows:Hereby, Suc all i denotes the set of all tasks that can be reached from i in precedence graph G via a directed path. This definition combines two greedy heuristics that are often used in the context of assembly line balancing problems. The first one concerns the task processing times and the second one concerns the size of Suc all i . The influence of both heuristics can be adjusted via the setting of weights \u03ba 1 and \u03ba 2 . In order to be more flexible we decided to allow for both weights a value from [\u22121, 1]. This means that we consider for each heuristic potentially also its negation. Given the \u03b3 i -values, the greedy values \u03b7 i are then derived as follows:where \u03b3 min , respectively \u03b3 max , denote the minimum, respectively maximum, values of all \u03b3 i . Note that this lower bound is inspired by splitting-based bounds for the one-dimensional bin-backing problem. The Algorithmic SchemeThe BS component outlined in the previous section is used by an iterative algorithmic scheme that is presented in Algorithm 3. Henceforth this algorithmic scheme is labelled iterated beam search (IBS). The first step consists in determining a starting cycle time C, which is computed in funcion DetermineStartingCycleTime() of Algorithm 3 asThe algorithm works in two phases. In the first phase (see lines 3-11 of Algorithm 3) the algorithm tries to quickly find a first cycle time C for which a valid solution can be found. For this purpose BS is applied with the setting k bw = 5 and k ext = 2. Note that this setting was chosen after tuning by hand. Moreover, note that the first phase only takes a fraction of a second of computation time. This holds for all instances considered in Section 4. The second phase of the algorithm iteratively tries to find a valid solution for the next smaller cycle time. In this phase, the algorithm disposes over a certain time limit for each considered cycle time.Remember that the working of BS is partially probabilistic. Therefore, BS can repeatedly be applied to the same instance with potentially different outcomes. The first five percent of the above-mentioned time limit are spent by BS applications that use the setting k bw := 10 and k ext := 5. This is done with the intention of not wasting too much computation time, if not necessary. However, if BS is not able to solve the given cycle time with this setting, the remaining 95% of the available time are spent by BS applications using the setting k bw := 150 and k ext := 20. With this setting BS is considerably slower. However, the probability of if success then C := C \u2212 1 else stop := true end if 24: end while 25: C := C + 1 26: output: cycle time C finding feasible solutions is much higher than with the setting described before. The second phase of the algorithm ends when the time limit has passed without having found a feasible solution for the considered cycle time. Experimental EvaluationIbs was implemented in ANSI C++, and GCC 3.4.0 was used for compiling the software. Experimental results were obtained on a PC with an AMD64X2 4400 processor and 4 Gb of memory. In the following we first describe the set of benchmark instances that we used for the experimental evaluation. Subsequently we present the tuning process that we conducted and the results of the proposed algorithm. Benchmark InstancesWe used the usual set of 302 benchmark instances from the literature. They can be obtainedtogether with information about optimal and best-known solutions-from a website especially dedicated to all kind of assembly line balancing problems maintained by Armin Scholl, Algorithm TuningDuring preliminar experiments we realized that parameters k bw and k ext have a rather low impact on the final results of Ibs. In other words it is easy to find a reasonable setting for these parameters quite quickly. Their setting dynamically changes during a run of the algorithm as specified in Section 3.2. On the contrary, parameters \u03ba 1 and \u03ba 2 (see Eq. 10) have a high impact on the algorithms' performance. Remember that \u03ba 1 is the weight of the greedy function concerning the task processing times, while \u03ba 2 is the weight of the greedy function concerning the number of tasks that have to be processed after the task under consideration. As mentioned before, for both parameters we allowed values from [\u22121, 1]. Instead of trying to find a good parameter setting for each instance, we decided for a tuning process based on precedence graphs, that is, we wanted to choose a single setting of \u03ba 1 and \u03ba 2 for all instances concerning the same precedence graph. For that purpose we applied a specific version of Ibs for all combinations of \u03ba 1 , \u03ba 2 \u2208 {\u22121.0, \u22120.9, . . . , 0.0, . . . , 0.9, 1.0} to all 302 instances. This makes a total of 441 different settings for each instance. The specific version of Ibs that we applied for the tuning process differs from Ibs as outlined in Algorithm 3 in that lines 14-24 were replaced by a single, deterministic, application of beam search with k bw = 150 and k ext = 20. This was done for the purpose of saving computation time. Based on the obtained results we chose the settings presented in  for the different precedence graphs. It is interesting to note that, apart from a few exceptions, the greedy heuristic based on task processing times does not seem necessary for obtaining good results. In  tuning information is provided in graphical form for a few representative examples. The y-axis of the presented graphics varies over the different values of \u03ba 1 , while the x-axis ranges over the allowed values of \u03ba 2 . Note that each graphic consists of 441 squares representing the 441 different combinations of values for \u03ba 1 and \u03ba 2 . The gray level in which each square is painted indicates the quality of the algorithm when run with the corresponding parameter setting. In particular, black color denotes the worst setting, whereas white color indicates the best algorithm setting. In some cases such as (Buxey, m = 7) and (Tonge, m = 16), as shown in , there is a wide range of good settings, which are basically all those with \u03ba 1 \u2264 0. In other examples such as (Arcus1, m = 12) and (Scholl, m = 38) it is strictly required to set \u03ba 1 to 0 and \u03ba 2 to a positive value for obtaining good solutions; see . Finally, the graphics shown in  and 1(f) indicate that even for the same precedence graph a good parameter setting might depend strongly on the number of work stations. It is also interesting to quantify the differences in algorithm performance for different parameter settings.  shows for the six cases presented in  the result of the algorithm with the best setting (column Best setting), the result of the algorithm with the worst setting (column Worst setting), and the difference (in percent) between these two settings. The results in  show that there are considerable differences in performance between the best and the worst algorithm setting. This underlines the importance of finding opportune values for \u03ba 1 and \u03ba 2 . ResultsAlgorithm Ibs was applied 20 times to all 302 instances. Herefore we used a computation time limit of 180 seconds for each cycle time, that is, Ibs was given maximally 180 seconds for finding a feasible solution for a given cycle time. In case of success, the algorithm has again 180 seconds for the next smaller cycle time, etc. Detailed results of Ibs for all 302 instances are given in  that are to be found in Appendix A. The data is, in both tables, presented as follows. The first two columns provide the name of the precedence graph and the number of work stations (m). The third column provides the value of the optimal (respectively, best-known) solutions. In case a value is not proved to be optimal it is overlined. More in detail, in 15 out of 302 cases optimality has not been proved yet. The remaining five columns are reserved for the results of Ibs. The first of these five columns contains the value of the best solution found by Ibs over 20 runs. In case this value is presented with a gray background, a new best-known solution has been found. On the other side, if this value is marked by an asterisk, the obtained result does not reach the value of a best-known solution. In all other cases the values correspond to the values of best-known solutions. The second column provides the average over 20 runs, while the third column  contains the corresponding standard deviation. The fourth column gives the average time (in seconds) at which the best solution of a run was found, averaged over 20 runs. The fifth column provides the corresponding standard deviation. From the results presented in  (see Appendix A) we can observe that Ibs obtains best-known (respectively, optimal) solutions in 276 out of 302 cases. Moreover, new best solutions are obtained in 6 cases. This is remarkable as-despite a considerable amount of ongoing research-in the last 14 years no improved solutions have been reported. Only in 20 cases (all concerning precedence graphs Arcus1, Arcus2, Scholl, and Warnecke) our algorithm was not able to find the best solutions known. However, in most of these cases the deviation from the best-known solution is no more than one unit of cycle time. In addition to  the results of Ibs are presented in a summarized way in , in comparison to three other algorithms. TabuSearch , even though already published in 1996, still counts as the current state-of-the-art heuristic method for SALBP-2. DE rks is the best version of a differential evolution (DE) algorithm proposed in , and PNA-for is a Petri net-based heuristic published in . The last two methods are, to our knowledge, the most recently published heuristic methods for SALBP-2. Three measure are used in  for the comparison of Ibs with these three algorithms. The row labelled #opt provides the number of best-known solutions found by each method (over 302). Moreover, the row labelled mrd (%) gives the mean relative deviation (in percent) of the results obtained by the four algorithms from the best-known solutions for all 302 instances. Finally, row time contains the average computation time of the algorithms for all 302 instances. Concerning the quality of the results, we can conclude that Ibs clearly outperforms its competitors. For the correct interpretation of the computation times it has to be taken into account that the four algorithms were executed on computers with very different processor speeds. While TabuSearch was executed on a 80486 DX2-66 processor, PNA-for was run on an Athlon XP 2000+ processor with 1.67 GHz, and DE rks was run on a Pentium IV processor with 1.7 GHz. This means that TabuSearch was run by far on the slowest machine, Ibs by far on the fastest machine, and PNA-for and DE rks on comparable machines. Given the computation times in  we can savely conclude that TabuSearch is the fastest algorithm, and PNA-for is the slowest one. However, note that assembly line balancing is, in most cases, not a time-critical application. In other words, for most practical purposes it does not matter if an algorithm takes 1 minute or 6 hours of computation time.In the following we present the results of Ibs in comparison to DE rks and PNA-for in the same way as done in  and . In these works, results were presented as averages over instances based on the same precedence graph, and also averaged over Dataset1 and Dataset2. The quality of the results is given in terms of the mean relative deviation (in percent) from Results of a High-Performance VersionIn an attempt to further improve on the results of our algorithm we decided to apply a high-performance version of Ibs to all problem instances for which the optimal solution is unknown and, additionally, to all instances where Ibs-with the settings as outlined in the previous section-was not able to find a best-known solution. This high-performance version is obtained as follows. First, 1800 seconds are used as a time limit for each cycle time. Second, in line 17 of Algorithm 3 only 1% of the time limit is used (instead of 5%). Third, for each application of beam search in lines 18 and 20 of Algorithm 3 the beam width k bw is randomly chosen from [150, 250] and the number of extensions is randomly chosen from . Moreover, with a probability of 0.5 the heuristic information is-for each application of beam search-calculated using the weight values as outlined in . Otherwise, the weight values are chosen randomly from [\u22121, 1]. With these modifications we applied Ibs exactly once to all the instances of . The results of the algorithm are given in column Result. Indeed, the number of instances for which a best-known solution can not be found before is reduced from 20 to 10 instances. Moreover, the algorithm is now able to find new best-known solutions in 9 (instead of only 6) cases. Summarizing, this amounts to 283 best-known solutions found and 9 new best-known solutions obtained. In one case, (Scholl, m = 49), the new best-known solution is provenly optimal, as its value coincides with the best known lower bound. Conclusions and Future WorkIn this work we have proposed an iterative beam search algorithm for the simple assembly line balancing problem with a fixed number of work stations, SALBP-2. The experimental evalution of the algorithm has shown that it is currently a state-of-the-art method for this problem. Appart from producing optimal, respectively best-known, solutions in 283 out of 302 test cases, our algorithm generated new best-known solutions in further 9 test cases. Encouraged by the results for the SALBP-1 version of the problem (as published in ) and the results obtained in this paper for the SALBP-2 we intent to apply similar algorithms based on beam search to other assembly line balancing problems.\n###\n"}
{"text": "#Properties\nHas value, Method, Time period, has beginning, has end, Lower confidence limit, Upper confidence limit, Located in, Basic reproduction number, Confidence interval (95%), has research problem, location, Approaches, same as, description\n#Text\nSummarySelf-sustaining human-to-human transmission of the novel coronavirus (2019-nCov) is the only plausible explanation of the scale of the outbreak in Wuhan. We estimate that, on average, each case infected 2.6 (uncertainty range: 1.5-3.5) other people up to 18 th January 2020, based on an analysis combining our past estimates of the size of the outbreak in Wuhan with computational modelling of potential epidemic trajectories. This implies that control measures need to block well over 60% of transmission to be effective in controlling the outbreak. It is likely, based on the experience of SARS and MERS-CoV, that the number of secondary cases caused by a case of 2019-nCoV is highly variable -with many cases causing no secondary infections, and a few causing many. Whether transmission is continuing at the same rate currently depends on the effectiveness of current control measures implemented in China and the extent to which the populations of affected areas have adopted risk-reducing behaviours. In the absence of antiviral drugs or vaccines, control relies upon the prompt detection and isolation of symptomatic cases. It is unclear at the current time whether this outbreak can be contained within China; uncertainties include the severity spectrum of the disease caused by this virus and whether cases with relatively mild symptoms are able to transmit the virus efficiently. Identification and testing of potential cases need to be as extensive as is permitted by healthcare and diagnostic testing capacityincluding the identification, testing and isolation of suspected cases with only mild to moderate disease (e.g. influenza-like illness), when logistically feasible. SUGGESTED CITATION ResultsOur analysis indicates that it is highly likely that the human-to-human transmissibility of 2019-nCoV is sufficient to support sustained human transmission (R0>1) unless effective control measures are implemented.We judge that the most likely estimate corresponds to the smallest level of zoonotic exposure explored here (40 cases), namely R0=2.6 (Table 1 and ). Uncertainty caused by the intrinsically random nature of epidemics and the uncertainty in the level of zoonotic exposure gives a range of 1.5-3.5, assuming a total of 4000 cases by 18 th January. Central estimates of R0 for the (unlikely) scenario that the true outbreak size in Wuhan was at the lower end of the uncertainty range of our previous estimates (namely 1000 cases) vary from 1.7 to 2.6, depending on the level of zoonotic exposure. Estimates of R0 assuming 9,700 cases by 18 th January (our highest estimate from report 2) were higher, at R0=3.1 (uncertainty: 1.9-4.2), for 40 cases caused by zoonotic exposure.The only scenario which supports R0<1 requires a low number (1000) of cases overall in Wuhan by 18 th January and a very large number (200) of those cases being caused by zoonotic exposure , and even then, R0 is <1 only for our best case (most optimistic) estimate. Infection of 200 individuals with a novel virus with very limited genetic diversity would represent an unprecedently large point source zoonotic exposure event for the initial seeding of this epidemic. Current evidence of very limited genetic diversity in the published genetic sequences of the virus suggests a smaller seeding event (perhaps smaller than the 40 cases assumed in our lowest zoonotic seeding scenario) .Our baseline analysis assumes SARS-like levels of case-to-case variability in the numbers of secondary cases generated by each case (i.e. it includes super-spreading type events), and a SARS-like generation time. We also examined sensitivity to these assumptions. Assuming a shorter generation time (mean of 6.8 days rather than 8.4 days) reduces our central estimate of R0 to 2.1 (uncertainty range: 1.3-2.7), but does not change overall conclusions about the likelihood of self-sustaining human-to-human transmission. Increasing the generation time to 10.7 days results in a higher central estimate of R0 of 3.1 (uncertainty range: 1.7-4.3) but again does not change basic conclusions.  Assuming a lower level of variability in infectiousness (at the minimum level statistically consistent with SARS data ) narrows the uncertainty range of R0 but changes the central estimate only marginally: R0=2.5 (uncertainty range 1.6-2.9), for a negative binomial offspring distribution with k=0.64. Assuming a lower level of variability in infectiousness (at the minimum level statistically consistent with SARS data ) narrows the uncertainty range of R0 but changes the central estimate little: R0=2.5 (uncertainty range 1.6-2.9), for a negative binomial offspring distribution with k=0.64.If the current virus causes more cases with mild to moderate symptom severity than SARS, and these cases are infectious -a scenario consistent with some recently published data on a family cluster of cases , both the generation time and level of heterogeneity in infectiousness may be lower than for SARS. This scenario might be more realistic if a majority of 2019-nCoV cases have mild to moderate ('flulike') symptoms and both milder and severe cases are able to transmit infection onwards. This results in both a lower central estimate of R0 and a narrower uncertainty range: R0 = 2.0 (uncertainty: 1.4-2.3) for a mean generation time of 6.7 days and k=0.64.\n###\n", "summary": " Has value: 2.5\n, Method: \"Mathematical model\n, Time period: Time interval\n, has beginning: 2020-01-18\n, has end: 2020-01-18\n, Lower confidence limit: 1.5\n, Upper confidence limit: 3.5\n, Basic reproduction number: Basic reproduction number estimate value specification\n, Confidence interval (95%): Confidence interval (95%)\n, has research problem: Determination of the COVID-19 basic reproduction number\n, location: Wuhan\n, Approaches: \"Assume SARS-like levels of case-to-case variability in the numbers of secondary cases and a SARS-like generation time with 8.4 days\n, same as: https://en.wikipedia.org/wiki/COVID-19_pandemic\n, description: This research problem aims at determining the basic reproduction rate of COVID-19.\n###"}
{"text": "#Properties\nimplementation, metric, has research problem, evaluation, Approach type, Document type, Summary usage, Summary characteristics\n#Text\nThis paper presents a new generic text summarization method using Non-negative Matrix Factorization (NMF) to estimate sentence relevance. Proposed sentence relevance estimation is based on normalization of NMF topic space and further weighting of each topic using sentences representation in topic space. The proposed method shows better summarization quality and performance than state of the art methods on DUC 2002 standard dataset. In addition, we study how this method can improve the performance of supervised and unsupervised text classification tasks. In our experiments with Reuters-21578 and Classic4 benchmark datasets we apply developed text summarization method as a preprocessing step for further multi-label classification and clustering. As a result, the quality of classification and clustering has been significantly improved. II. GENERIC TEXT SUMMARIZATION METHODSNowadays the most state of the art methods of automatic text summarization which build generic summaries in the extracts form are based on Latent Semantic Analysis (LSA) . In these methods the original text is represented in the form of a numerical matrix. Matrix columns correspond to text sentences (or other fragments), and each sentence is represented in the form of a vector in the text term space. Further, LSA is applied to the received matrix to construct sentences representation in the text topic space. The dimensionality of the topic space is much less than dimensionality of the initial term space. The choice of the most important sentences is carried out on the basis of sentences representation in the topic space. The number of important sentences is defined by the length of the demanded summary.LSA performs one of the matrix decomposition algorithms on the original text matrix to construct sentences representation in text topic space, thereby bringing out the semantic connectedness present among the sentences . Singular Value Decomposition (SVD) is the traditional matrix decomposition algorithm used for LSA, wherein lower dimensional components from the decomposition are truncated. On truncation, the linguistic noise present in the vector representation is removed, and the semantic connectedness is made visible. One of the disadvantages of using SVD is that the truncated matrix will have negative components, which is not natural for interpreting the textual representation. Nonnegative Matrix Factorization (NMF) addresses this issue by constructing non-negative parts-based representation as the matrix decomposition algorithm for performing LSA .Further we describe the proposed generic text summarization method using NMF to estimate sentence relevance. And also we adduce its experimental comparison with state of the art methods using SVD and NMF. A. Proposed Generic Text Summarization MethodThe first step is the creation of a term by sentences matrix A = [A 1 , A 2 , \u2026, A n ], where each column A i represents the weighted term frequency vector of sentence i in the document under consideration. The sentence vector A j = [a 1j , a 2j , \u2026, a mj ] T is defined as: a ij = L(t ij )\u22c5G i , where t ij denotes the frequency with which term i occurs in sentence j, L(t ij ) is the local weight for term i in sentence j, and G i is the global weight for term i in the whole document. We have experimented with various weighting schemes and received the best results at usage binary local weight and entropy global weight:\u2022 Binary local weight: L(t ij ) = 1, if term i appears at least once in sentence j; L(t ij ) = 0, otherwise. \u2022 Entropy global weight (1):where p ij = t ij / F i , F i is the total number of times that term i occurs in the whole document, N is the number of sentences in the document. If there are m terms and n sentences in the document we obtain an m \u00d7 n matrix A for the document. The next step is to apply NMF to matrix A: A \u2248 W\u22c5H. Matrix W derives a mapping between the m dimensional term space and the r dimensional topic space. Each column of H represents corresponding text sentence as an additive combination of the basis topics. Thenwe normalize the topic space: A k = W\u22c5H = NormW\u22c5NormH, where NormW = W\u22c5Norm -1 , NormH = Norm\u22c5H, Norm = diag(||W ||, \u2026, ||W r ||). We use Euclidean norm for columns of matrix W (2):(2)Columns of matrix NormH correspond to n sentences in the normalized topic space. The k-th row NormH k = [normh k1 , \u2026, normh kn ] indicates weights of k-th topic in all n sentences. The greater norm of rows of NormH, the greater weights of corresponding topics in all text. Proceeding from it, we calculate topic weights as norms of rows of matrix NormH. Weight of k-th topic is (3, 4): The weighted sentences representation in the normalized topic space is matrix WeightedH (5, 6):Sentences for the summary are selected according to their sum of topic weights. Relevance of j-th sentence is (7, 8):Finally required number of sentences with the highest relevance values is selected for the summary. B. ExperimentsWe use the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) package to evaluate the proposed method . It includes measures to automatically determine the quality of a summary by comparing it to other model (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. ROUGE measures ROUGE-2, ROUGE-L, ROUGE-S, ROUGE-W is recommended to use for evaluation single-document summarization methods on datasets DUC 2001 and DUC 2002 . In the last editions of Document Understanding Conferences (DUC), ROUGE was used as an automatic evaluation method. As experimental data, we use the DUC 2002 standard dataset. This dataset consists of 533 documents and 925 model summaries.We evaluated state of the art summarization methods such as the SVD-Classic (Gong and Liu approach ), the SVD-Square (Steinberger and Je\u017eek approach ), the NMF-Generic (Lee, Park, Ahn, Kim approach ), and our proposed method based on NMF.Also we consider\n###\n", "summary": " implementation: Implementation\n, metric: ROUGE 1/ROUGE 2/ROUGE L/ROUGE SU4/ROUGE W\n, has research problem: Automatic text summarization\n, evaluation: Evaluation\n, Approach type: Graph based/Statistical\n, Document type: Multiple documents\n, Summary characteristics: Extractive\n###"}
{"summary": "structure: Knowledge Data and questionnaires/Behavioral Data/Resource Data\nparticipants: 114\nhas research problem: Dataset creation\nAttributes: browsing tracks/browsing events/gaze/browsing timeline/HTML/Screen Recordings\ndescription: HyperText Markup Language", "text": "#Properties\nstructure, participants, has research problem, Attributes, description\n#Text\nThe emerging research field Search as Learning investigates how the Web facilitates learning through modern information retrieval systems. SAL research requires significant amounts of data that capture both search behavior of users and their acquired knowledge in order to obtain conclusive insights or train supervised machine learning models. However, the creation of such datasets is costly and requires interdisciplinary efforts in order to design studies and capture a wide range of features. In this paper, we address this issue and introduce an extensive dataset based on a user study, in which 114 participants were asked to learn about the formation of lightning and thunder. Participants' knowledge states were measured before and after Web search through multiple-choice questionnaires and essay-based free recall tasks. To enable future research in SAL-related tasks we recorded a plethora of features and person-related attributes. Besides the screen recordings, visited Web pages, and detailed browsing histories, a large number of behavioral features and resource features were monitored. We underline the usefulness of the dataset by describing three, already published, use cases. questionnaires for pre-and post-tests, for instance, is a challenging task, as topic domain and item difficulty have to be well calibrated. Lastly, the logging process itself is non-trivial since available software, to our knowledge, usually only covers part of the features of interest. Therefore handcrafted, non-intrusive logging mechanisms need to be implemented manually.To the best of our knowledge, there are currently only two SAL-focused datasets available: Proa\u00f1o-R\u00edos and Gonz\u00e1lez-Ib\u00e1\u00f1ez  provide a set of 83 expert-generated learning paths on a diversity of topics. Each expert assembles a set of three web resources useful towards a certain learning goal, including a justification of their choice. However, data on real-world user behavior is not included. Gadijaru et al.  present a dataset comprised of 420 crowdsourced learning sessions, investigating the information needs on the search behavior and knowledge gain of users. Our dataset improves their contribution by presenting data gathered in a controlled lab study and it captures behavioral, resource, and gaze data. Other resources focus on either search or learning: (1) Search Focus -Datasets for the conception and optimization of search systems provide the basis for improved automatic analysis of queries , identification of user tasks , the influence of found resources on the user's viewpoints , and novel interaction methods such as conversational search ;  Learning focus -Datasets from the educational domain often explore recommendation tasks , provide data on user behavior in restricted learning environments  or specific instructional practices .Finally, there is an active area of research on predicting the memorability of visual resources  and the impact of resource modality on learning success . The scope of these datasets is usually limited to a single feature type; none of them collects user behavior information in a realistic and open, learning-related Web search scenario.In this paper, we contribute to SAL and related areas by providing a unique dataset to overcome some of the aforementioned limitations. Our SaL-Lightning dataset contains detailed recordings, pre-and post-knowledge assessments of 114 participants, interaction data on real-world search behavior, as well as resource features of a user study. This data diversity has the potential to help researchers answer diverse questions tied to the entire online learning framework, from individual psychological aspects, over usability tests and data visualization over retrieval and ranking issues existing in the technology that enables this process. We prove this claim by presenting three already published works.The remainder of the paper is structured as follows. First, we go into great detail regarding the data acquisition process before laying out how we curated the gathered information (Section 2).Afterward, we showcase already existing use cases (Section 3). Section 4 concludes with a summary and possible future work. DATASET DESCRIPTIONThis section gives an in-depth explanation of the dataset acquisition process: (1) the initial user study in Section 2.1,(2) the technical environment in Section 2.2 and (3) the detailed description of the different data subsets in Section 2.3. User Study2.1.1 Participants and Task. The participants (N=114), German speaking university students (95 female, =22.88, =2.93) from different majors were asked to solve a realistic learning task to understand the principles of thunderstorms and lightning. This topic has been used before to study multimedia learning (e.g., ) and has been chosen since it requires learners to gain knowledge about different physical and meteorological concepts and their interplay, i.e., they need to learn about causal chains of events and acquire declarative as well as procedural knowledge .The acquisition of information about such task a can be accomplished through studying different representation formats, such as text, pictures, videos, or combinations of those. This circumstance is beneficial for our goal to get a general idea about optimal multimedia learning resource design, especially in SAL scenarios.2.1.2 Procedure and Measurements. The experiment consisted of an online and a laboratory part. In the online part, which had to be completed around one week before the lab appointment, participants had to respond for the first time to the 10-item multiple-choice and 4-item transfer knowledge test based on previous work . Further, participantsworked on questionnaires regarding their achievement motivation  and their Web-specific epistemic justification beliefs . At the lab appointment, participants first completed tests assessing their reading comprehension  and working memory capacity . The participants were asked to write a first essay (t1) about the topic of the formation of thunderstorms and lightning. Afterward, they were instructed to learn about this specific topic by searching the Web in a self-regulated manner. Participants were informed about the time limit of max. 30 minutes for their web search, and that they could also end the task early. They were encouraged to use every kind of Web resource they would like.After the learning phase, they were asked to write again everything what they now knew about the topic in a free essay (t2) format. Lastly, the participants were asked to answer the multiple-choice questionnaires (t2) again followed by a questionnaire assessing task engagement  and cognitive reflection tasks . Technical environmentAll search and learning activities of participants were conducted within a tracking framework consisting of two layers.The first layer was the SMI (SensoMotoric Instruments) ExperimentCenter (3.7) software environment that enabled us to track participants' eye movements as well as their activities during Web search in the formats of screen recordings and navigation log files.For the second layer we utilized browser plugins to gather resources of all visited HTML files and adapted Talibi et al's method  to track navigation and interaction data (e.g., mouse movements). For more details we refer to . Dataset StructureThe following section describes the information per user provided by the individual data subsets. Apart from the screen recordings and HTML data, which we cannot make publicly available due to licensing restriction, all dataset parts are available under  The screen recording's video format is MP4, and they have been recorded with a resolution of 1280 720 at 30 frames per second. The audio track is not included. They are not longer than 30 minutes and start at the point in time the learning session starts. We manually cut the start of the video that showed the participants entering their session IDs. ResourceData -HTML. Since online content is not persistent, to achieve our goal to enable research on the actual data seen by the participants, we decided to record the content of each visited website, including but not limited to *html, *css, *js, and image files. Due to technical difficulties this process was not entirely successful, forcing us to fill in the gaps at later points in time. In detail, we managed to capture 87.9% of the data at the time of the study, another 4% in March 2020 and finally, another 2.5% in September 2021. For the remaining 5.6% (181 URLs) we were not able to record any data. With very few exceptions (a few websites that are not available anymore) these were search engine result pages from Google and YouTube that do not contain any learning relevant information, and when crawled at a later point in time, differ strongly from the original. For these two reasons we decided to exclude them from the dataset. For full transparency we disclosed the date of acquisition in the provided timeline.   Lastly, we captured copy and paste events and, if available, the intended target elements. The structure of the data record per event is displayed in .p_id timestamp track_id type value x y target url . The columns in the event data files for each participant chronologically displaying the browsing interaction events. 2.3.6Behavioral Data -Browsing Tracks. The browser tracking tool associates events to websites by means of tracks.Upon navigating to a website, a track is created and exists until the user navigates somewhere else within the same browser tab or closes it. This setup is geared towards realistic search sessions with multiple concurrent tabs. For each track, our dataset contains the time of creating the track, URL, and title of the website, as well as the viewport dimensions. Additionally, the data contains the lifetime of the track, as well as the time the track was active, i.e., it was displayed to the user in the active browser tab.p_id timestamp track_id url title viewport width viewport height time stay time active . The columns in the track data files, capturing information such as URL and active time for a visited website.2.3.7 Knowledge Data and estionnaires. As mentioned in 2.1.2, we measured the knowledge state of learners at multiple points in time. Additionally, through questionnaires and tests, we captured cognitive abilities and assessments of participants across the study. Thereby, several sub datasets were generated for which we provide the documentations with explanations of measured variables and, if possible, the original German items. This section will give brief explanations of these files, while detailed documentation can be found in the dataset.demo_knowledge_sum.csv: This file contains demographic information of participants and the summary of the knowledge-related scores (multiple-choice, essay) and cognitive abilities (working memory capacity, reading comprehension, cognitive reflection) (  mc_data.csv: This file contains the scores for all multiple-choice questions answered by participants before the lab session (t1) and after the search in the lab (t2). Includes also the confidence rating of participants for each question and the information if the answer was guessed.essay_data.csv: This file contains the raw essays written by the participants before (t1) and after the search (t2).internet_specific_epistemic_justification.csv: This file contains the measured Web-specific epistemic justification based on a translated version of .selfassessment_data.csv: To measure participant s self-assessed performance on the knowledge tests, we used both global self-assessment (estimated numbers of items answered correctly, estimated placement as compared to others and perceived ability to explain the concepts of the learning topic) as well as local on-item confidence rating, indicating how confident participants were that their given answer was correct.CRT_data.csv: To measure an individual s tendency for cognitive reflection, participants worked on five items of the cognitive reflection task (CRT ) translated into German. Within the dual-process model of reasoning, there is a distinction between faster responses with little deliberation and slower and more reflective responses. Solving more of the CRT items shows a higher disposition for the latter one, i.e., reflective cognition.achievement_data.csv: We used the German version of the achievement motives scale USE CASESThe following paragraphs describe three studies that were conducted based on the SaL-Lightning dataset.Knowledge Prediction. One of the most sought after tasks in SAL is the prediction of the learning outcome given a set of learning resources and, optimally, considering the learner's individual needs. The prediction of the user's knowledge would allow retrieval algorithms to present more accurate search results given a set of resources for a respective topic.Otto et al.  investigated how the layout and content of the multimedia data on the websites influence, besides behavioral and resource features, the knowledge gain of the participants.Further, they conducted correlation analyses of (multimodal) resource features and pre-/post-knowledge scores, which might indicate whether certain resources or resource types (e.g., videos) are more frequently used by novices, intermediates, or experts. Since our pre-and post-study questionnaires have measured the knowledge state both using multiple-choice questions and essays, the dataset can be used to advance the state of the art in this research area.Cognitive Abilities and Search Tasks. Pardi et al.  investigated the connection between behavioral data, namely the time spent on different information resources, and the learners' cognitive abilities and learning outcome. Therefore, they classified based on the URL and screen recordings the time participants spent on text-dominated websites (potentially accompanied by images) and the time spent on online videos. Furthermore, they analyzed the assessed working memory capacity and reading comprehension of participants in connection to their learning outcome. The learning outcome was derived from the pre-and post essays users wrote before and after their web search. Based on these data, the correlations between learning outcome (pre, post), cognitive abilities (working memory, reading comprehension), and time spent on resources (search activities, on video, on text-dominated websites) were analyzed.Metacognitive Judgments in SAL. Hoyer et al.  used the global and local confidence ratings to investigate how the evaluation of one s own knowledge changes after a learning phase. The authors found that after the 30-minutes selfregulated online learning phase, learners are able to gain knowledge and are in general capable of accurately estimating their knowledge on measures of global confidence ratings. However, there was also an increase in overclaiming, which indicates that more knowledge is claimed after learning. Additionally, the authors report an unexpected false certainty effect indicated by increased local confidence ratings given to incorrectly answered questions after the learning phase.Since accurate local metacognitive judgments are essential for controlling learning processes, this result points to a possible detrimental effect of short online learning. CONCLUSIONS AND FUTURE WORKIn this paper, we have presented a comprehensive, multilateral dataset for research in the field of Search as Learning.It enables interdisciplinary research in various disciplines related, but not limited to, multimodal information retrieval and learning psychology, and usability. The potential is underlined by three already published publications that made use of the SaL-Lightning dataset. Our plan for the future is to explore further interdisciplinary topics related to Search as Learning, in addition to the ones already mentioned. Besides the exploration of psychological phenomena based on the provided knowledge metrics, a variety of computer science applications are also unexplored, for instance, multimedia resource recommendation, web document layout analysis, analysis of user behavior during search, and query refinement.\n###\n"}
{"text": "#Properties\nHas result, has research problem, uses, has name, has version, has documentation, Has Horizontal Discretization, River Routing, Scheme method, Has Prognostic Variable, Model Family, has spatial resolution, drift of sea level height, Simulated Antarctic sea ice extent, Total Arctic sea ice extent, Sea ice off Alaska, Sea ice over the eastern part of the Siberian Basin, has horizontal resolution, Coupling With Atmosphere, Earth System Model, Carbon Cycle, has species, Has scheme, Basic Approximations, major ice sheets, Minor ice caps, Cold deep snow albedo , Bare ice albedo, Melting deep snow albedo, Vertical Physics, Critical Richardson number, Radiation scheme, Surface Air Temperature, RMSE of global mean surface air temperature against ERA-Interim reanalysis, Bias of mean surface temperature over northern hemisphere land against ERA-Interim reanalysis:, Bias of mean surface temperature over high latitudes Southern Ocean (>60\u00b0)  against ERA-Interim reanalysis, Bias of global mean precipitation against GPCP climatology, Cloud scheme, Bias of northern hemisphere (0\u00b0-60\u00b0N) annual mean cloud amount against the D2 ISCCP observations , Sea-ice volume trend in the northern hemisphere,     Sea-ice volume trend in the southern hemisphere, Trend of SST, Bias of global mean SST, Bias of global mean SSS against the WOA2009 data, Dynamical Core, Scheme Type\n#Text\nWe describe here the development and evaluation of an Earth system model suitable for centennial-scale climate prediction. The principal new components added to the physical climate model are the terrestrial and ocean ecosystems and gas-phase tropospheric chemistry, along with their coupled interactions. The individual Earth system components are described briefly and the relevant interactions between the components are explained. Because the multiple interactions could lead to unstable feedbacks, we go through a careful process of model spin up to ensure that all components are stable and the interactions balanced. This spun-up configuration is evaluated against observed data for the Earth system components and is generally found to perform very satisfactorily. The reason for the evaluation phase is that the model is to be used for the core climate simulations carried out by the Met Office Hadley Centre for the Coupled Model Intercomparison Project (CMIP5), so it is essential that addition of the extra complexity does not detract substantially from its climate performance. Localised changes in some specific meteorological variables can be identified, but the impacts on the overall simulation of present day climate are slight. This model is proving valuable both for climate predictions, and for investigating the strengths of biogeochemical feedbacks. Model componentsThe HadGEM2 Earth system model (HadGEM2-ES) comprises underlying physical atmosphere and ocean components with the addition of schemes to characterise aspects of the Earth system. The particular Earth system components that have been added to create the HadGEM2 Earth system model discussed in this paper are the terrestrial and oceanic ecosystems, and tropospheric chemistry. The ecosystem components TRIFFID ) and diat-HadOCC are introduced principally to enable the simulation of the carbon cycle and its interactions with the climate. Diat-HadOCC also includes the feedback of dust fertilisation on plankton growth. The UKCA scheme ) is used to model tropospheric chemistry interactively, allowing it to vary with climate. UKCA affects the radiative forcing through simulating changes in methane and ozone, as well as the rate at which sulphur dioxide and DMS emissions are converted to sulphate aerosol. In HadGEM1 the chemistry was provided through climatological distributions that were unaffected by meteorology or climate. Although the HadGEM2 model was designed from the outset to include the above Earth system components, they can be readily de-activated and the data they produce replaced with relevant climatological mean values. A paper (The HadGEM2 Model Development Team, 2011) describes how these components can be combined to create different configurations of HadGEM2 that are in use. Particular configurations from that paper to be noted are HadGEM2-ES which includes all the earth system components to be described here, and HadGEM2-CC which includes the all the earth system components except the gas-phase tropospheric chemistry. We also describe in this paper improvements to components of HadGEM2 which could be considered as part of the Earth system, but are not readily de-activated and therefore still required in the \"physics-only\" model configuration referred to as HadGEM2-AO. These components are: hydrology, land surface exchange scheme, river routing and aerosols. A list of the Earth system components and couplings is provided in . The individual components have been calibrated separately and evaluated in the complete Earth system set up. Terrestrial Carbon cycle TRIFFID dynamic vegetation scheme To model the exchange of CO 2 between the atmosphere and the terrestrial biosphere, and to model changes in the vegetation distribution.Ocean carbon cycle diat-HadOCC ocean biology scheme To model the exchange of carbon dioxide between the atmosphere and the oceanic biosphere This allows the concentrations of these species to vary with climate and tropopause heights. Chemistry-hydrologyThe emissions of methane from wetlands are supplied from the hydrology scheme to the chemistry scheme.The emissions and hence concentrations of methane will vary as climate impacts on the extent of wetlands Chemistry-Aerosols Sulphate oxidation scheme takes its oxidants from the interactive chemistry.The sulphur oxidation will now be affected by meteorology and climateOcean carbon cycle-DMS DMS emission now interactively generated by the ocean biology This important source of sulphate aerosol will now vary as climate change affects the plankton Vegetation-Dust Dust emissions depend on the bare soil fraction generated by the vegetation scheme Dust production will vary as climate change affects the vegetation distributionDust-Ocean carbon cycle Dust deposition affects plankton growth The supply of nutrients to the plankton varies with the dust production. This coupling also allows geo-engineering experiments to be simulated. Underlying physical modelThe physical model configuration is derived from the HadGEM1 climate model and references therein) with improvements as discussed in The HadGEM2 Development Team (2011) and , and is only described briefly here. The atmospheric component uses a horizontal resolution of 1.25 \u2022 \u00d7 1.875 \u2022 in latitude and longitude with 38 layers in the vertical extending to over 39 km in height. The oceanic component uses a latitudelongitude grid with a zonal resolution of 1 \u2022 everywhere and meridional resolution of 1 \u2022 between the poles and 30 \u2022 latitude, from which it increases smoothly to 1/3 \u2022 at the equator. It has 40 unevenly spaced levels in the vertical.The addition of Earth system components to the climate model introduces more stringent criteria on the physical per-formance. For instance, the existence of biases in temperature or precipitation on the regional scale need not detract from a climate model's ability to simulate future changes in climate; however such biases can seriously affect the ability of the Earth system model to simulate reasonable vegetation distributions in these areas. Hence a focus for the development of the physical components of HadGEM2 was improving the surface climate, as well as other outstanding errors such as El Ni\u00f1o Southern Oscillation (ENSO) and tropical climate. Details of the major developments to the physical basis of the HadGEM2 model family are described in and The HadGEM2 Development Team (2011). Land surface exchange schemeThe land surface scheme in HadGEM2 is MOSES II , from which the JULES scheme is derived. This was targeted for development since surface exchange directly influences the vegetation scheme and the terrestrial carbon cycle. Even without land carbon-climate feedbacks, these\n###\n", "summary": " has research problem: CMIP5/Global climate modelling\n, has name: HADGEM2-A\n, has documentation: http://www.metoffice.gov.uk/media/pdf/8/f/HCTN_73.pdf\n, Has Horizontal Discretization: PoleSingularityTreatment/SchemeMethod/SchemeOrder/SchemeType\n, Has Prognostic Variable: 3D mass/volume mixing ratio for aerosols/Clouds/Potential temperature/Vapour/solid/liquid/Wind components/Water\n, Model Family: AGCM\n, Earth System Model: Aerosols/Atmosphere/Land Ice/Land Surface\n, Basic Approximations: \"Modal scheme\n###"}
{"text": "#Properties\nHas value, Global Mean Sea level Rise Projection, has lower limit for likely range, has upper limit for likely range, has  start of period, has end of period, climate scenario, has research problem, has unit\n#Text\nSea level rises at an accelerating pace threatening coastal communities all over the world. In this context sea level projections are key tools to help risk mitigation and adaptation. Projections are often made using models of the main contributors to sea level rise (e.g., thermal expansion, glaciers, and ice sheets). To obtain the total sea level these contributions are added; therefore, the uncertainty of total sea level depends on the correlation between the uncertainties of the contributors. This fact is important to understand the differences in the uncertainty of sea level projections from different methods. Using two process-based models to project sea level for the 21st century, we show how to model the correlation structure and its time dependence. In these models the correlation primarily arises from uncertainty of future global mean surface temperature that correlates with almost all contributors. Assuming that sea level contributors are independent of each other, an assumption made in many sea level projections underestimates the uncertainty in sea level projections. As a result, high-end low probability events that are important for decision making are underestimated. The uncertainty in the strength of the dependence between contributors is also explored. New dependence relations between the uncertainty of dynamical processes and surface mass balance in glaciers and ice sheets are introduced in our model. Total sea level uncertainty is found to be as sensitive to the dependence between contributors as to uncertainty in certain individual contributors like thermal expansion and Greenland ice sheet. 10.1029/2018EF000849process-based method that also tries to evaluate the magnitude of each sea level rise contributor individually but using the most detailed physics possible. In the process-based method numerical models of physical processes are used when they are reliable and other sources of information are used otherwise . Typically, thermal expansion comes from state-of-the-art climate models; ice sheet surface mass balance (SMB) comes from regional models or empirical relationship between increase precipitation and increase temperature; and ice sheet dynamics comes from either ice sheet models, expert judgement, or statistical projections or from a combination of all of these. For all these methods, once the probability distribution or some other uncertainty measure has been quantified for each contributor to sea level rise, they are combined to obtain the total future sea level rise and its uncertainty. Information about the dependence between the sea level contributors is necessary for that step . How this dependence influences the projection of total sea level is the subject of this paper.A change of the correlation structure in the sea level projections of the Intergovernmental Panel on Climate Change (IPCC) Assessment Report 4 compared to the Third Assessment Report ) was the main reason for the reduction of the uncertainty. Still, this subject has received little attention in the literature until now probably because the focus has mainly been on projecting the expected value or the likely range of probabilities (e.g., a range that has a probability of 66% or more, , while the quantiles far away from the expected value are more sensitive to the dependence between contributors. Now the probability range of interest broadens because low probability events are also important for risk management if they have a high impact . For example, , , and go up to the 95th percentile; , , and Le , up to the 99th percentile; and , up to the 99.9th percentile. It is therefore time to look at the sensitivity of results from the process-based method to the dependence between contributors.The study of dependence between sea level contributors is similar to the study of coincidence of storm surge, tides, and river discharge that can lead to coastal flooding. Mathematically, the problem is the same, but in practice it is easier to constrain the dependence between coastal processes because observational data and more complete physical models are available . This allows the use of bivariate statistics tools like copulas to investigate compounding effects . The problem of dependence of sea level contributors is also more difficult to understand because it is not about events that correlate in time, for which we have a good intuition, but about events that correlate in the ensemble of possible futures that is a more abstract concept.In section 2 we shortly review current practices to propagate the uncertainty from individual contributors to total sea level. The two sea level rise projection models that we use in this paper are then described in section 3, and their results are analyzed in section 4. The paper finishes with a discussion and a conclusion. Dependence Between Sea Level Contributors: The Problem and a Review of Current PracticesMathematically, sea level projections can be seen as a sum of random variables. The random variables, which are time dependent, are the contributors to sea level rise (e.g., thermal expansion and glaciers), and the total sea level rise is therefore a random variable. The expected value of the total sea level is the sum of the expected values of the contributors and is therefore independent of the dependencies between the sea level contributors . However, the distribution of the total sea level is sensitive to the dependencies. When two independent random variables are added, the variance of their sum is the sum of their variances, but for positive correlation the variance of the sum increases compared to the independent case and for negative correlation it decreases . This result is obtained without any assumption on the probability distribution of the random variables and is key to understand the results described in section 4.To compute the total sea level probability distribution it is therefore necessary to know the joint probability distribution formed by the sea level contributors. The probability distributions of each sea level contributor are then the marginal probability distributions of this joint probability distribution. This is a well-known mathematical problem that has been widely discussed but not yet in the context of 10.1029/2018EF000849 sea level projections. A consequence is that the importance of the choice of dependencies between sea level contributors is not yet\n###\n", "summary": " Has value: 1.84\n, Global Mean Sea level Rise Projection: Global Mean Sea Level Rise Projections\n, has lower limit for likely range: 1.24\n, has upper limit for likely range: 2.46\n, has  start of period: 1986-2005\n, has end of period: 2081-2100\n, climate scenario: RCP8.5\n, has research problem: Global Mean Sea Level Rise Projections\n, has unit: m\n###"}
{"summary": "has benchmark: Benchmark NYT-single/Benchmark NYT/Benchmark WebNLG\nhas research problem: Relation Extraction\nhas model: ETL-Span\nsame as: https://en.wikipedia.org/wiki/Relationship_extraction", "text": "#Properties\nhas benchmark, has research problem, has model, same as\n#Text\nJoint extraction of entities and relations aims to detect entity pairs along with their relations using a single model. Prior work typically solves this task in the extract-then-classify or unified labeling manner. However, these methods either suffer from the redundant entity pairs, or ignore the important inner structure in the process of extracting entities and relations. To address these limitations, in this paper, we first decompose the joint extraction task into two interrelated subtasks, namely HE extraction and TER extraction. The former subtask is to distinguish all head-entities that may be involved with target relations, and the latter is to identify corresponding tail-entities and relations for each extracted head-entity. Next, these two subtasks are further deconstructed into several sequence labeling problems based on our proposed span-based tagging scheme, which are conveniently solved by a hierarchical boundary tagger and a multi-span decoding algorithm. Owing to the reasonable decomposition strategy, our model can fully capture the semantic interdependency between different steps, as well as reduce noise from irrelevant entity pairs. Experimental results show that our method outperforms previous work by 5.2%, 5.9% and 21.5% (F1 score), achieving a new state-of-the-art on three public datasets. METHODOLOGYIn this section, we first introduce our tagging scheme, based on which the joint extraction task is transformed into several sequence labeling problems. Then we detail the hierarchical boundary tagger, which is the basic labeling module in our method. Finally, we move on to the entire extraction system. Tagging SchemeLet us consider the head-entity (HE) extraction first. As discussed in the previous section, it is decomposed into two sequence labeling subtasks. The first subtask mainly focuses on identifying the start position of one head-entity. One token is labeled as the corresponding entity type if it is the start word, otherwise it is assigned the label \"O\" (Outside). In contrast, the second subtask aims to identify the end position of one head-entity and has a similar labeling process except that the entity type is labeled for the token which is the end word.For each identified head-entity, TER extraction is also decomposed into two sequence labeling subtasks which make use span boundaries to extract tail-entities and predict relations simultaneously. The first sequence labeling subtask mainly labels the relation type for the token which is the start word of the tail-entity, while the second subtask tags the end word.  illustrates an example of our tagging scheme, in which the words \"United\", \"States\", \"Trump\", \"Queens\", \"New\" and \"City\" are all related to final extraction results, thus they are labelled with special tags. For example, the word \"Trump\" is the first and also the last word of head-entity \"Trump\", so the tags are both PER-SON in the start and end tag sequences when tagging HE. For TER extraction, when the given head-entity is \"Trump\", there are two tail-entities involved in with wanted relations, i.e., (\"Trump\", President Of, \"United States\") and (\"Trump\", Born In, \"New York City\"), so \"United\" and \"New\" are labeled as President Of and Born In respectively in the start tag sequences. Similarly, we can obtain end tag sequences that \"States\" and \"City\" are marked. Beyond that, the other words irrelevant to the final result are labeled as \"O\".Note that our tagging scheme is quite different from PA-LSTM . For an n-word sentence, PA-LSTM builds n different tag sequences according to different query position while our model tags the same sentence for 2 + 2 \u00d7 m times to recognize all overlapping relations, where m is the number of head-entities and m << n. This means our model is more time-saving and efficient. Besides, it uses \"BIES\" signs to indicate the position of tokens in the entity while we only predict the start and end positions without loss of the ability to extract multi-word entity mentions. Hierarchical Boundary TaggerAccording to our tagging scheme, we utilize a unified architecture to extract HE and TER. In this paper, we wrap such extractor into a general module named hierarchical boundary tagger (abbreviated as HBT). For the sake of generality, we do not distinguish between head and tail-entity, and they are collectively referred to as targets in this subsection. Formally, the probability of extracting a target t with label l (entity type for head-entity or relation type for tail-entity) from sentence S is universally modeled as:  . An illustration of our model. The left panel is an overview of our joint extraction system, and the right panel shows the detailed structure of our sequence tagger HBT. Here, \"Queens\" is extracted by the HE extractor, then its hidden state in the shared encoder is marked as the yellow box and entered into the TER extractor as prior knowledge.where s l t is the start index of t with label l and e l t is the end index. Such decomposition indicates that there is a natural order among the tasks: predicting end positions may benefit from the prediction results of start positions, which motivates us to employ a hierarchical tagging structure. As shown in the right panel of , we associate each layer with one task and take the tagging results as well as hidden states from the low-level task as input to the high-level. In this work, we choose BiLSTM  as the basic encoder. Formally, the label of word xi when tagging the start position is predicted as Eq. 4.sta tag(xi) = arg maxwhere hi denotes token representation and ai is an auxiliary vector. For HE extraction, ai is a global representation learned from the entire sentence. It is beneficial to make more accurate predictions from a global perspective. For TER extraction, ai is the concatenation of a global representation and a head-entity-related vector to indicate the position and semantic information of the given head-entity. Here we adopt BiLSTMsta to fuse hi with ai into a single vector h sta i . Analogously, xi's end tag can be calculated by Eq. 6.The difference between Eq. 2-4 and Eq. 5-7 is twofold. Firstly, we replace hi in Eq. 2 with h sta i to make model aware of the hidden states of start positions when predicting end positions. Secondly, inspired by the position encoding vectors used in , we feed the position embedding p se i to the BiLSTM end layer as its additional input. p se i can be obtained by looking up p se i in a trainable position embedding matrix, whereHere s * is the nearest start position before current index, and p se i is the relative distance between xi and s * . When there is no start position before xi, s * will not exist, then p s i is assigned as a constant C that is normally set to the maximum sentence length. In this way, we explicitly limit the length of the extracted entity and teach model that the end position is impossible to be in front of the start position. To prevent error propagation, we use the gold p se (distance to the correct nearest start position) during training process.We define the training loss (to be minimized) of HBT as the sum of the negative log probabilities of the true start and end tags by the predicted distributions:where\u0177 sta i and\u0177 end i are the true start and end tags of the i-th word, respectively, and n is the length of the input sentence.At inference time, to adapt to the multi-target extraction task, we propose a multi-span decoding algorithm, as shown in Algorithm 1. For each input sentence S, we first initialize several variables (Lines 1-4) to assist with the decoding: (1) n is defined as the length of S. (2) R is initialized as an empty set to record extracted targets and type tags. (3) s * is introduced to hold the nearest start position before current index. (4) p se is initialized as a list of length n with default value C to save the position sequence [p seNext, we obtain the start tag sequence by Eq. 4 (Line 5) and compute p se i for each token by Eq. 8 (Lines 6-10). On the basis of p se , we can get p se by looking up position embedding matrix (Line 11) . Then the tag sequence of end position can be computed by Eq. 7 (Line 12). Now, all preparations necessary are in place, we start to decoding sta tag(S) and end tag(S). We first traverse sta tag(S) to find the start position of a target (Line 13). If the tag of current index is not \"O\", it denotes that this position may be a start word (Line 14), then we will traverse end tag(S) from this index to search for a end position (Line 15). The matching criterion is that if the tag of the end position is identical to the start position (Line 16), the words between the two indices are considered to be a candidate target (Line 17), and the label of start position (or end position) is deemed as the tag of this target (Line 18). The extracted target along with its tag is then added to the set R (Line 19), and the search in end tag(S) is terminated to continue to traverse sta tag(S) to find the next start position (Line 20). Once all the indices in sta tag(S) are iterated, this decoding function ends by returning the recordset R (Line 21). EXTRACTION SYSTEMWith the span-based tagging scheme and the hierarchical boundary tagger, we propose an end-to-end neural architecture ( ) to extract entities and overlapping relations jointly, which first encodes the sentence with a shared BiLSTM encoder. Then, a HE extractor is built to extract head entities. For each extracted head entity, the TER extractor is triggered with this head-entity's semantic and position information to detect corresponding tail-entities and relations. Shared EncoderGiven sentence S = {x1, \u2022 \u2022 \u2022 , xn}, we utilize a BiLSTM layer to incorporate information from both forward and backward directions:where hi is the hidden state at position i, and xi is the word representation of xi which contains pre-trained embeddings and characterbased word representations generated by running a CNN on the character sequence of xi. Following , we also employ part-of-speech (POS) embedding to enrich xi. where RHE = {(hj, type h j )} m j=1 contains all the head-entities and corresponding entity type tags in S. TER ExtractorSimilar to HE extractor, TER extractor also uses the basic representation hi and global vector g as input features. However, simply concatenating hi and g is not enough for detecting tail-entities and relations with the specific head-entity. The key information required to perform TER extraction includes: (1) the words inside the tail-entity;(2) the depended head-entity; (3) the context that indicates the relationship; (4) the distance between tail-entity and head-entity. Under these considerations, we propose the position-aware, head-entityaware and context-aware representationxi. Given a head-entity h, we definexi as follows:where , in which to is the o-th extracted tail-entity and relo is its relation tag with the given head-entity. RTER = HBTTER(HTER)Then we can assemble triplets by combining h and each (to, relo) to form {(h, relo, to)} z o=1 , which contains all triplets with headentity h in sentence S 5 . It is worth noting that at the training time, h is the gold head-entity, while at the inference time we select headentity one by one from RHE to complete the extraction task. Training of Joint ExtractorTwo learning signals are provided to train the model: LHE for HE extraction and LTER for TER extraction, both are formulated as Eq.9. To share input utterance across tasks and train them jointly, for each training instance, we randomly select one head-entity from gold head-entity set as the specified input of the TER extractor. We can also repeat each sentence many times to ensure all triplets are utilized, but the experimental results show that this is not beneficial. Finally, the joint loss is given by:Then, the model is trained with stochastic gradient descent. Optimizing Eq.14 enables the extraction of head-entity, tail-entity, and relation to be mutually influenced, such that, errors in each component can be constrained by the other. EXPERIMENTS Experimental Settings DatasetsFollowing popular choices and previous work , We conduct experiments on three benchmark datasets: (1) NYT- single is sampled from the New York Times corpus  and published by Ren et al . The training data is automatically labeled using distant supervision, while 395 sentences are annotated manually as test data, most of which have single triplet in each sentence. NYT-multi is published by Zeng et al.  for testing overlapping relation extraction, they selected 5000 sentences from NYT-single as the test set, 5000 sentences as the validation set and the rest 56195 sentences are used as training set. WebNLG is proposed by Claire et al.  for Natural Language Generation task. We use the dataset pre-processed by Zeng et al  and the train set contains 5019 sentences, the test set contains 703 sentences and the validation set contains 500 sentences. Statistics of the datasets are shown in . Besides, as suggested in , we also divided the test set into three categories: Normal, SingleEntityOverlap (SEO), and Entity-PairOverlap (EPO) to verify the effectiveness on extracting overlapping relations. Specifically, a sentence belongs to Normal class if none of its triplets has overlapping entities. If the entity pairs of two triplets are identical but the relations are different, the sentence will be added to the EPO set. A sentence belongs to SEO class if some of its triplets have an overlapped entity and these triplets dont have any overlapped entity pair. Note that a sentence in the EPO set may contain multiple Normal and SEO triplets. We discuss the result for different categories in the detailed analysis. EvaluationWe follow the evaluation metrics in previous work . A triplet is marked correct if and only if its relation type and two corresponding entities are all correct, where the entity is considered correct if the head and tail offsets are both correct. We adopt the standard micro Precision, Recall and F1 score to evaluate the results. Implementation DetailsWe use the 300 dimension Glove  to initialize word embeddings. The POS, character and position embeddings are randomly initialized with 30 dimensions. The window size of CNN for characterbased word representations is set to 3, and the number of filters is 50. For Bi-LSTM encoder, the hidden vector length is set to 100. Parameter optimization is performed using Adam  with learning rate 0.001 and batch size 64. Dropout is applied to word embeddings and hidden states with a rate of 0.4. To prevent the gradient explosion problem, we set gradient clip-norm as 5. All the hyper-parameters are tuned on the validation set. We run 5 times for each experiment then report the average results. Comparison ModelsFor comparison, we employ the following models as baselines:  Cotype  learns jointly the representations of entity mentions, relation mentions and type labels; (2) NovelTagging  is the first proposed unified sequence tagger which predicts both entity type and relation class for each word; (3) MultiDecoder  considers relation extraction as a sequence-to-sequence problem and uses dynamic decoders to extract relation triplets; (4) MultiHead  first identifies all candidate entities, then perform relation extraction by identifying multiple relations for each entity, these two tasks are trained jointly; (5) PA-LSTM  is the current best unified labeling method, which tags entity and relation labels simultaneously according to a query word position and achieves the recent state-of-the-art results on the NYT-single dataset; (6) GraphRel  is the latest extrat-thenclassify method, which first employs GCNs to extract hidden features, then predicts relations for all word pairs of an entity mention pair extracted by a sequence tagger; (7) OrderRL  is the state-ofthe-art method on the NYT-multi and WebNLG datasets, which applies the reinforcement learning into a sequence-to-sequence model to generate multiple triplets.We call our proposed extract-then-label method with span-based scheme as ETL-Span. In addition, to access the performance influence of span-based scheme, we also implement another competitive baseline by replacing our tagger with widely used BiLSTM-CRF without any change in the input features (xi andxi), and utilize BIES-based scheme accordingly, which associates each type tag (entity type or relation type) with four position tags to indicate entity positions and types simultaneously, denoted as ETL-BIES.  reports the results of our models against other baseline methods. It can be seen that our method, ETL-Span, significantly outperforms all other methods and achieves the state-of-the-art F1 score on all three datasets. Over the latest extract-then-classify method GraphRel, ETL-Span achieves substantial improvements of 16.1% and 40.2% in F1 score on the NYT-multi and WebNLG datasets respectively. We attribute the performance gain to two design choices: (1) the integration of tail-entity and relation extraction as it captures the interdependency between entity recognition and relation classification; (2) the exclusion of redundant (non-relation) entity pairs by the judicious recognition of head-entities which are likely to take part in some relations. For the NYT-single dataset, ETL-Span improves by a relative margin of 5.2% against the strong baseline PA-LSTM. We consider that it is because (1) we decompose the difficult joint extraction task into several more manageable subtasks and handle them in a mutually enhancing way; (2) our TER extractor effectively captures the semantic and position information of the depended headentity, while PA-LSTM detects tail-entities and relations relying on a single query word. In addition, we find that the results of our model are better than sequence-to-sequence methods like MultiDecoder and OrderRL, it is likely due to the innate restrictions on RNN unrolling, the capacity of generating triplets is limited . Beyond that, we notice that the Precision of our model drops compared with NovelTagging on the NYT-single dataset. One possible reason is that many overlapping relations are not annotated in the manually labeled test data. Following PA-LSTM , we add some gold triplets into NYTsingle test set and further achieve a large improvement of 12.5% in F1 score and 18.7% in Precision compared with the results in . Overall, these results indicate that our extraction paradigm which first extracts head-entity then labels corresponding tail-entity and relation can better capture the relational information in the sentence. Experimental Results and Analyses Main ResultsWe also observe that ETL-Span performs remarkably better than ETL-BIES, we guess it is because ETL-BIES must do additional work to learn the semantics of the BIES tags, while in ETL-Span, the entity position is naturally encoded by the set of type labels, thus . Main results on three benchmark datasets. Bold marks highest number among all models. \u2021 marks results quoted directly from the original papers. \u2020 marks results reported in  and . * marks results produced with offcial implementation. ModelNYT reducing the tag space of each functional tagger. Another advantage of span-based tagging is that it avoids the computing overhead of CRF, as shown in , ETL-Span accelerates the decoding speed of ETL-BIES by up to 3.7 times. The main reason is that decoding the best chain of labels with CRF requires a significant amount of computing resources especially when the tag space is huge (e.g., on WebNLG with 246 relations and 989 tags). Besides, ETL-Span only takes about 1/4 time per batch and 1/5 GPU memory compared with ETL-BIES during training, which further verdicts the superiority of our span-based scheme. Ablation StudyTo demonstrate the effectiveness of each component, we remove one particular component at a time to understand its impact on the performance. Concretely, we investigated character embedding, position embedding p ht , hierarchical tagging (by tagging boundary positions at the outmost BiLSTM layer), head-entity type tagging (by tagging 0/1 instead of entity types in the HE extractor) and joint learning (by training HE extractor and TER extractor separately without parameter sharing). From these ablations shown in , we find that: (1) Consistent with PA-LSTM , the character-level representations are helpful to capture the morphological information and deal with OOV words. (2) When we remove p ht , the score drops by 3.8%, which indicates that it is vital to let tail-entity extractor aware of position information of the given head-entity to filter out irrelevant entities by implicit distance constraint. Removing the hierarchical tagging structure hurts the result by 2.5% F1 score, which indicates that predicting end positions benefits from the prediction results of start positions. (4) By predicting entity type in the HE extractor, we can implicitly incorporate type information into head-entity representation, which is beneficial to the subsequent TER tagging. (5) Compared with the pipelined manner, joint learning framework brings a remarkable improvement (5.3%) in F1 score, which demonstrates that our HE extractor and TER extractor actually work in the mutual promotion way, and again confirms the effectiveness and rationality of our decomposition strategy. Analysis on Different Sentence TypesTo verify the ability of our model in handling the overlapping problem, following , we conduct further experiments on the NYTmulti test set. The results are shown in . Among the compared baselines, GraphRel and OrderRL are the latest two models with the capacity to handle the EPO triplets. For this purpose, GraphRel predicts relations for all word pairs, in this case, its relation classifier will be overwhelmed by the superfluous candidates. OrderRL utilizes a sequence-to-sequence model to decode overlapping relations but can decode only the first word of multi-word entity, while ours can detect the whole. Readers may have noticed that ETL-Span cannot solve the problem of entity pair overlapping. Nevertheless, ETL-Span still surpasses baselines in all categories. Specifically, ETL-Span outperforms OrderRL by 6.1% on the Normal class, 6.9% on the SEO class, and 0.6% on the EPO class. In fact, even on the EPO set, there are still a significant amount of triplets where entity pairs don't overlap. The most common triplets in the real-life corpus are those of Normal and SEO class and our substantial surpass on these two categories masks our shortcomings on the EPO class. We leave the identification of EPO triplets for future work. We also compare the results given different numbers of triplets in a sentence, and sentences in the NYT-multi test set are divided into 5 subclasses, each class contains sentences that has 1,2,3,4 or \u2265 5 triplets. As illustrated in , ETL-Span outperforms the baselines under all numbers of triplets in a sentence. When the sentence only contains one triplet, ETL-Span yields a 8.8% improvement in comparison with OrderRL. When there are multiple triplets in a sentence, ETL-Span still outperforms GraphRel and OrderRL significantly. These observations demonstrate that our extraction paradigm is effective to handle the multiple relation extraction task. CONCLUSIONSIn this paper, we present an end-to-end sequence labeling framework for joint extraction of entities and relations based on a novel decomposition strategy. Experimental results show that the functional decomposition of the original task simplifies the learning process and leads to a better overall learning outcome, achieving a new state-ofthe-art on three public datasets. Further analysis demonstrates the ability of our model in handling normal, overlapping and multiple relation extraction. In the future, we would like to explore similar decomposition strategy in other information extraction tasks, such as event extraction and aspect extraction. The source code of this paper can be obtained from\n###\n"}
{"text": "#Properties\nimplementation, has research problem, Data types, Visualization types, User recommendation, Preferences, Statistics, Sampling, Aggregation, Incremental, Disk, Domain, Application type\n#Text\nThe user has requested enhancement of the downloaded file. LODWheel-JavaScript-based Visualization of RDF Data State of the Art Overview in RDF Data VisualizationThere exist numerous tools for visualizing RDF data, both on the Web and as desktop applications. Most of the desktop-based applications are built with the purpose of aiding developers when constructing ontologies (RDFS, OWL) and RDF data, and do no not offer much value to users with little or no technical skills. Some examples of such applications are Prot\u00e9g\u00e9 , RDF Gravity and Welkin . These tools provide complex tree-and graphvisualizations of the data entities and the relationships between them, which is useful for getting a better understanding of the data structure. However, they are built around the purpose of supporting technical development of ontologies and RDF data only, thus not focusing on visualizing Linked Open Data on the In addition to the tree-and graph-visualizations that dominate desktop-applications, the most common way of visualizing RDF data on the Web is simply by using tables. Most existing Linked Open Data sets currently use this way of visualizing RDF data, including DBpedia and Freebase . Pubby is a tool developed by the University of Berlin for visualizing RDF data in a table format. This tool gives a good overview over the data, but offers no intuitive ways of categorizing the data or applying additional semantics to them. For instance, there may be several data resources that points to an image-URL, but there are no intuitive ways of separating these data from the other data, apart from the predicate and the file extension of the URL. Moreover, the common table-view of RDF data does not display different colors, or clusters similar data together, nor does it provide intuitive ways of showing what data resources are the subjects and objects in the different triples.An uncovered territory when it comes to visualizing RDF data is how JavaScript-based data visualization libraries can support the visualization of larger quantum of data. There exist a great deal of JavaScript libraries made for visualizing data in graphs and different charts, such as bar-charts, pie-charts and timelines, to mention a few. Compared to traditional tools for visualizing RDF data, JavaScript libraries can offer more in the aspects of grouping data together, adding colors to the data, and applying arrows or pointers to the different data resources. However, not all of these JavaScript libraries are sufficient when it comes to visualizing larger data sets. The next section will provide insights into the most relevant JavaScript libraries for visualizing data, and their strengths and weaknesses when using them for visualizing RDF data. Evaluation of JavaScript libraries for RDF data visualizationBefore the development of LODWheel started, a thorough evaluation of state-of-the-art JavaScript data visualization libraries was performed. The libraries had to be purely based on JavaScript, as it was an ultimate criterion that the libraries should be platform-independent. Therefore, libraries built on both JavaScript and Flash would not be taken into consideration. The evaluation was designed to address important aspects that such tools should include when it comes to functionality and usability. The evaluation should be based on a thoroughly devised set of criteria. These criteria should take into consideration both technical and usability aspects. The libraries were separated into two different categories in the evaluation: graph-visualization libraries and chart-visualization libraries. Based on these categories, the criteria of each librarytype were unique. All the libraries had criteria categorized as three separate groups: usability criteria, data criteria and quality criteria. The usability-criteria focused on the libraries' possibilities of clicking on, hovering over and dragging the nodes in the graph, supporting different colors on data elements, displaying text on the edges and nodes of the graph, supporting arrows/pointers between data resources, and some form of legend-functionality. All of the usability criteria were designed in order to get an overview over how much usability functionality the library offered, thus addressing the visualization possibilities. Also, it was important that the library should support the JavaScript Object Notation (JSON) data format , making it easy to convert the RDF data and integrate it into JavaScript. Further, it was important to address if the libraries supported the loading of external JSON files, as well as the simplicity of the JSON format, and whether or not the library could update the graph via AJAX, instead of loading the whole page for each request. Finally, the overall layout and performance of the library was evaluated, as well as how space-efficient the library was. The criteria consisting of the possibilities of dragging nodes, clicking on nodes, displaying text on nodes and text on edges were exclusive to the evaluation of the graph-visualization libraries. The criteria for evaluating JavaScript visualization libraries are summarized in . All of the evaluation criteria were given a score from 1-5 based on how well the library matched the given criteria. 1 was the worst score and 5 was the best score. Some of the evaluation criteria were found to be more important than others, thus demanding a way of weighting the different criteria. The most important criteria were weighted as score x3 and the next most important criteria as score x2. The remaining criteria had a weight of score x1. shows the criteria that were weighted, why the criteria were especially important, and how the criteria were weighted. Weighted criteria Why important Weighting scoreDifferent colors Important to categorize data and apply additional semantics to the data. Score x3JSON compatible Important for storing data and making the data structure behind the visualization easier to implement, and more robust. Score x3Performance Important for the overall usability of the library. The library should handle requests fast, and be easy to interact with. Score x3Text on nodes Important in order to see what data are displayed in the graph. Score x2Arrows/pointers Important for showing what data resources are the subjects and objects in each RDF triple. Score x2JSON simplicity Important that the JSON format is simple to use and code, and can store all the data needed in order\n###\n", "summary": " implementation: LODWheel\n, has research problem: Generic visualization systems\n, Data types: Graph (network)/Numeric/Special\n, Visualization types: Chart/Graph/Map/Pie\n, User recommendation: F\n, Preferences: F\n, Statistics: F\n, Sampling: F\n, Aggregation: F\n, Incremental: F\n, Disk: F\n, Domain: Generic\n, Application type: Web\n###"}
{"summary": "Results: T5-based Relationship extraction/Naive Bayes and Random Forests-based Relationship extraction/Rule-based relationship extraction\nMethod: machine learning method/rule-based method\nRelationships: Negation and speculation/Mode of action/Gene-Disease/Drug-Disease/Drug-Gene/mutation or modification/activation or up-regulation/inhibition or down-regulation/Biomarker/Target/Plays a role/Biomarker/Effect on/Increase Disease/Decrease Disease/Cause/Adverse event/Therapeutic use/Treatment/Substrate/Co-factor/Modulator/Part of/Regulator/Down-regulator/inhibitor/Up-regulator/activator\nF1: 0.88 on Balanced dataset/0.86 on Unbalanced dataset/0.78 on Balanced dataset with Random Forests/0.74 on Balanced dataset with Naive Bayes/0.66 on Unbalanced dataset with Random Forests/0.48 on Unbalanced dataset with Naive Bayes/0.80\nRecall: 0.88 on Balanced dataset/0.87 on Unbalanced dataset/0.79 on Balanced dataset with Random Forests/0.75 on Balanced dataset with Naive Bayes/0.71 on Unbalanced dataset with Random Forests/0.62 on Unbalanced dataset with Naive Bayes/0.74\nprecision: 0.88 on Balanced dataset/0.87 on Unbalanced dataset/0.79 on Balanced dataset with Random Forests/0.73 on Balanced dataset with Naive Bayes/0.88\nData: 2,684,742 relationships between Genes and Diseases (20,065,385 including co-occurrences)/1,468,639 relationships between Drug-Diseases (8,378,599 including co-occurrences)/631,573 named relationships found between Drug-Genes (6,885,810 including co-occurrences)/4,784,985 relationships (with co-occurrences 35,900,521)/10,000 sentences were obtained from rule-based method, with confidence 1./2000 sentences were reviewed and correctedby human annotators\nApproaches: Text-to-text model/Random Forest and Naive Bayes Classifier\ndescription: fine-tuned text-to-text algorithm called T5/sentence classification using traditional machine learning algorithms, such as Random Forests, Naive Bayes./T5 model is a multi-task model that can be fine-tuned and new tasks can be added during the fine-tuning of the model. This model is a text-to-text model, which means that both inputs and outputs are text. Input text usually comes with the prefix, specifying the task./a rule-based method, based on sentence patterns and dictionaries of trigger verbs and phrases\nmodel: T5 model\nurl: https://huggingface.co/t5-base\nFeatures: uni-grams, bi-grams, tri-grams and four-grams\nGrammar: Predicate_type/Subject_type/Negation/Speculation/Trigger/Predicate/Subject\nTypes: Increase Disease/Decrease Disease/Cause/General\nsame as: product of", "text": "#Properties\nResults, Method, Relationships, F1, Recall, precision, Data, Approaches, description, model, url, Features, Grammar, Types, same as\n#Text\nBiomedical research is growing in such an exponential pace that scientists, researchers and practitioners are no more able to cope with the amount of published literature in the domain. The knowledge presented in the literature needs to be systematized in such a ways that claims and hypothesis can be easily found, accessed and validated. Knowledge graphs can provide such framework for semantic knowledge representation from literature. However, in order to build knowledge graph, it is necessary to extract knowledge in form of relationships between biomedical entities and normalize both entities and relationship types. In this paper, we present and compare few rule-based and machine learning-based (Naive Bayes, Random Forests as examples of traditional machine learning methods and T5-based model as an example of modern deep learning) methods for scalable relationship extraction from biomedical literature for the integration into the knowledge graphs. We examine how resilient are these various methods to unbalanced and fairly small datasets, showing that T5 model handles well both small datasets, due to its pre-training on large C4 dataset as well as unbalanced data. The best performing model was T5 model fine-tuned on balanced data, with reported F1-score of 0.88. MethodIn this paper, we compare methods for relationship extraction. All of our methods use in-house modified Linnaeus  tool for named entity recognition and normalization. For relationship extraction, we present and compare three methods: (1) a rule-based method, based on sentence patterns and dictionaries of trigger verbs and phrases, (2) a machine learning method, based on traditional machine learning models (i.e. Naive Bayes, Random Forests), and (3) a deep learning method based on text to text transformer T5 model . Named entity recognition and normalization (modification of Linnaeus)Named entity recognition was done by internally modified version of Linnaeus tool . We have added a number of features that would allow us to perform more flexible entity matching, while relying on Linnaeus algorithm which is fast and reliable with good dictionaries. The added features include:\u2022 Handling defined set of characters as white spaces and treating multiple sequential white space characters as a single white space character.\u2022 Implementing global flag that allows ignoring cases of letters in matches if needed\u2022 A flag for automatic pluralization (adding -s and -es suffixes) of dictionary terms\u2022 A functionality that can handle and transliterate Greek characters (e.g. beta -\u03b2) as well as functionality that can handle variation of position of Greek characters (e.g. Interferon-\u03b1 vs \u03b1 -interferon).\u2022 Removing or ignoring diacritics\u2022 Synonym level exact, case sensitive and regular expression matching\u2022 Flag to match only the longest matchThe dictionaries used for named entity recognition and normalization into entities have been carefully internally developed, expanded and refined over the past fifteen years by our internal information scientists. We have used dictionaries for human genes, diseases and approved drugs. Relationship data modelRelationships of interests are relationships between drug, gene and disease entities. The three pairs of relationships of interest are: (1) Drug-Gene, (2) Drug-Disease and (3) Gene-Disease. Each of these pairs may have a number of distinct relationship types. Drug-Gene relationshipsThe relationships between chemical and proteins were subject of last two BioCreative shared tasks in 2017 1 and 2020 2 . CPR GROUP TYPE CPR:1Part of CPR:2 Regulator CPR:3Up-regulator, Activator CPR:4 Down-regulator, Inhibitor CPR:5 Agonist CPR:6 Antagonist CPR:7Modulator CPR:8Co-factor CPR:9Substrate, Product of CPR:10 Not Both of the tasks defined the same interaction types. These can be seen in . However, for majority of purposes, some of the defined types may be redundant. Therefore, we have simplified the data model by merging some of the classes and excluding ones that are rarely mentioned in text. At the end, our Drug-Gene model had the following relationship classes:\u2022 Substrate or product of Note that Regulator is a type of relationship in which it is not possible to determine the direction of regulation from the sentence. Drug-Disease relationshipsRelationships between drugs and diseases do not have any gold standard data model that was used in previous shared tasks. Therefore a new model was proposed containing the following relationship classes:There may seem that there could be redundancy between Therapeutic use and Decrease Disease classes, or between Cause and Increase Disease classes. However, Therapeutic use and Cause indicate relationships where disease is indication or counter indication for given drug. Increase and Decrease disease may refer to any finding that given drug improved or worsened the state of disease, and therefore is a weaker relationship. The weakest relationship is Effect on, because, in this case, only the fact that there is some effect of drug to disease is know, without any additional details (e.g. whether it improves disease or makes it worse).Chemical compound can be a biomarker for some diseases. In medicine and drug discovery, it is important to have a picture of biomarkers and therefore it is included in the model. Gene-Disease relationshipsThe relationship between genes and diseases are the most complex one among the three types in scope. This is because a single gene can improve, worsen or even cause a certain disease. Therefore, it is often not enough to classify type of the relationship, but the algorithm needs to extract also mode of action on the gene. In terms of possible relationship types, we have identified the following ones:\u2022 Plays a role -From sentence can be concluded clearly that there is a connection between gene and the disease, however, it is not clear what kind of role the gene plays in the disease, only that it plays some role.\u2022 Target-General -The gene or protein can be considered a target for the given disease, with no more specific information on the modulation of the disease.-Cause -The sentence indicates that activation, mutation or inhibition or any other action over gene is causing a given disease.-Modulator * Decrease disease -There is clear indication that gene is responsible for decreasing and alleviating the disease. * Increase Disease -There is clear indication that gene is responsible for increasing and worsening the disease.\u2022 Biomarker -The presence or lack of given gene/protein is indicator for diagnosis disease or pathology. Mode of actionTogether with the relationship classes, if available, mode of action is important modifier for Gene-Disease relationships. It establishes the action taken on a gene in order for relationship to take place. For example, gene may both decrease and cause disease, depending on whether the gene was activated or inhibited. Possible modes of actions are (1) inhibition or down-regulation, (2) activation or up-regulation, (3) mutation or modification. Negation and speculationRelationship between entities in sentence may be negated, which reverses the semantics of the relationship. Therefore, it is important to detect whether the relationship is negated.Likewise, statements in text can be factual, stated as well known facts, or speculative. Speculative claims need to be taken with more caution and therefore, speculation detection is included in our model. Relationship extraction using rulebased methodBased on the previously described relationship model, we have developed a rulebased method for relationship extraction. The method relies on vocabularies for relationship trigger words, negation cues, speculation cues, mode of action (MoA) cues and grammar pattern rule set. Example of vocabularies, and patterns in the rule set with an example sentence from which relationship is extracted using given rules and vocabularies is presented in .The trigger word vocabulary contains a list of relationship trigger words and phrases, with metadata, such as to which relationship given word or phrase maps, between which entities, whether entities have to be in a given order of mentioning or can be in reverse order (e.g. for Drug-Disease relationship whether it is allowed for drug to be after disease in sentence) and whether the phrase should be interpreted as regular expression.Mode of action cues have mapping to the mode of action type (i.e. Inhibition, Activation, Mutation). The vocabularies for negation and speculation cues are simple lists of words (e.g. no, not for negation; hypothesise, may for speculative).In grammar are defined sequences that need to be matched in order to extract relationship. This grammar has several keywords, such as Subject, which refers to the subject entity, Predicate, which refers to predicate entity, Trigger, referring to trigger cue, Speculation, referring to Speculation, Negation, referring to negation cue, Subject type, referring to entity that is not subject in current evaluation pair of entities, but has same type as Subject entity (i.e. Drug or Gene), Predicate type, referring to entity having same type as predicate, but not evaluated in current pair. Additionally, there may be defined a number of words that are between labelled entities, trigger words, negations, and speculative phrases. For example the following pattern: Speculation W3 Subject W3 Trigger W3 Predicate would match sequences where speculative cue is followed by up to three tokens, followed by Subject, followed by up to three tokens, followed by trigger phrase, followed by another three tokens and predicate. This means it would match sentences such as \"We hypothesise that aspirin can alleviate most cases of headache\", in case token \"hypothesise\" is in the list of speculative cues, \"aspirin\" is marked as drug and is subject, \"headache\" is a disease and predicate, and \"alleviate\" is a trigger word.The matching algorithm iterates over labelled entities in each sentences, and find all pairs that may constitute relationships. It annotates sentence with potential trigger phrases, speculative cues, and negations. Finally, the algorithm tries to match any pattern from the grammar to the sequence in sentence. If the matching is successful, the relationship is extracted and mapped to the relationship type and metadata, such as mode of action, negation and speculation cues are extracted.The confidence score is calculated as proportion of words in sentences that exactly match named phrases (Subject, Predicate, Trigger, Speculation, Negation), divided by all the words in the pattern (this includes named phrases and tokens that were matched as part of allowed distance tokens, e.g. up to three words for each W3 statement in grammar).In addition, the method also extracts cooccurrences, giving them fixed confidence of 0.0001, and labelling them with \"Cooccurrence\" label.Each extracted relationship contains information about entities (entity string, type, preferred term, internal ID), relationship type, whether it negated, whether it is speculative, and confidence score. Also, evidence sentence and Medline ID of the article where the evidence was found is recorded. Machine learningWe have developed two machine learning methods. The first method is based on sentence classification using traditional machine learning algorithms, such as Random Forests, Naive Bayes. The other method is using fine-tuned text-to-text algorithm called T5 . Training and testing dataThe data are collected by using rulebased relationship extractor previously described for Gene-Disease relationship. We are evaluating our approach on Gene-Disease data, as it proved to have the most complex data model and therefore is the most complex to correctly extract relationships. Also, this relationship type is important from biomedical perspective, as it may give insights on potential targets for treating respective diseases. Out of this dataset, 2000 sentences were reviewed and corrected by human annotators. For this task, a company called Molecular Connections was contracted. Other 10 000 sentences were obtained from rule-based method, with confidence 1. These sentences would match correct sentences, as they do not allow for any tokens that may change context, apart from named phrases. Therefore, the dataset contained about 12 000 sentences. The data was split as 90% training and 10% testing data for training and testing of machine learning approaches.In order to create a more balanced dataset, we have generated the second dataset by taking 2000 manually annotated sentences, but then adding sentences from rule-based method with the confidence 1 in such way that each relationship class had at least 1500 sentences.We have also created a dataset for classification of mode of action. We created again one unbalanced and balanced dataset. Since for mutation class we had only 140 examples, we initially balanced dataset by taking 140 examples from each class. This is fairly small dataset and may be improved by adding examples. We have created an additional dataset taking 300 data samples from each class, allowing duplicated for class that did not have enough samples (mutation). Random forest and Naive Bayes classifiersIn both Random forest and Naive Bayes classifier the sentences were tokenized and stemmed using Porter Stemmer. Since for relationship extraction, it is important to examine the sequences of words, the features for our classifiers were uni-grams, bigrams, tri-grams and four-grams. Finally, data was trained using Random Forest and Naive Bayes Classifier. Text-to-text modelWe have fine-tuned T5 model that is readily available on HuggingFace 3 . T5 model is a multi-task model that can be fine-tuned and new tasks can be added during the fine-tuning of the model. This model is a text-to-text model, which means that both inputs and outputs are text. Input text usually comes with the prefix, specifying the task. We have added a new prefixes for gene-disease relationship classification and gene mode-of-action classification (with four classes -activation, inhibition, mutation, and not reported). We have fine-tuned the model on our dataset using Adafactor optimizer .In the training and testing sentences, the text was pre-prepared in such way that subject of the relationship (e.g. gene) was replaced with the keyword SUBJECT and the predicated of the relationship (e.g. disease) was replaced with the keyword PREDICATE. In this way, we hypothesized that internal attention mechanism of the model would be able to learn how to treat vicinity of subjects and predicates of the relationships. Results Rule-based relationship extractionWe have processed base Medline data until January 2021, containing about 35 million abstracts. The processing with both Linnaeus and relationship extraction engine took about 7 days on a single machine.We managed to extract in total 4,784,985 relationships (with co-occurrences 35,900,521). There were 631,573 named relationships found between Drug-Genes (6,885,810 including cooccurrences), 1,468,639 relationships between Drug-Diseases (8,378,599 including co-occurrences), and 2,684,742 relationships between Genes and Diseases(20,065,385 including co-occurrences).Extracted relationships can be loaded in graph or relational database, where these relationships can answer complex medical questions with evidence. By summing confidence scores, it is possible to retrieve genes interacting with given drug (e.g. top results for drug Tolvaptan was inhibition of AVPR2, while the second one was inhibition of vasopressin receptor family), drugs that have effect on certain disease (e.g. for autosomal dominant polycystic kidney disease retrieved Tolvaptan, which is approved for autosomal dominant polycystic kidney disease, Sirolimus, which inhibits mTOR and as well have been often used in polycystic kidney disease, and Somatostatin, which was published as a hormone having potential role in treatment of autosomal dominant polycystic kidney disease (Messchendorp et al., 2020)), or what genes are important for a given disease (e.g. for autosomal dominant polycystic kidney disease retrieved PKD1 and PKD2 as targets that both play a role and have causative relationship with disease, as well as mTOR, REN, CCL2). We evaluated a case study related to autosomal dominant polycystic kidney disease. We created a graph whose edges end in autosomal dominant polycystic kidney disease. In order to reduce noise, we consider only edges that represent relation-ship that was mentioned at least 5 times in PubMed. We have then evaluated the graph and all entities were indeed known from literature to experts. Portion of knowledge graph with relationships ending in ADPKD is presented in .We have evaluated 100 abstracts containing at least one relationship manually and calculated precision, recall and F1-score. The evaluation is depending on the extent of trigger phrases and completeness of grammar, that is over time improving. The measured performance was 0,88 precision, 0,74 recall and 0,80 F1-score. It is expected that rule-based approach would have high precision and lower recall, as it would miss some of the relationships, but annotate relatively few false positives. Despite making some false positive relationships, the answering of relevant biomedical questions of between genes, drugs and disease performed well. The cumulative effect is that noise can be ignored by setting a threshold and manually validating results bellow given threshold if necessary. Naive Bayes and Random Forests-based Relationship extraction Machine learning method was evaluated on two sets (2000 manually annotated relationships + 10,000 random relationships with confidence 1 -unbalanced set, and 2000 manually annotated relationship + random relationships with confidence 1, so there is at least 1,500 examples for each class -balanced set). For both data sets, 90% of data was used as training data, while 10% of data (about 1200 sentences) was used as testing set. The results of our evaluation can be seen in the table 2.Balancing data significantly improves precision and recall in both classifiers.   always pick the most probable class -the class with the most results. Random forest classifier was better in learning how to recognize classes. However, balancing data, gained 26% in F1-score for Naive Bayes and 14% for overall results in Random Forests. The worst performance has a class which we were unable to balance due to the lack of annotated examples -No Explicit relationships. Other classes performed with F1score over 70%. T5-based Relationship extractionWe have fine-tuned base T5 model for relationship extraction by adding a new prefix (\"Relationship extraction:\") on both unbalanced and balanced data. We have monitored the performance of algorithm over epochs. The results can be seen on .Balancing data improves also T5 model, although the increase in performance is just 2% (F1-score increase from 0.86 to 0.88 after five epoch). However, certain rela- tionship types in unbalanced dataset had large gap between precision and recall (e.g. \"No Explicit relationship\" in unbalanced had P=0.88, R=0.26), while in balanced dataset precision and recall were closer (for same class P=0.88, R=0.72).We present the results of relationship classification after five epochs in .We have also created a model for modeof-action that classifies mode of action associated with a gene into four possible classes: activation, inhibition, mutation and not reported. The model was trained on unbalanced and balanced dataset (each class containing 300 examples of each class). The model was trained for 5 epochs. The results are presented in .Mode-of-action detection performs well with quite small amount of data. This is because terms used for mode-of-action are in relatively closed set , and language model is able to transfer and infer them from the time model was pre-trained on the C4 dataset. However, adding data helps improve it. However, since testing data for the first balanced dataset was fairly small (containing 56 samples), there may be variance in performance when testing on different batch of unseen data.\n###\n"}
{"text": "#Properties\nMethod, Analyte, Application, Detection limit, PubChem AID, Protein Target, Modiy Date, Deposit Date, has, has confirmatory assay, has assay format, Has participant, has role, is bioassay type of, has assay method, has detection method, has assay title, has temperature value, has incubation time value, has assay phase characteristic, has endpoint, has concentration unit, Has detection instrument, Has, status , has doi, has author, has publication month, has publication year, has research field, has contribution, has research problem, has subfield, same as\n#Text\nElsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre-including this research content-immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active. Short communication Design, synthesis and crystallographic analysis of nitrile-based broad-spectrum peptidomimetic inhibitors for coronavirus 3C-like proteases SARS-CoV 3CL pro is inhibited by nitrile-based peptidomimetic inhibitorsBased on the autocleavage tetrapeptide sequence (AVLQ) of SARS-CoV 3CL pro , three nitrile-based inhibitors with different protective groups, 5-methylisoxazole-3-carboxyl (Mic), tert-butyloxycarbonyl (Boc) and carboxybenzyl (Cbz), were synthesized (Scheme 1). The protease activity in the presence of inhibitors was measured by using a fluorescent-protein substrate as described previously . The IC 50 values of 'Mic-AVLQ-CN', 'Boc-AVLQ-CN', and 'Cbz-AVLQ-CN' were 49 AE 2, 49 AE 2, and 4.6 AE 0.2 mM, respectively . Interestingly, the inhibitor with the Cbz protective group was w10x more potent than the other inhibitors tested. To test the effect of increasing the length of the peptidomimetic inhibitor, we synthesized a hexapeptide inhibitor 'Cbz-TSAVLQ-CN'. The IC 50 value for this inhibitor was 39 AE 1 mM ( ), suggesting that increasing the length of the inhibitor did not improve inhibition and the Cbz group at the P4 position may be responsible for the higher potency of 'Cbz-AVLQ-CN'. Crystal structures of enzymeeinhibitor complexesTo better understand the enzymeeinhibitor interaction, we determined the crystal structures of SARS-CoV 3CL pro in complex with the four inhibitors at 1.95e2.5 A resolution , Supplementary data). The 3CL pro formed a dimer in the crystal structure, and the inhibitors occupied the substrate binding site in both protomers. Electron density was observed for the tetrapeptide (AlaeValeLeueGln) at P1 to P4 positions for all inhibitor complexes . Overlaying these inhibitor structures demonstrated that the tetrapeptides adopted the same conformation when bound to the 3CL pro . On the other hand, the protective group of 'Mic-AVLQ-CN', the P5-Ser and P6-Thr of 'Cbz-TSAVLQ-CN' were disordered and not observable in the electron density. Step (a) was the coupling of peptides by using N,N 0 -dicyclohexylcarbodiimide and Nhydroxysuccinimide.Step (b) was the coupling of peptides by the mixed anhydride method.Step (c) was the deprotection of benzyl carbamates by using palladium on charcoal.Step (d) was the deprotection of Boc, Trt, Pbf groups and tertiary butyl esters by using trifluoroacetic acid.Step (e) was the transformation of primary amide to nitrile by using trifluoroacetic anhydride. Final products were highlighted with dashed box. 'Cbz-AVLQ-CN' (square) and 'Cbz-TSAVLQ-CN' (diamond) at 0e256 mM, followed by measuring the protease activity to determine the inhibitory effect. The IC 50 values were determined by fitting the curve to a four-parameter logistics equation.In SARS-CoV, the catalytic dyad consists of Cys145 and His41 . The inhibitor was covalently bonded with the thiol group of Cys145 via the carbon atom of the nitrile warhead ( ). This mechanism is consistent with that observed in other cysteine proteases . The side chains of P1-Gln, P2-Leu and P4-Ala occupied the S1, S2 and S4 subsites, respectively, of SARS-CoV 3CL pro , while the side-chain of P3-Val pointed towards the solvent ( . A number of hydrogen bonds are involved in enzymeeinhibitor interactions. The P1-Gln side chain was hydrogen-bonded with the backbone O atom of Phe140 and the side-chain of His163 and Glu166. The backbone amide groups of P2 to P4 formed hydrogen bonds with Glu166, Gln189 and Thr190, and these hydrogen bonds help the P3 and P4 residues to adopt a bstrand like conformation .Noteworthy, the aromatic ring of the Cbz group is docked into a pocket where the ring can form favourable hydrophobic interactions with Pro168 and P3-Val ). These extra interactions, which are absent in other enzymeeinhibitor complexes, may enhance the binding affinity between 3CL pro and increase the potency of the 'Cbz-AVLQ-CN' inhibitor. Nitrile-based peptidomimetic inhibitor exhibits broadspectrum inhibition against 3CL pro from different groups of coronavirusesNext, we tested if the 'Cbz-AVLQ-CN' inhibitor has broadspectrum inhibition against 3CL pro from different groups of coronaviruses. We measured and compared the IC 50 values of 'Cbz-AVLQ-CN' against 3CL pro from human coronavirus (HCoV) strains 229E (group 1), NL63 (group 1), OC43 (group 2a), HKU1 (group 2a), SARS-CoV (group 2b), and infectious bronchitis virus (IBV) (group 3). Our results showed that 'Cbz-AVLQ-CN' can inhibit all 3CL pro tested, with values of IC 50 ranging from 1.3 to 4.6 mM . In contrast, the 'Cbz-AVLQ-CN' had no observable inhibition effect on caspase-3 at 0.5e256 mM , suggesting that the inhibitor is specific for 3CL pro from coronaviruses. As structures of 3CL pro from group 1, 2a, 2b, and 3 are similar, our modelling study suggests that the 'Cbz-AVLQ-CN' inhibitor can fit nicely to the active site of 3CL pro from all groups of coronaviruses and inhibit the protease via the same mechanism , Supplementary material). Crystal structure of 3CL pro -inhibitor complex. (A) The 3CL pro was shown in surface representation, while the inhibitor was shown in stick representation. The side-chains of P1-Gln, P2-Leu and P4-Ala occupy the S1, S2, S4 subsites of 3CL pro , while the side-chain of P3-Val points towards the solvent. (B) The hydrogen bonds (dotted lines) between the inhibitor (orange) and the 3CL pro (grey) were shown. (C) The Cbz protective group of the 'Cbz-AVLQ-CN' docks into a pocket formed by Glu166, Leu167, and Pro168 of SARS-CoV 3CL pro . Surface was shown for atoms of 3CL pro and P1-P4 residues of the inhibitors. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) ConclusionIn this study, we have shown that nitrile-based peptidomimetic inhibitors display\n###\n", "summary": " Method: Fluorescent dye labelled antibody or fluorescent dye labelled antigen\n, Analyte: Albumin in serum/whole blood\n, Application: Albumin as a biomarker for diabetes\n, Detection limit: 12.2\u201360 g L\u22121\n, PubChem AID: 725072\n, Protein Target: 3C-like protease\n, Modiy Date: 2018-09-25\n, Deposit Date: 2013-11-07\n, has confirmatory assay: confirmatory assay\n, has assay format: enzyme format/in vitro format/protein format\n, Has participant: \"20 mM Tris\n, has role: buffer/target/recombinant fluorogenic substrate/active compound/https://pubchem.ncbi.nlm.nih.gov/bioassay/725072#section=Data-Table&fullscreen=true\n, is bioassay type of: in vitro evaluation of inhibitory activity\n, has assay method: fluorescence resonance energy transfer\n, has detection method: fluorescence changes fitted to a single-exponential decay\n, has assay title: Inhibition of SARS coronavirus 3C-like protease pretreated for 5 mins before substrate addition by fluorimetry\n, has temperature value: no\n, has incubation time value: 5 minutes\n, has assay phase characteristic: homogeneous phase\n, has endpoint: IC50\n, has concentration unit: micromolar\n, Has detection instrument: EnVision 2101 Multilabel Plate Reader\n, Has: BAO semantic description\n, status : Live\n, has doi: 10.1016/j.cccn.2003.10.002\n, has author: Dong Joon Kim/Eui Yul Choi/Jae Hoon Kim/Sang Wook Oh/Sunga Choi/Tai Sun Kim\n, has publication month: 1\n, has publication year: 2004\n, has research field: Research field\n, has contribution: Contribution 1\n, has research problem: Lateral flow (immuno)assay\n, has subfield: \"Aerodynamics and Fluid Mechanics/Aeronautical Vehicles/Astrodynamics/Multi-Vehicle Systems and Air Traffic Control/Navigation\n, same as: https://en.wikipedia.org/wiki/Operations_research\n###"}
{"text": "#Properties\nMethod, dataset, has research problem, approach, deals with, uses similarity, Performance metric, Evidence, Uncertainty, Limitations\n#Text\nIn many databases, science bibliography database for example, name attribute is the most commonly chosen identifier to identify entities. However, names are often ambiguous and not always unique which cause problems in many fields. Name disambiguation is a non-trivial task in data management that aims to properly distinguish different entities which share the same name, particularly for large databases like digital libraries, as only limited information can be used to identify authors' name. In digital libraries, ambiguous author names occur due to the existence of multiple authors with the same name or different name variations for the same person. Also known as name disambiguation, most of the previous works to solve this issue often employ hierarchical clustering approaches based on information inside the citation records, e.g. co-authors and publication titles. In this paper, we focus on proposing a robust hybrid name disambiguation framework that is not only applicable for digital libraries but also can be easily extended to other application based on different data sources. We propose a web pages genre identification component to identify the genre of a web page, e.g. whether the page is a personal homepage. In addition, we propose a re-clustering model based on multidimensional scaling that can further improve the performance of name disambiguation. We evaluated our approach on known corpora, and the favorable experiment results indicated that our proposed framework is feasible. Research goal and challengesName disambiguation has been studied in several areas with particular emphases. Our work focuses on the disambiguation on large databases, e.g. DBLP, because the data there is simple and clean but often its information is not sufficient. is a sample for citation records in DBLP, in which only minimum information like author names, paper titles and publication venues are provided. Therefore the framework we propose needs to be able to handle this kind of situation. The research goal can be defined as below in the case of DBLP:Definition 1 Given a list of citations C, where each citation c i 2 C is a bibliographic reference containing at least a list of author names A i \u00bc a 1 ; a 2 ; . . .; a n ; a work title t i , and publication year y i . The goal of our approach is to find out the set U \u00bc u 1 ; u 2 ; . . .; u k of unique authors and attribute each citation record c i to the corresponding author u j 2 U:Apparently, we will need sufficient information to properly attribute each record to the corresponding person. As discussed above, since the information in DBLP is very limited, we first need to retrieve more information, and the method to gain more information must be domain independent, which means it is not only applicable on DBLP but also can be applied to any other similar domains.There are two methods currently widely investigated by researchers. One is is to retrieve information from other resource, e.g. web pages. The challenge is that we need to identify which pages are useful and which pages are noise. For example, personal homepages are definitely useful in the case of name disambiguation, as many researchers will put their publications in their homepages. Therefore, a model to identify the genre of a web page is needed.The other is to explore the linking information in the data as the relative information among each object in a graph format can improve the quality of disambiguation. The challenge to use this method is how to compute the linking similarity among citations. The details about how we use linking information and combine with web information in our name disambiguation framework to achieve the research goal are given in section ''Hybrid name disambiguation framework''. Our contributionsWe use DBLP as case study in this paper because the data there is clean but provides little information. We highlight the following contributions of this paper that aims to propose a robust hybrid name disambiguation framework for large databases: (1) We propose a web pages genre identification component to identify the genre of a web page, e.g. if the page is personal homepage. The component contains three models, neural network model, traffic model and Genre-SVM model which can be used together or separately. We implemented a multiple models combination (MMC) method to efficiently combine three models together. We propose a re-clustering model as the last step of the framework, which contains unsupervised and supervised methods and enables the strength of both types of methods, namely, scalability and robustness of the former and accuracy of the latter.The re-clustering model can group those records which can not be merged through co-authorship and web information directly based on multidimensional scaling (MDS). (3) Extensive experiments on real corpora are performed to demonstrate the efficiency and effectiveness of our web pages genre identification component and the whole name disambiguation framework. Organization of the paperThe paper is organized as follows: in section ''Related work'', we present the related work on the disambiguation problem, in particular supervised and unsupervised learning methods for author name disambiguation in digital libraries. Section ''Hybrid name disambiguation framework'' presents a hybrid and scalable name disambiguation framework to integrate data linking and web information together with re-clustering mechanism. We then evaluated key components of the framework on different corpora. The details are given in section ''Experiments''. Conclusions and future research directions are outlined in section ''Conclusions and future research directions''. Hybrid name disambiguation frameworkIn this section, we propose a framework that not only uses information in traditional name disambiguation approaches, e.g. co-authorship, but also applies web pages as a source of additional information. This framework uses the information by leveraging a traditional hierarchical clustering method with re-clustering mechanism to deeply process those citation records which can not be found in any web pages, and group them together if they refer to one person. Our web pages genre identification component involves neural network, traffic rank analysis and our GenreSim-SVM classifier to reduce the web information extraction cost while minimizing the\n###\n", "summary": " Method: Web page genre identification based graph re-clustering\n, dataset: DBLP\n, has research problem: Author name disambiguation\n, approach: Semi-Supervised Learning\n, deals with: Homonyms problem/Synonyms problem\n, uses similarity: Multi-dimensional scaling\n, Performance metric: F1\n, Evidence: Explicit web genre\n, Uncertainty: Robust\n, Limitations: Slow as using web information\n###"}
{"text": "#Properties\nHas result, has research problem, uses, has name, has version, has documentation, Has Horizontal Discretization, River Routing, Scheme method, Has Prognostic Variable, Model Family, has spatial resolution, drift of sea level height, Simulated Antarctic sea ice extent, Total Arctic sea ice extent, Sea ice off Alaska, Sea ice over the eastern part of the Siberian Basin, has horizontal resolution, Coupling With Atmosphere, Earth System Model, Carbon Cycle, has species, Has scheme, Basic Approximations, major ice sheets, Minor ice caps, Cold deep snow albedo , Bare ice albedo, Melting deep snow albedo, Vertical Physics, Critical Richardson number, Radiation scheme, Surface Air Temperature, RMSE of global mean surface air temperature against ERA-Interim reanalysis, Bias of mean surface temperature over northern hemisphere land against ERA-Interim reanalysis:, Bias of mean surface temperature over high latitudes Southern Ocean (>60\u00b0)  against ERA-Interim reanalysis, Bias of global mean precipitation against GPCP climatology, Cloud scheme, Bias of northern hemisphere (0\u00b0-60\u00b0N) annual mean cloud amount against the D2 ISCCP observations , Sea-ice volume trend in the northern hemisphere,     Sea-ice volume trend in the southern hemisphere, Trend of SST, Bias of global mean SST, Bias of global mean SSS against the WOA2009 data, Dynamical Core, Scheme Type\n#Text\nA full description of the ModelE version of the Goddard Institute for Space Studies (GISS) atmospheric general circulation model (GCM) and results are presented for present-day climate simulations (ca. 1979). This version is a complete rewrite of previous models incorporating numerous improvements in basic physics, the stratospheric circulation, and forcing fields. Notable changes include the following: the model top is now above the stratopause, the number of vertical layers has increased, a new cloud microphysical scheme is used, vegetation biophysics now incorporates a sensitivity to humidity, atmospheric turbulence is calculated over the whole column, and new land snow and lake schemes are introduced. The performance of the model using three configurations with different horizontal and vertical resolutions is compared to quality-controlled in situ data, remotely sensed and reanalysis products. Overall, significant improvements over previous models are seen, particularly in upper-atmosphere temperatures and winds, cloud heights, precipitation, and sea level pressure. Data-model comparisons continue, however, to highlight persistent problems in the marine stratocumulus regions. Model philosophyThe GISS model philosophy has always been to improve the physics of each modeled component, and to allow as great a degree as possible of flexibility in model configurations. This has led to a great deal of innovative and challenging science ; and many others) al-though some compromises (such as for horizontal resolution) were necessary. We have chosen not to uniquely pursue higher resolution, since that can severely limit the length and variability of the experiments possible, but rather we have maintained a variety of resolutions that can be used based on scientific need. Our experience has been that while some aspects of a simulation can be improved by increasing the resolution (frontal definition, boundary layer processes, etc.), many equally important improvements are likely to arise through improvements to the physical parameterizations. Indeed, some features (such as the stratospheric semiannual oscillation, high-latitude sea level pressure, or the zonality of the flow field) are degraded in higherresolution simulations, indicating that resolution increases alone, without accompanying parameterization improvement, will not necessarily create a better climate model. As models improve and computer resources expand, there will always be a tension between the need to include more physics (tracers, a more resolved stratosphere, cloud microphysics, etc.), to run longer simulations, and to have more detailed vertical and horizontal resolution. The balance that is struck will be different for any particular application and so a flexible modeling environment is a prerequisite. In this paper, we therefore show results from three different configurations that differ principally in their horizontal and vertical resolution. Model physicsThe model physics are predominantly based on the physics of the GISS Model II\u0408 (SI2000 version) described in previous publications , and references therein). However, many details have changed, and some physics has been completely reworked. We therefore provide a brief description of the current physics along with a summary of the major changes over the last few years. In all the subsequent text we are referring to the February 2004, ModelE1 public release version of the code.In common with most other models, we make some basic assumptions at the outset, which though minor, have consequences throughout the model: namely, that water vapor does not add to atmospheric mass (i.e., globally integrated surface pressure is constant), the latent heat of atmospheric water vapor does not depend on temperature (i.e., all atmosphere-surface freshwater fluxes are assumed to be at 0\u00b0C), the potential energy of water vapor/condensate is neglected, condensate is not advected, and the pressure gradient calculation does not include humidity effects. We hope to be able to relax these constraints in future versions. The prin-cipal prognostic variables in the atmosphere are the potential temperature, water vapor mixing ratio, and the horizontal velocity components. Virtual potential temperature is used for all density/buoyancy-related calculations.The advection is mass conserving for humidity and tracers, and potential enthalpy conserving for heat. All processes including the dynamics, cloud schemes, gravity wave drag, and turbulence conserve air, water, and tracer mass and energy to machine accuracy. All dissipation of kinetic energy through various mixing processes is converted to heat locally. In the long-term mean, the net flux of heat at the surface is equal to the net top-of-the-atmosphere (TOA) radiation. Angular momentum is conserved except due to drag and pressure torques at the solid land surface. a. ConfigurationsThe model has a Cartesian gridpoint formulation for all quantities. Available horizontal resolutions are 4\u00b0\u03eb 5\u00b0and 2\u00b0\u03eb 2.5\u00b0latitude by longitude (and 8\u00b0\u03eb 10\u00b0for historical and pedagogical reasons). The effective resolution for tracer transport is significantly greater than these nominal resolutions because of the nine higherorder moments that are carried along with the mean tracer values in each grid box (see section 3d). The velocity points in the atmosphere are on the Arakawa-B grid and the vertical discretization follows a sigma coordinate to 150 hPa with constant pressure layers above. The standard vertical resolution has 20 layers and a model top at 0.1 hPa ). Compared to previous 12-layer versions (i.e., , the 20-layer code has 2 extra layers near the surface, 2 more in the lower stratosphere, and 4 extra layers above 10 hPa. We also describe a 23-layer version that better resolves the stratosphere and has a model top near the mesopause (\u03f70.002 hPa; .The results described below are from three different configurations : the 4\u00b0\u03eb 5\u00b020-layer model (denoted M20), the corresponding full-stratospheric model (denoted M23, which differs in the vertical layering, model top, and use of a parameterized gravity wave scheme), and a simulation at 2\u00b0\u03eb 2.5\u00b0(denoted F20) but which is identical to M20 in most other respects. The F20 simulation should be thought of as a sensitivity test to increased horizontal resolution, rather than a fully developed configuration (which continues to be worked on).The surface is split into four types: open water (including lakes and oceans), ice-covered water (again including lake ice and sea ice areas), ground (including bare soil and vegetated regions), and glaciers. Within each type there may be further subdivisions (fraction of plant functional types, fractional snow cover, melt pond fraction over sea\n###\n", "summary": " has research problem: CMIP5/Global climate modelling\n, has name: GISS-E2-R\n, has documentation: http://pubs.giss.nasa.gov/docs/2007/2007_Bauer_etal_2.pdf/http://pubs.giss.nasa.gov/docs/2006/2006_Shindell_etal_2.pdf\n, Has Horizontal Discretization: PoleSingularityTreatment/SchemeMethod/SchemeType/PoleSingularityTreatment/SchemeMethod/Staggering\n, Has Prognostic Variable: \"3D mass/volume mixing ratio for aerosols/Potential temperature/Surface pressure/Total water/Total water moments/Wind components/3D mass/mixing ratio for gas/Snow heat content/Snow temperature/Snow water equivalent/Heat/Tracers/Water/Potential temperature/SSH/Salinity/U-velocity/V-velocity/Enthalpy\n, Model Family: AGCM/OGCM\n, Earth System Model: Aerosols/Atmosphere/Atmospheric Chemistry/Land Ice/Land Surface/Ocean/Sea Ice\n, Basic Approximations: \"Modal scheme\n###"}
{"summary": "contains: ProblemStatement/ProblemStatement/Contribution/Conclusion/Results/Contribution/Evaluation Score {HAS_VALUE}/Methods/Methods/Model\nhas content: Previous efforts to build educational search and recom-mender systems using the rapidly growing amount of OERs[5]\u2013[7] revealed the lack of high-quality OER metadata, andquality control [8]. These issues seriously limit the deploymentof high-quality OER search and recommendation services [9],[10]./However, their applicability has been limited, due tolow metadata quality and complex quality control. These issuesresulted in a lack of personalised OER functions, like recommen-dation and search./Learners interact with the recommender througha dashboard, in which they can search for their desired job,display the list of required skills, set their level of expertise foreach skill, set their learning context, and receive relevant OERrecommendations accordingly./This study is one of the first steps to 1) empower learners totake control and responsibility for their own skill development,2) improve skills on the basis of labour market informationand personalised OER recommendation, and 3) progress theliterature on OER quality control./Each participant generated at least17, but maximum 20 recommendations. During the valida-tion, altogether 415 OERs were recommended, and 336 ofthem (80.9%) were reported as useful (OK,Satisfied, orVerySatisfied)./In this paper, we also demonstrated an OER recommendersystem prototype, which collects OERs from 7 different OERrepositories, predicts their quality, and applies automatic qual-ity control./Finally, to recommend an OER to a learnerufor a particulartarget skills, the system retrieves all OERs with the levelthat learneruhas in skills, and uses cosine similarity torecommend the OER with the closestXvector to the userpreference vector (P)./Therefore, in this paper wecapitalized on this scoring and prediction model, not only toqualify metadata, but also to predict the quality of the OERswe collected form 7 distinct repositories./As a consequence,we built a model that predicts OER quality based on theirmetadata withF1-scoreof94.6%.", "text": "#Properties\ncontains, has content\n#Text\nThis Work in Progress Research paper departs from the recent, turbulent changes in global societies, forcing many citizens to re-skill themselves to (re)gain employment. Learners therefore need to be equipped with skills to be autonomous and strategic about their own skill development. Subsequently, highquality, on-line, personalized educational content and services are also essential to serve this high demand for learning content. Open Educational Resources (OERs) have high potential to contribute to the mitigation of these problems, as they are available in a wide range of learning and occupational contexts globally. However, their applicability has been limited, due to low metadata quality and complex quality control. These issues resulted in a lack of personalised OER functions, like recommendation and search. Therefore, we suggest a novel, personalised OER recommendation method to match skill development targets with open learning content. This is done by: 1) using an OER quality prediction model based on metadata, OER properties, and content; 2) supporting learners to set individual skill targets based on actual labour market information, and 3) building a personalized OER recommender to help learners to master their skill targets. Accordingly, we built a prototype focusing on Data Science related jobs, and evaluated this prototype with 23 data scientists in different expertise levels. Pilot participants used our prototype for at least 30 minutes and commented on each of the recommended OERs. As a result, more than 400 recommendations were generated and 80.9% of the recommendations were reported as useful. II. STATE OF THE ARTWe can group the related studies into two main categories: A) OER quality assessment, and B) OER recommender systems. A. OER Quality AssessmentDue to the large amount of OERs being created and uploaded by people around the world, manual quality control of OERs is getting more and more complicated, if not impossible. In general, high-quality metadata is a mandatory property for providing data driven services . In the area of OERs, lowquality metadata not only reduces the discoverability of OERs, but also has a strong negative effect on their usability . Accordingly, some studies attempted to define metrics (e.g. completeness and consistency of metadata , provenance, and accuracy of metadata ) in order to assess the quality of OER metadata. To support this,  showed that a metadata scoring method can be used to define a model to predict OER quality.Additionally,  provides a quality assessment framework based on existing developmental models in the area of elearning, semantic-based methods, and NLP techniques. predicted on the basis of video transcripts, popularity metrics (i.e. rate, likes, dislikes), and length . B. OER Recommendation SystemsThe literature on high-quality OER recommender systems is very limited . However, in recent years, some studies built recommendation algorithms based on ontologies, linked data, and open source RDF data to leverage semantic content , . As an example,  defined an ontology for learners, learning objects, and their environments in order to provide adaptive recommendations, based on similarities between object properties.  examined the Cold Start problem  in the case of new micro OERs, by defining rules based on recommended sequences of learning objects using existing ontologies. Furthermore,  built an OER recommendation system for learners in order to help them to achieve skill based learning objectives. III. DATA COLLECTIONIn this section, we explain the steps we followed to collect relevant and reliable data sources for our recommender prototype, focusing on data science related skills and jobs. A. Labour Market InformationWe applied the method proposed by  on 500 job vacancies, published in February and January 2020, which resulted in the extraction of the 16 most relevant and required skills for data science jobs. Additionally, we used Wikipedia python API 1 and crawled Wikipedia content describing these 16 extracted skills. B. OER ResourcesRegarding the educational content, we collected more than 700 OERs from the following repositories: Youtube, MIT OpenCourseWare 2 , Skills Commons 3 , OER Commons 4 , Wisc-Online  , Khan Academy 6 , and Wikipedia by using their APIs and/or search services. For each OER, we store the following fields (if available), in order to provide effective recommendations: source, title, description, target skill, level (Beginner, Intermediate, Advanced, Master), URL, length, transcription, view count, rating (or combination likes and dislikes), ranking position score (reverse of ranking position in the source repository search: 1/ranking position in repository search), and transcription similarity with the target skill description (calculated by the cosine similarity between the transcription vector and the target skill description vector using pre-trained 300-dimensional Glove vectors ). IV. METHODSIn this section, we detail the most important components of our OER recommender prototype. These components are responsible for OER quality control, recommendation generation, and the interaction with the learner. A. OER Quality PredictorThe aim of this component is to predict the overall quality of the collected OERs from different repositories, and define an automatic quality control mechanism.1) Quality Prediction Based on Properties: As a first step, we established a measure for OER-skill matching. For this measure we used procedures from  to define a random forest model that classifies OERs into two groups: Fit for Achieving a Skill, and Not Fit for Achieving a Skill. The model uses the following OER properties to execute this prediction: length, rate, transcription similarity with the target skill description, view count, and ranking position score.Furthermore, we had to find a solution for missing values in our prediction model, since not all OERs contained all required features. For this problem, we built a number of alternative random forest models with various combinations of the above-mentioned features, and selected the one with the highest F1-score. It should be mentioned that we applied alternative models on the dataset defined by , and got a minimum accuracy of 77.2 %. The feature-set was adjusted towards maximum accuracy, but in some cases we had to rely on models with fewer features (from all desired OER features) than optimal, which resulted in a slightly reduced accuracy.2) Quality Prediction Based on Metadata: Previously we proposed a metadata scoring model based on 8,887 crawled OERs from Skill Commons   . This model divides OERs in two groups: OERs with Manual Quality Control, and OERs Without Quality Control. At the same time, we also showed that there is a tight relationship between OER metadata quality and OER quality control processes. This means, that the more an OER passes quality controls, the higher the probability of containing high-quality metadata is. However, due to the rapidly growing amount of OERs, manual quality control is getting more and more impossible . As a consequence, we built a model that predicts OER quality based on their metadata with F1-score of 94.6%. Therefore, in this paper we also capitalized on this scoring and prediction model, to qualify metadata, and also to predict the quality of OERs we collected form 7 distinct repositories.3) Quality Control Based on the Defined Models: To apply an automatic quality control process on the collected OERs, we used the above-mentioned models in a probability mode, which shows the probability of the target class in the classifiers. Therefore, for each OER, based on their properties and metadata, we applied both of our prediction models and defined two float numbers between 0 and 1 to determine the OER quality scores. Finally, we removed OERs with quality scores less than 0.5 (altogether 184 OERs) from the list of potential OERs for recommendation. B. Recommendation GenerationIn order to build our OER recommender system, we created a 6-dimensional vector of X for each OER including:\u2022 Normalised length \u2022 Normalised rate \u2022 Text similarity with target skill description \u2022 Metadata based quality probability\u2022 Property based quality probability \u2022 Source (containing 7 binary values to identify the 7 OER repositories we used for this study) Respectively, for each learner, we define a 6-dimensional vector P as a preference vector that contains a weight (between 0 and 1) for each parameter in X.Moreover, to provide personalised recommendations for learners, we rely on the following features describing learners' context:\u2022 Country \u2022 City \u2022 Job experience including skills, expertise levels in each skill (Beginner, Intermediate, Advanced, Master) and industry 9 \u2022 Birth date \u2022 Gender (optional) The goal is to find the best weights (P vector) for each learner. Therefore, at learner registration, we initialise the P vector for each learner, based on preference vectors of similar learners in the learner's context (e.g. Country, City, Job experience). Subsequently, we use Gradient Descend to optimize the preference vector based on users' ratings by minimizing the following loss function:where X o is the 6-dimensional vector of an OER o and Y o is the satisfaction rating of the learner for that particular OER o.Finally, to recommend an OER to a learner u for a particular target skill s, the system retrieves all OERs with the level that learner u has in skill s, and uses cosine similarity to recommend the OER with the closest X vector to the user preference vector (P ). C. Learning DashboardIn this section, we explain the features of the implemented learner dashboard prototype, focusing on data science related jobs, using the above-mentioned components  . As an illustration,  shows the different components of our system and their interaction.  We defined industries based on  Classification of Economic Activities in the European Community 10 You can find a demo of the prototype from:  FIE2020When a learner uses our recommender for the first time, (s)he has to go through a two step registration process, which is critical to initialise essential learner features for effective recommendations:1) Profile Creation: Learners register in our system by adding their demographic features (e.g. country, city, and birth date). Subsequently, they can also add their previous job experience(s) to their profile. Based on their previous jobs, our system recommends related skills (based on our labour market analysis) for their profile. Learners also need to set their expertise level(s) for each skill.2) Defining Goals: Once a learner profile is created, our system asks learners to set their desired job. Based on this target occupation, our system shows all skills, which are required for that job (in the desired geographical location of the learner: city, country). Learners can select their skill goals from a list, but also search and add additional skill targets to their profiles  . D. Educational Content PageFor each selected skill objective, our system recommends the most relevant OERs. Learners can click and navigate to the physical location of the OER, and proceed with the learning. After learning, users can rate their satisfaction with each OER (from 1: very dissatisfied to 5: very satisfied), request another OER on the same level of difficulty, or ask for OERs with a higher level of difficulty. Moreover, if dissatisfied with the recommendation, learners can either replace the recommended OERs with another recommendation, or mark the OER as irrelevant for their skill target and ask for a replacement OER  .1) Progress Page and Reports: At each stage, learners can monitor their progress towards their goals (target job and skills), and consult reports on finished OERs at each expertise level they passed. Moreover, they can retrieve reports on periodically (e.g. monthly) completed OERs, and also the status of their current recommendations (e.g. in progress, finished, changed)  . V. VALIDATIONTo validate our approach and the proposed prototype, we invited 23 experts (including 16 PhD students and 7 university instructors) in the area of Data Science (with minimum 1 year of related teaching experience and 3 years of related industrial experience) and asked them to work with our prototype for at least 30 minutes. It should be mentioned that participants had different expertise levels regarding the required skills for a data scientist job as they had various range of teaching and industrial experience. Phd students had related teaching experience between 1 year and 6 years and related industrial experience between 3 years to 10 years. These numbers for  Screenshot of the goal definition page:  FIE2020 12 Screenshot of the educational content page:  FIE2020  Screenshot of the progress page:   instructors were between 7 to 16 years for teaching experience and 8 to 19 years for industrial experience. Accordingly, when experts registered in our system, 22% of skill expertise levels were set to Beginner, 26% were set to Intermediate, 31% were set to Advanced, and 21% were set to Master. After creating their profile and setting their goals, participants started working with their learning dashboard, including their recommended OERs. Each participant generated at least 17, but maximum 20 recommendations. During the validation, altogether 415 OERs were recommended, and 336 of them (80.9%) were reported as useful (OK, Satisfied, or Very Satisfied). To illustrate how the quality of recommendations increased over time, we grouped all recommendations into four clusters. Each quarter contains a bit more than 100 recommendations, and  shows participants' satisfaction rate for each cluster (over time). The Report as Useful column reveals that the more the users work with our system, the more satisfactory recommendations they receive, since our system attempts to identify learner preferences and provide personalised recommendations. Moreover, based on our quality prediction and quality control process, more than 70% of the recommendations satisfied learners right from the beginning. As a consequence, we believe that this exercise revealed that our approach and the implemented prototype can potentially provide high-quality support to learners working towards labour market's demanded skills. VI. CONCLUSION AND FUTURE WORKThis study is one of the first steps to 1) empower learners to take control and responsibility for their own skill development, 2) improve skills on the basis of labour market information and personalised OER recommendation, and 3) progress the literature on OER quality control. As a long term vision, we expect that learners will show enhanced self-regulation, and spend less effort to find relevant, high-quality OERs. This approach also contributes to OER property identification accuracy, which is essential to increase OER (re)usability.In this paper, we also demonstrated an OER recommender system prototype, which collects OERs from 7 different OER repositories, predicts their quality, and applies automatic quality control. Learners interact with the recommender through a dashboard, in which they can search for their desired job, display the list of required skills, set their level of expertise for each skill, set their learning context, and receive relevant OER recommendations accordingly. During their learning process, learners rate their satisfaction with recommendations, and update their learning preference vector. This strategy detects changes in learner profiles and fine tune the precision of recommendations.For the next steps, we plan to add more OER repositories, extract more learner and OER properties, and use (quasi-)experimental designs for further developing and validating our prototype in a number of use cases.\n###\n"}
{"summary": "has research problem: Optimization of the time-frequency resolution of spectrograms/Development of open source research software/Formation of two-frequency pulse compounds/Development of open source research software/Formation of two-frequency pulse compounds/Formation of two-frequency pulse compounds/Resonant radiation of optical solitons in presence of high-order dispersion/Development of open source research software/Formation of two-frequency pulse compounds/Formation of two-frequency pulse compounds/Formation of two-frequency pulse compounds/Formation of two-frequency pulse compounds\nRelated work: Dynamics of localized dissipative structures in a generalized Lugiato\u2013Lefever model with negative quartic group-velocity dispersion/Multi-frequency radiation of dissipative solitons in optical fiber cavities/Crossover from two-frequency pulse compounds to escaping solitons/Soliton Molecules with Two Frequencies/pyGLLE: A Python toolkit for solving the generalized Lugiato\u2013Lefever equation/Crossover from two-frequency pulse compounds to escaping solitons/Soliton Molecules with Two Frequencies/Multi-frequency radiation of dissipative solitons in optical fiber cavities/Dynamics of localized dissipative structures in a generalized Lugiato\u2013Lefever model with negative quartic group-velocity dispersion/Soliton Molecules with Two Frequencies/Crossover from two-frequency pulse compounds to escaping solitons/pyGLLE: A Python toolkit for solving the generalized Lugiato\u2013Lefever equation/Multi-frequency radiation of dissipative solitons in optical fiber cavities/Dynamics of localized dissipative structures in a generalized Lugiato\u2013Lefever model with negative quartic group-velocity dispersion/Soliton Molecules with Two Frequencies/Crossover from two-frequency pulse compounds to escaping solitons/Crossover from two-frequency pulse compounds to escaping solitons/Soliton Molecules with Two Frequencies\nMathematical model: Spectrogram/Generalized Lugiato-Lefever equation/Generalized Lugiato-Lefever equation/Forward model for the analytic signal (FMAS)/Forward model for the analytic signal (FMAS)/Generalized Lugiato-Lefever equation/Generalized Lugiato-Lefever equation/Forward model for the analytic signal (FMAS)/Forward model for the analytic signal (FMAS)/Forward model for the analytic signal (FMAS)/Forward model for the analytic signal (FMAS)\nkeywords: Ultrashort pulse propagation/Optics/Analytic signal/Lugiato-Lefever equation/Nonlinear dynamics/Localized dissipative structures/Open source software/Python/Dissipative solitons/Lugiato-Lefever equation/Nonlinear partial differential equations/Ultrashort optical pulses/Nonlinear dynamics/Numerical simulations/Solitons/Nonlinear dynamics/Numerical simulations/Microring resonators/Nonlinear propagation dynamics/Oscillating dissipative solitons/Open source software/Python/Dissipative solitons/Lugiato-Lefever equation/Nonlinear partial differential equations/Ultrashort optical pulses/Nonlinear dynamics/Numerical simulations/Solitons/Nonlinear dynamics/Numerical simulations/Solitons/Nonlinear dynamics/Numerical simulations/Ultrashort optical pulses/Nonlinear dynamics/Numerical simulations\nhas contribution: Dissipative solitons in the generalised Lugiato-Lefever equation/Software for simulating dissipative solitons in the generalised Lugiato-Lefever equation/Formation of two-frequency pulse compounds/Existence of two-frequency pulse compounds/Description of resonant emission of dispersive waves by oscillating solitary structures/Software for simulating dissipative solitons in the generalised Lugiato-Lefever equation/Formation of two-frequency pulse compounds/Existence of two-frequency pulse compounds/Existence of two-frequency pulse compounds/Formation of two-frequency pulse compounds\nhas research field: Optics, Quantum Optics and Physics of Atoms, Molecules and Plasmas/Computational Physics/Optics, Quantum Optics and Physics of Atoms, Molecules and Plasmas/Optics, Quantum Optics and Physics of Atoms, Molecules and Plasmas/Optics, Quantum Optics and Physics of Atoms, Molecules and Plasmas/Computational Physics/Optics, Quantum Optics and Physics of Atoms, Molecules and Plasmas/Optics, Quantum Optics and Physics of Atoms, Molecules and Plasmas/Optics, Quantum Optics and Physics of Atoms, Molecules and Plasmas/Optics, Quantum Optics and Physics of Atoms, Molecules and Plasmas\nHas venue: Optics Letters/SoftwareX/Scientific Reports/Physical Review Letters/Scientific Reports/SoftwareX/Scientific Reports/Physical Review Letters/Physical Review Letters/Scientific Reports\nhas publication year: 2020/2021/2021/2019/2020/2021/2021/2019/2019/2021\nhas publication month: 5/7/5/12/6/7/5/12/12/5\nhas author: A. Demircan/A. Yulin/O. Melchert/Ayhan Demircan/Oliver Melchert/A. Demircan/I. Babushkin/U. Morgner/S. Willms/O. Melchert/Ayhan Demircan/Ihar Babushkin/Uwe Morgner/Fedor Mitschke/Bernhard Roth/Alexey Yulin/Ayhan Demircan/Oliver Melchert/Ayhan Demircan/Oliver Melchert/A. Demircan/I. Babushkin/U. Morgner/S. Willms/O. Melchert/Ayhan Demircan/Ihar Babushkin/Uwe Morgner/Fedor Mitschke/Bernhard Roth/Ayhan Demircan/Ihar Babushkin/Uwe Morgner/Fedor Mitschke/Bernhard Roth/A. Demircan/I. Babushkin/U. Morgner/S. Willms/O. Melchert\nurl: ///\nhas doi: 10.1364/ol.392180/10.1016/j.softx.2021.100741/10.1038/s41598-020-65426-x/10.1016/j.softx.2021.100741\nArea of study: Computational photonics/Propagation of ultrashort optical pulses in nonlinear media/Nonlinear optics/Light-matter interaction/Propagation dynamics of ultrashort optical pulses/Computational photonics/Propagation of ultrashort optical pulses in nonlinear media/Nonlinear optics/Light-matter interaction/Propagation dynamics of ultrashort optical pulses/Nonlinear optics/Light-matter interaction/Propagation dynamics of ultrashort optical pulses/Propagation of ultrashort optical pulses in nonlinear media\ndescription: All aspects of computational science applied to physics./All aspects of computational science applied to physics.\nsame as: Arxiv ID: physics.comp-ph/http://uri.gbv.de/terminology/dfg/308-01/http://uri.gbv.de/terminology/dfg/308/http://uri.gbv.de/terminology/dfg/308-01/http://uri.gbv.de/terminology/dfg/308/http://uri.gbv.de/terminology/dfg/308-01/http://uri.gbv.de/terminology/dfg/308/Arxiv ID: physics.comp-ph/http://uri.gbv.de/terminology/dfg/308-01/http://uri.gbv.de/terminology/dfg/308/http://uri.gbv.de/terminology/dfg/308-01/http://uri.gbv.de/terminology/dfg/308/http://uri.gbv.de/terminology/dfg/308-01/http://uri.gbv.de/terminology/dfg/308/http://uri.gbv.de/terminology/dfg/308-01/http://uri.gbv.de/terminology/dfg/308/http://uri.gbv.de/terminology/dfg/308-01/http://uri.gbv.de/terminology/dfg/308\nAlgorithm: Fourth-order Runge-Kutta in the interaction picture method (RK4IP)/Fourth-order Runge-Kutta in the interaction picture method (RK4IP)/Fourth-order Runge-Kutta in the interaction picture method (RK4IP)/Fourth-order Runge-Kutta in the interaction picture method (RK4IP)/Fourth-order Runge-Kutta in the interaction picture method (RK4IP)/Fourth-order Runge-Kutta in the interaction picture method (RK4IP)\nHas ORCID: 0000-0002-0015-2077/0000-0002-5128-3763/0000-0002-0015-2077/0000-0002-0015-2077", "text": "#Properties\nhas research problem, Related work, Mathematical model, keywords, has contribution, has research field, Has venue, has publication year, has publication month, has author, url, has doi, Area of study, description, same as, Algorithm, Has ORCID\n#Text\nA Python package for the calculation of spectrograms with optimized time and frequency resolution for application in the analysis of numerical simulations on ultrashort pulse propagation is presented. Gabor's uncertainty principle prevents both resolutions from being optimal simultaneously for a given window function employed in the underlying short-time Fourier analysis. Our aim is to yield a time-frequency representation of the input signal with marginals that represent the original intensities per unit time and frequency similarly well. As use-case we demonstrate the implemented functionality for the analysis of simulations on ultrashort pulse propagation in a nonlinear waveguide. Motivation and significanceThe spectrogram, providing a particular timefrequency representation of signals that vary in time , represents an inevitable tool in the analysis of the characteristics of ultrashort optical pulses. E.g., allowing to monitor the change in frequency of pulse features as function of time permits to determine quantities that cannot be obtained from either the time or frequency domain representation of the optical pulse alone. The applicability of the spectrogram to both, data retrieved from experiments  where it is referred to as frequency resolved optical gating (FROG) analysis, as well as from numerical simulations , carried out to complement experiments and to provide a basis for the interpretation of the observed effects, highlights the relevance of signal processing in the field of nonlinear optics and demonstrates the need to be able to compute such spectrograms in the first place. Here, we consider the issue of obtaining optimal time-frequency representations of signals for the interpretation of numerical experiments on ultrashort pulse propagation in nonlinear waveguides.In principle, a spectrogram measures the properties of the signal under scrutiny as well as those of a userspecified window function for localizing parts of the signal during analysis. Exhibiting features of both, the interpretation of the spectrogram is strongly affected by the particular function used for windowing. Different window functions estimate different signal properties, e.g., if a given function achieves a good approximation of the intensities per unit time of the underlying signal, its approximation of the intensities per unit frequency might be bad. Consequently, the spectrogram might suffer from distortion yielding an unreasonable characterization of the timefrequency features of the signal under scrutiny. The usual approach for deciding on a particular window function is by trial-and-error and guided by the liking and experience of the individual.Here we present a software tool that aims at minimizing the mismatch between the intensities per unit time and frequency and their corresponding estimates based on the spectrogram itself, obtained for a user-supplied parame-terized window function. The resulting spectrograms are \"optimal\" in the sense that their visual inspection exhibits a minimal amount of distortion and thus allow for a reliable interpretation of the time-frequency composition of the input signal. Such an approach was previously shown to result in a reasonable characterization of the underlying time-frequency features . It is further independent of the experience of the individual user and thus yields reproducible results. Software descriptionThe presented package facilitates the construction of spectrograms for the analytic signal (AS)  E(t) of the real field E(t). In the Fourier domain, the angular frequency components of both are related via\u00ca(\u03c9) = [1 + sgn(\u03c9)]\u00ca(\u03c9) . Due to its one-sided spectral definition the time-domain representation of the AS is complex, further satisfying E(t) = Re[E(t)]. The construction of an AS spectrogram relies on the repeated calculation of the spectrum of the modified signal E(t)h(t\u2212\u03c4) at different delay times \u03c4 in terms of the short-time Fourier transformwherein h(t) specifies a narrow window function centered at t = 0 and decaying to zero for increasing |t|. The latter allows to selectively filter parts of the AS and to estimate its local frequency content. Scanning over a range of delay times then yields the spectrogram as P S (\u03c4, \u03c9) = |S \u03c4 (\u03c9)| 2 , providing a joint time-frequency distribution of both, the AS and the window function . For assessing the approximation quality of P S we utilize its time and frequency marginals P 1 (\u03c4) = P S (\u03c4, \u03c9) d\u03c9, and (2)Note that in the limit where h(t) approaches a delta function, the time marginal will approach the intensity per unit time |E(t)| 2 but the frequency marginal will represent the intensity per unit frequency |\u00ca(\u03c9)| 2 only poorly. As a result, time resolution will be good and frequency resolution will be bad, see the discussion in section 3 below. The time-frequency uncertainty principle prevents both resolutions from being optimal simultaneously . The aim of the presented package is to obtain a timefrequency representation of the input signal for which the integrated absolute error (IAE) between its normalized marginals and the original intensities per unit time and frequency are minimal. We consider a single parameter window function h(t, \u03c3), e.g. a Gaussian function with mean t and root-mean-square (rms) width \u03c3, and solve forAbove, the underlying spectrogram is computed via h(t, \u03c3), indicated by the superscript \u03c3 on the marginals, and we assume normalization to |E(t)| 2 dt = 1 and a total signal energy E S = P S (\u03c4, \u03c9) d\u03c4 d\u03c9 in terms of the spectrogram. For a good agreement of the marginals and the original intensities, the objective function Q assumes a small value. The additional parameter \u03b1 might be adjusted to give more weight to frequency resolution (\u03b1 < 0) or time resolution (\u03b1 > 0) if appropriate. The particular choice \u03b1 = 0 yields a balanced time-frequency representation, see the example provided in section 3. The optimized spectrogram is then computed by using h(t) \u2261 h(t, \u03c3 ) for windowing. Software ArchitectureOptFROG, following the naming convention  for Python packages implemented as optfrog, uses the Python programming language  and depends on the functionality of numpy and scipy . It further follows a procedural programming paradigm. Software FunctionalitiesThe current version of optfrog comprises five software units having the subsequent responsibilities:vanillaFrog Compute a standard spectrogram P S (\u03c4, \u03c9)for the normalized time-domain analytic signal for a particular window function h(t, \u03c3).optFrog Compute a time-frequency resolution optimized spectrogram for the normalized time-domain analytic signal using the window function h(t, \u03c3 ) that minimizes the total IAE of both marginals. Note: for the minimization of the scalar function Q(\u03c3, \u03b1) in the variable \u03c3, the scipy native function scipy.optimize.minimize scalar is employed in bounded mode.timeMarginal Compute the marginal distribution in time P 1 based on the spectrogram.frequencyMarginal Compute the marginal distribution in frequency P 2 based on the spectrogram.totalEnergy Compute the total energy E S provided by the spectrogram approximation of the timefrequency characteristics of the signal.For a more detailed description of function parameters and return values we refer to the documentation provided within the code . Sample code snippetIn our research work we use optfrog mainly in script mode. An exemplary data postprocessing script, reproducing  discussed in section 3 below, is shown in listing 1. Therein, after importing the functionality of numpy, optfrog, and a custom figure generating routine in lines 1-3, the location of the input data (line 5) and filter options for the spectrogram output-data (lines 6p) are specified. Note that the user defined window function (lines 9p) does not need to be normalized. After loading the input data (lines 12p) the routine optFrog is used to compute an optimized spectrogram in line 15. Finally, a visual account of the latter is prepared by the routine spectrogramFigure in line 17. Illustrative ExamplesSo as to demonstrate the functionality of optfrog we consider the numerical propagation of a short and intense few-cycle optical pulse in presence of the refractive index profile of an \"endlessly single mode\" (ESM) photonic crystal fiber . The underlying unidirectional propagation model includes the Kerr effect and a delayed Raman response of Hollenbeck-Cantrell type . For the preparation of the initial condition we considered a single soliton with duration t 0 = 7 fs, i.e. approximately 3.8 cycles, and soliton order N s = 8, prepared at a center frequency \u03c9 = 1.7 rad/fs. See Refs.  for a detailed account of the propagation model and Ref.  for a more thorough discussion of the particular problem setup. In  we illustrate the time-frequency characteristics of the pulse at propagation distance z = 0.12 m by using a Gaussian window function h(t, \u03c3) centered at t = 0 and having rms-width \u03c3. Note that the delay time \u03c4 has to be interpreted as being relative to the origin of a co-moving frame of reference in which the soliton is initially at rest.In  we demonstrate an inevitable drawback of a trial-and-error choice of a window function used for calculating a spectrogram. As discussed earlier, the properties of the window implies a trade-off in resolution that might be achieved. I.e., if the user opts for a window function that is either too wide or too narrow in comparison to the signal features in the time domain, only one marginal will approximate its underlying original intensity well and, as a result, the spectrogram will appear distorted. This is shown in , where a vanillaFrog trace using \u03c3 = 140 fs demonstrates a good frequency resolution and a bad time resolution. Conversely, as evident from , a vanillaFrog trace using \u03c3 = 10 fs yields a good time resolution and a bad frequency resolution. In contrast, if the IAEs of both marginals are minimized simultaneously by aid of a numerical algorithm, both marginals of the optimized spectrogram are found to approximate the original intensities per unit time and frequency similarly well. Consequently, the resulting spectrogram provides a most reasonable time-frequency representation of the underlying signal. To demonstrate this, the balanced (\u03b1 = 0) optFrog trace for the optimized window function, obtained for \u03c3 = 39.1 fs with Q = 0.39, is shown in . ImpactComputing reliable spectrograms represents an integral part in the analysis of the characteristics of ultrashort optical pulses. The publicly available and free Python package optfrog performs the nontrivial task of computing such spectrograms with optimized time-frequency resolution. It is based on a computational approach to parameter optimization in opposition to common trial-and-error approaches, helping to save time and effort and yielding reproducible results independent of the skill of the individual user. It addresses researchers in the field of ultrashort pulse propagation and related disciplines where signal analysis in terms of short-time Fourier transforms is of relevance. As independent software postprocessing tool it is ideally suited for the analysis of output data obtained by existing pulse propagation codes, as, e.g., the open source LaserFOAM (Python)  and gnlse (Matlab)  solver for the generalized nonlinear Schr\u00f6dinger equation. ConclusionsThe optfrog Python package provides easy-to-use tools that yield a time-frequency representation of a real valued input signal and allow to quantify how well the resulting spectrogram approximates the signal under scrutiny for a user supplied window function.We have shown how optfrog can be used to calculate analytic signal based spectrograms that are optimal in the sense that their visual inspection exhibits a minimal amount of distortion, allowing for a reliable interpretation of the time-frequency composition of the input signal.The optfrog software tool, including scripts that implement the exemplary use-cases illustrated in section 3, is available for download and installation under Ref. . C8 Support email for questions oliver.melchert@hot.uni-hannover.de\n###\n"}
{"summary": "has benchmark: Benchmark HoC/Benchmark NCBI Disease/Benchmark BC5CDR-disease/Benchmark JNLPBA/Benchmark BC2GM/Benchmark BC5CDR-chemical/Benchmark BioASQ/Benchmark PubMedQA/Benchmark DDI/Benchmark ChemProt", "text": "#Properties\nhas benchmark\n#Text\nPretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this paper, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly-available datasets. Our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical NLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with BERT models, such as using complex tagging schemes in named entity recognition (NER). To help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and task-specific models for the community, and created a leaderboard featuring our BLURB benchmark (short for Biomedical Language Understanding & Reasoning Benchmark) at https://aka.ms/BLURB. CCS Concepts: \u2022 Computing methodologies \u2192 Natural language processing; \u2022 Applied computing \u2192 Bioinformatics. METHODS 2.1 Language Model PretrainingIn this section, we provide a brief overview of neural language model pretraining, using BERT  as a running example.2.1.1 Vocabulary. We assume that the input consists of text spans, such as sentences separated by special tokens . To address the problem of out-of-vocabulary words, neural language models generate a vocabulary from subword units, using Byte-Pair Encoding (BPE)  or variants such as WordPiece . Essentially, the BPE algorithm tries to greedily identify a small set of subwords that can compactly form all words in the given corpus. It does this by first shattering all words in the corpus and initializing the vocabulary with characters and delimiters. It then iteratively augments the vocabulary with a new subword that is most frequent in the corpus and can be formed by concatenating two existing subwords, until the vocabulary reaches the pre-specified size (e.g., ,000 in standard BERT models or 50,000 in RoBERTa ). In this paper, we use the WordPiece algorithm which is a BPE variant that uses likelihood based on the unigram language model rather than frequency in choosing which subwords to concatenate. The text corpus and vocabulary may preserve the case (cased) or convert all characters to lower case (uncased). ModelArchitecture. State-of-the-art neural language models are generally based on transformer architectures , following the recent success of BERT . The transformer model introduces a multi-layer, multi-head self-attention mechanism, which has demonstrated superiority in leveraging GPU-based parallel computation and modeling long-range dependencies in texts, compared to recurrent neural networks, such as LSTMs . The input token sequence is first processed by a lexical encoder, which combines a token embedding, a (token) position embedding and a segment embedding (i.e., which text span the token belongs to) by element-wise summation. This embedding layer is then passed to multiple layers of transformer modules . In each transformer layer, a contextual representation is generated for each token by summing a non-linear transformation of the representations of all tokens in the prior layer, weighted by the attentions computed using the given token's representation in the prior layer as the query. The final layer outputs contextual representations for all tokens, which combine information from the whole text span.  is the use of a Masked Language Model (MLM) for self-supervised pretraining. Traditional language models are typically generative models that predict the next token based on the preceding tokens; for example, n-gram models represent the conditional probability of the next token by a multinomial of the preceding n-gram, with various smoothing strategies to handle rare occurrences . Masked Language Model instead randomly replaces a subset of tokens by a special token (e.g., ), and asks the language model to predict them. The training objective is the cross-entropy loss between the original tokens and the predicted ones. In BERT and RoBERTa, 15% of the input tokens are chosen, among which a random 80% are replaced by , 10% are left unchanged and 10% are randomly replaced by a token from the vocabulary. Instead of using a constant masking rate of 15%, a standard approach is to gradually increase it from 5% to 25% with 5% increment for every 20% of training epochs, which makes pretraining more stable . The original BERT algorithm also uses Next Sentence Prediction (NSP), which determines for a given sentence pair whether one sentence follows the other in the original text. The utility of NSP has been called into question , but we include it in our pretraining experiments to enable a head-to-head comparison with prior BERT models. Self-Supervision. A key innovation in BERT Advanced Pretraining Techniques.In the original formulation of BERT , the masked language model (MLM) simply selects random subwords to mask. When a word is only partially masked, it is relatively easy to predict the masked portion given the observed ones. In contrast, whole-word masking (WWM) enforces that the whole word must be masked if one of its subwords is chosen. This has been adopted as the standard approach because it forces the language model to capture more contextual semantic dependencies.In this paper, we also explore adversarial pretraining and its impact on downstream applications. Motivated by successes in countering adversarial attacks in computer vision, adversarial pretraining introduces perturbations in the input embedding layer that maximize the adversarial loss, thus forcing the model to not only optimize the standard training objective (MLM), but also minimize adversarial loss . Biomedical Language Model PretrainingIn this paper, we will use biomedicine as a running example in our study of domain-specific pretraining. In other words, biomedical text is considered in-domain, while others are regarded as out-domain. Intuitively, using in-domain text in pretraining should help with domain-specific applications. Indeed, prior work has shown that pretraining with PubMed text leads to better performance in biomedical NLP tasks . The main question is whether pretraining should include text from other domains. The prevailing assumption is that pretraining can always benefit from more text, including out-domain text. In fact, none of the prior biomedical-related BERT models have been pretrained using purely biomedical text . Here, we challenge this assumption and show that domain-specific pretraining from scratch can be superior to mixed-domain pretraining for downstream applications. Mixed-Domain Pretraining.The standard approach to pretraining a biomedical BERT model conducts continual pretraining of a general-domain pretrained model, as exemplified by BioBERT . Specifically, this approach would initialize with the standard BERT model , pretrained using Wikipedia and BookCorpus. It then continues the pretraining process with MLM and NSP using biomedical text. In the case of BioBERT, continual pretraining is conducted using PubMed abstracts and PubMed Central full text articles. BlueBERT  uses both PubMed text and de-identified clinical notes from MIMIC-III .Note that in the continual pretraining approach, the vocabulary is the same as the original BERT model, in this case the one generated from Wikipedia and BookCorpus. While convenient, this is a major disadvantage for this approach, as the vocabulary is not representative of the target biomedical domain.Compared to the other biomedical-related pretraining efforts, SciBERT  is a notable exception as it generates the vocabulary and pretrains from scratch, using biomedicine and computer science as representatives for scientific literature. However, from the perspective of biomedical applications, SciBERT still adopts the mixed-domain pretraining approach, as computer science text is clearly out-domain. Domain-SpecificPretraining from Scratch. The mixed-domain pretraining approach makes sense if the target application domain has little text of its own, and can thereby benefit from pretraining using related domains. However, this is not the case for biomedicine, which has over thirty million abstracts in PubMed, and adds over a million each year. We thus hypothesize that domain-specific pretraining from scratch is a better strategy for biomedical language model pretraining.  . Comparison of common biomedical terms in vocabularies used by the standard BERT, SciBERT and PubMedBERT (ours). A \u2713 indicates the biomedical term appears in the corresponding vocabulary, otherwise the term will be broken into word pieces as separated by hyphen. These word pieces often have no biomedical relevance and may hinder learning in downstream tasks. Biomedical TermA major advantage of domain-specific pretraining from scratch stems from having an in-domain vocabulary.  compares the vocabularies used in various pretraining strategies. BERT models using continual pretraining are stuck with the original vocabulary from the general-domain corpora, which does not contain many common biomedical terms. Even for SciBERT, which generates its vocabulary partially from biomedical text, the deficiency compared to a purely biomedical vocabulary is substantial. As a result, standard BERT models are forced to divert parametrization capacity and training bandwidth to model biomedical terms using fragmented subwords. For example, naloxone, a common medical term, is divided into four pieces ([na, ##lo, ##xon, ##e]) by BERT, and acetyltransferase is shattered into seven pieces ([ace, ##ty, ##lt, ##ran, ##sf, ##eras, ##e]) by BERT.  Both terms appear in the vocabulary of PubMedBERT.Another advantage of domain-specific pretraining from scratch is that the language model is trained using purely in-domain data. For example, SciBERT pretraining has to balance optimizing for biomedical text and computer science text, the latter of which is unlikely to be beneficial for biomedical applications. Continual pretraining, on the other hand, may potentially recover from out-domain modeling, though not completely. Aside from the vocabulary issue mentioned earlier, neural network training uses non-convex optimization, which means that continual pretraining may not be able to completely undo suboptimal initialization from the general-domain language model.In our experiments, we show that domain-specific pretraining with in-domain vocabulary confers clear advantages over mixed-domain pretraining, be it continual pretraining of general-domain language models, or pretraining on mixed-domain text. BLURB: A Comprehensive Benchmark for Biomedical NLPBioBERT  SciBERT  . Comparison of the biomedical datasets in prior language model pretraining studies and BLURB.The ultimate goal of language model pretraining is to improve performance on a wide range of downstream applications. In general-domain NLP, the creation of comprehensive benchmarks, such as GLUE , greatly accelerates advances in language model pretraining by enabling head-to-head comparisons among pretrained language models. In contrast, prior work on biomedical pretraining tends to use different tasks and datasets for downstream evaluation, as shown in . This makes it hard to assess the impact of pretrained language models on the downstream tasks we care about. To the best of our knowledge, BLUE  is the first attempt to create an NLP benchmark in the biomedical domain. We aim to improve on its design by addressing some of its limitations. First, BLUE has limited coverage of biomedical applications used in other recent work on biomedical language models, as shown in . For example, it does not include any question-answering task. More importantly, BLUE mixes PubMed-based biomedical applications (six datasets such as BC5, ChemProt, and HoC) with MIMIC-based clinical applications (four datasets such as i2b2 and MedNLI). Clinical notes differ substantially from biomedical literature, to the extent that we observe BERT models pretrained on clinical notes perform poorly on biomedical tasks, similar to the standard BERT. Consequently, it is advantageous to create separate benchmarks for these two domains.To facilitate investigations of biomedical language model pretraining and help accelerate progress in biomedical NLP, we create a new benchmark, the Biomedical Language Understanding & Reasoning Benchmark (BLURB). We focus on PubMed-based biomedical applications, and leave the exploration of the clinical domain, and other high-value verticals to future work. To make our effort tractable and facilitate head-to-head comparison with prior work, we prioritize the selection of datasets used in recent work on biomedical language models, and will explore the addition of other datasets in future work.  . Datasets used in the BLURB biomedical NLP benchmark. We list the numbers of instances in train, dev, and test (e.g., entity mentions in NER and PICO elements in evidence-based medical information extraction). DatasetBLURB is comprised of a comprehensive set of biomedical NLP tasks from publicly available datasets, including named entity recognition (NER), evidence-based medical information extraction (PICO), relation extraction, sentence similarity, document classification, and question answering. See  for an overview of the BLURB datasets. For question answering, prior work has considered both classification tasks (e.g., whether a reference text contains the answer to a given question) and more complex tasks such as list and summary . The latter types often require additional engineering effort that are not relevant to evaluating neural language models. For simplicity, we focus on the classification tasks such as yes/no question-answering in BLURB, and leave the inclusion of more complex question-answering to future work.To compute a summary score for BLURB, the simplest way is to report the average score among all tasks. However, this may place undue emphasis on simpler tasks such as NER for which there are many existing datasets. Therefore, we group the datasets by their task types, compute the average score for each task type, and report the macro average among the task types. To help accelerate research in biomedical NLP, we release the BLURB benchmark as well as a leaderboard at  are detailed descriptions for each task and corresponding datasets. Named Entity Recognition (NER).BC5-Chemical & BC5-Disease. The BioCreative V Chemical-Disease Relation corpus  was created for evaluating relation extraction of drug-disease interactions, but is frequently used as a NER corpus for detecting chemical (drug) and disease entities. The dataset consists of 1500 PubMed abstracts broken into three even splits for training, development, and test. We use a pre-processed version of this dataset generated by Crichton et al. , discard the relation labels, and train NER models for chemical (BC5-Chemical) and disease (BC5-Disease) separately.NCBI-Disease. The Natural Center for Biotechnology Information Disease corpus  contains 793 PubMed abstracts with 6892 annotated disease mentions linked to 790 distinct disease entities. We use a pre-processed set of train, development, test splits generated by Crichton et al. . BC2GM. The Biocreative II Gene Mention corpus  consists of sentences from PubMed abstracts with manually labeled gene and alternative gene entities. Following prior work, we focus on the gene entity annotation. In its original form, BC2GM contains 15000 train and 5000 test sentences. We use a pre-processed version of the dataset generated by Crichton et al. , which carves out 2500 sentences from the training data for development.JNLPBA. The Joint Workshop on Natural Language Processing in Biomedicine and its Applications shared task  is a NER corpus on PubMed abstracts. The entity types are chosen for molecular biology applications: protein, DNA, RNA, cell line, and cell type. Some of the entity type distinctions are not very meaningful. For example, a gene mention often refers to both the DNA and gene products such as the RNA and protein. Following prior work that evaluates on this dataset , we ignore the type distinction and focus on detecting the entity mentions. We use the same train, development, and test splits as in Crichton et al. . Evidence-Based Medical Information Extraction (PICO).EBM PICO. The Evidence-Based Medicine corpus  contains PubMed abstracts on clinical trials, where each abstract is annotated with P, I, and O in PICO: Participants (e.g., diabetic patients), Intervention (e.g., insulin), Comparator (e.g., placebo) and Outcome (e.g., blood glucose levels). Comparator (C) labels are omitted as they are standard in clinical trials: placebo for passive control and standard of care for active control. There are 4300, 500, and 200 abstracts in training, development, and test, respectively. The training and development sets were labeled by Amazon Mechanical Turkers, whereas the test set was labeled by Upwork contributors with prior medical training. EBM PICO provides labels at the word level for each PIO element. For each of the PIO elements in an abstract, we tally the F1 score at the word level, and then compute the final score as the average among PIO elements in the dataset. Occasionally, two PICO elements might overlap with each other (e.g., a participant span might contain within it an intervention span). In EBM-PICO, about 3% of the PIO words are in the overlap. Note that the dataset released along with SciBERT appears to remove the overlapping words from the larger span (e.g., the participant span as mentioned above). We instead use the original dataset  and their scripts for preprocessing and evaluation. Relation Extraction.ChemProt. The Chemical Protein Interaction corpus  consists of PubMed abstracts annotated with chemicalprotein interactions between chemical and protein entities. There are 23 interactions organized in a hierarchy, with 10 high-level interactions (including NONE). The vast majority of relation instances in ChemProt are within single sentences. Following prior work , we only consider sentence-level instances. We follow the ChemProt authors' suggestions and focus on classifying five high-level interactions -UPREGULATOR (CPR : 3), DOWNREGULATOR (CPR : 4), AGONIST (CPR : 5), ANTAGONIST (CPR : 6), SUBSTRATE (CPR : 9) -as well as everything else (false). The ChemProt annotation is not exhaustive for all chemical-protein pairs. Following previous work , we expand the training and development sets by assigning a false label for all chemical-protein pairs that occur in a training or development sentence, but do not have an explicit label in the ChemProt corpus. Note that prior work uses slightly different label expansion of the test data. To facilitate head-to-head comparison, we will provide instructions for reproducing the test set in BLURB from the original dataset.DDI. The Drug-Drug Interaction corpus  was created to facilitate research on pharmaceutical information extraction, with a particular focus on pharmacovigilance. It contains sentence-level annotation of drug-drug interactions on PubMed abstracts. Note that some prior work  discarded 90 training files that the authors considered not conducive to learning drug-drug interactions. We instead use the original dataset and produce our train/dev/test split of 624/90/191 files. GAD.The Genetic Association Database corpus  was created semi-automatically using the Genetic Association Archive.  Specifically, the archive contains a list of gene-disease associations, with the corresponding sentences in the PubMed abstracts reporting the association studies. Bravo et al.  used a biomedical NER tool to identify gene and disease mentions, and create the positive examples from the annotated sentences in the archive, and negative examples from gene-disease co-occurrences that were not annotated in the archive. We use an existing preprocessed version of GAD and its corresponding train/dev/test split created by Lee et al. . Sentence Similarity.BIOSSES. The Sentence Similarity Estimation System for the Biomedical Domain  contains 100 pairs of PubMed sentences each of which is annotated by five expert-level annotators with an estimated similarity score in the range from 0 (no relation) to 4 (equivalent meanings). It is a regression task, with the average score as the final annotation. We use the same train/dev/test split in Peng et al.  and use Pearson correlation for evaluation. Document Classification.HoC. The Hallmarks of Cancer corpus was motivated by the pioneering work on cancer hallmarks . It contains annotation on PubMed abstracts with binary labels each of which signifies the discussion of a specific cancer hallmark. The authors use 37 fine-grained hallmarks which are grouped into ten top-level ones. We focus on predicting the top-level labels. The dataset was released with 1499 PubMed abstracts  and has since been expanded to 1852 abstracts . Note that Peng et al.  discarded a control subset of 272 abstracts that do not discuss any cancer hallmark (i.e., all binary labels are false). We instead adopt the original dataset and report micro F1 across the ten cancer hallmarks. Though the original dataset provided sentence level annotation, we follow the common practice and evaluate on the abstract level . We create the train/dev/test split, as they are not available previously. Question Answering (QA).PubMedQA. The PubMedQA dataset  contains a set of research questions, each with a reference text from a PubMed abstract as well as an annotated label of whether the text contains the answer to the research question (yes/maybe/no). We use the original train/dev/test split with 450, 50, and 500 questions, respectively.BioASQ. The BioASQ corpus  contains multiple question answering tasks annotated by biomedical experts, including yes/no, factoid, list, and summary questions. Pertaining to our objective of comparing neural language models, we focus on the the yes/no questions (Task 7b), and leave the inclusion of other tasks to future work. Each question is paired with a reference text containing multiple sentences from a PubMed abstract and a yes/no answer. We use the official train/dev/test split of 670/75/140 questions. Task-Specific Fine-TuningPretrained neural language models provide a unifying foundation for learning task-specific models. Given an input token sequence, the language model produces a sequence of vectors in the contextual representation. A task-specific prediction model is then layered on top to generate the final output for a task-specific application. Given task-specific training data, we can learn the task-specific model parameters and refine the BERT model parameters by gradient descent using backpropragation.Prior work on biomedical NLP often adopts different task-specific models and fine-tuning methods, which makes it difficult to understand the impact of an underlying pretrained language model on task performance. In this section, we review standard methods and common variants used for each task. In our primary investigation comparing pretraining strategies, we fix the task-specific model architecture using the standard method identifed here, to facilitate a head-to-head comparison among the pretrained neural language models. Subsequently, we start with the same pretrained BERT model, and conduct additional investigation on the impact for the various choices in the task-specific models. For prior biomedical BERT models, our standard task-specific methods generally lead to comparable or better performance when compared to their published results. A GeneralArchitecture for Fine-Tuning Neural Language Models.  shows a general architecture of fine-tuning neural language models for downstream applications. An input instance is first processed by a TransformInput module which performs task-specific transformations such as appending special instance marker (e.g., [CLS]) or dummifying entity mentions for relation extraction. The transformed input is then tokenized using the neural language model's vocabulary, and fed into the neural language model. Next, the contextual representation at the top layer is processed by a Featurizer module, and then fed into the Predict module to generate the final output for a given task.To facilitate a head-to-head comparison, we apply the same fine-tuning procedure for all BERT models and tasks. Specifically, we use cross-entropy loss for classification tasks and mean-square error for regression tasks. We conduct hyperparameter search using the development set based on task-specific metrics. Similar to previous work, we jointly fine-tune the parameters of the task-specific prediction layer as well as the underlying neural language model. Task-Specific Problem Formulation and ModelingChoices. Many NLP applications can be formulated as a classification or regression task, wherein either individual tokens or sequences are the prediction target. Modeling choices usually vary in two aspects: the instance representation and the prediction layer.  presents an overview of the problem formulation and modeling choices for tasks we consider and detailed descriptions are provided below. For each task, we highlight the standard modeling choices with an asterisk (*).NER. Given an input text span (usually a sentence), the NER task seeks to recognize mentions of entities of interest. It is typically formulated as a sequential labeling task, where each token is assigned a tag to signify whether it is in an entity mention or not. The modeling choices primarily vary on the tagging scheme and classification method. BIO is the standard tagging scheme that classifies each token as the beginning of an entity (B), inside an entity (I), or outside (O). The NER tasks in BLURB are only concerned about one entity type (in JNLPBA, all the types are merged into one). In the case when there are multiple entity types, the BI tags would be further divided into fine-grained tags for specific types. Prior work has also considered more complex tagging schemes such as BIOUL, where U stands for the last word of an entity and L stands for a single-word entity. We also consider the simpler IO scheme that only differentiates between in and out of an entity. Classification is done using a simple linear layer or more sophisticated sequential labeling methods such as LSTM or conditional random field (CRF) .\u2022 TransformInput: returns the input sequence as is.\u2022 Featurizer: returns the BERT encoding of a given token.\u2022 Tagging scheme: BIO*; BIOUL; IO.\u2022 Classification layer: linear layer*; LSTM; CRF.PICO. Conceptually, evidence-based medical information extraction is akin to slot filling, as it tries to identify the PIO elements in an abstract describing a clinical trial. However, it can be formulated as a sequential tagging task like NER, by classifying tokens belonging to each element. A token may belong to more than one element, e.g., participant (P) and intervention (I).\u2022 TransformInput: returns the input sequence as is.\u2022 Featurizer: returns the BERT encoding of a given token.\u2022 Tagging scheme: BIO*; BIOUL; IO.\u2022 Classification layer: linear layer*; LSTM; CRF.Relation Extraction. Existing work on relation extraction tends to focus on binary relations. Given a pair of entity mentions in a text span (typically a sentence), the goal is to determine if the text indicates a relation for the mention pair. There are significant variations in the entity and relation representations. To prevent overfitting by memorizing the entity pairs, the entity tokens are often augmented with start/end markers or replaced by a dummy token. For featurization, the relation instance is either represented by a special [CLS] token, or by concatenating the mention representations. In the latter case, if an entity mention contains multiple tokens, its representation is usually produced by pooling those of individual tokens (max or average). For computational efficiency, we use padding or truncation to set the input length to 128 tokens for GAD and 256 tokens for ChemProt and DDI which contain longer input sequences.\u2022 TransformInput: entity (dummification*; start/end marker; original); relation ([CLS]*; original).\u2022 Featurizer: entity (dummy token*; pooling); relation ([CLS] BERT encoding*; concatenation of the mention BERT encoding). \u2022 Classification layer: linear layer*; more sophisticated classifiers (e.g., MLP).Sentence Similarity. The similarity task can be formulated as a regression problem to generate a normalized score for a sentence pair. By default, a special [SEP] token is inserted to separate the two sentences, and a special [CLS] token is prepended to the beginning to represent the pair. The BERT encoding of [CLS] is used to compute the regression score.\u2022 Document Classification. For each text span and category (an abstract and a cancer hallmark in HoC), the goal is to classify whether the text belongs to the category. By default, a [CLS] token is appended to the beginning of the text, and its BERT encoding is passed on by the Featurizer for the final classification, which typically uses a simple linear layer.\u2022 Question Answering. For the two-way (yes/no) or three-way (yes/maybe/no) question-answering task, the encoding is similar to the sentence similarity task. Namely, a [CLS] token is prepended to the beginning, followed by the question and reference text, with a [SEP] token to separate the two text spans. The [CLS] BERT encoding is then used for the final classification. For computational efficiency, we use padding or truncation to set the input length to 512 tokens.\u2022 Experimental SettingsFor biomedical domain-specific pretraining, we generate the vocabulary and conduct pretraining using the latest collection of PubMed 5 abstracts: 14 million abstracts, 3.2 billion words, 21 GB. (The original collection contains over 4 billion words; we filter out any abstracts with less than 128 words to reduce noise.)We follow the standard pretraining procedure based on the Tensorflow implementation released by NVIDIA.  We use Adam  for the optimizer using a standard slanted triangular learning rate schedule with warm-up in 10% of steps and cool-down in 90% of steps. Specifically, the learning rate increases linearly from zero to the peak rate of 6 \u00d7 10 \u22124 in the first 10% of steps, and then decays linearly to zero in the remaining 90% of steps. Training is done for 62,500 steps with batch size of 8,192, which is comparable to the computation used in previous  biomedical pretraining.  The training takes about 5 days on one DGX-2 machine with 16 V100 GPUs. We find that the cased version has similar performance to the uncased version in preliminary experiments; thus, we focus on uncased models in this study. We use whole-word masking (WWM), with a masking rate of 15%. We denote the resulting BERT model PubMedBERT. For comparison, we use the public releases of BERT , RoBERTa , BioBERT , SciBERT , Clinical-BERT , and BlueBERT . See  for an overview. BioBERT and BlueBERT conduct continual pretraining from BERT, whereas ClinicalBERT conducts continual pretraining from BioBERT; thus, they all share the same vocabulary as BERT. BioBERT comes with two versions. We use BioBERT++ (v1.1), which was trained for a longer time and performed better. ClinicalBERT also comes with two versions. We use Bio+Clinical BERT.Prior pretraining work has explored two settings: BERT-BASE with 12 transformer layers and 100 million parameters; BERT-LARGE with 24 transformer layers and 300 million parameters. Prior work in biomedical pretraining uses BERT-BASE only. For head-to-head comparison, we also use BERT-BASE in pretraining Pub-MedBERT. BERT-LARGE appears to yield improved performance in some preliminary experiments. We leave an in-depth exploration to future work.For task-specific fine-tuning, we use Adam  with the standard slanted triangular learning rate schedule (warm-up in the first 10% of steps and cool-down in the remaining 90% of steps) and a dropout probability of 0.1. Due to random initialization of the task-specific model and drop out, the performance may vary for different random seeds, especially for small datasets like BIOSSES, BioASQ, and PubMedQA. We report the average scores from ten runs for BIOSSES, BioASQ, and PubMedQA, and five runs for the others.For all datasets, we use the development set for tuning the hyperparameters with the same range: learning rate (1e-5, 3e-5, 5e-5), batch size  and epoch number . Ideally, we would conduct separate hyperparameter tuning for each model on each dataset. However, this would incur a prohibitive amount of computation, as we have to enumerate all combinations of models, datasets and hyperparameters, each of which requires averaging over multiple runs with different randomization. In practice, we observe that the development performance is not very sensitive to hyperparameter selection, as long as they are in a ballpark range. Consequently, we focus on hyperparameter tuning using a subset of representative models such as BERT and BioBERT, and use a common set of hyperparameters for each dataset that work well for both out-domain and in-domain language models. RESULTSIn this section, we conduct a thorough evaluation to assess the impact of domain-specific pretraining in biomedical NLP applications. First, we fix the standard task-specific model for each task in BLURB, and conduct a head-tohead comparison of domain-specific pretraining and mixed-domain pretraining. Next, we evaluate the impact of various pretraining options such as vocabulary, whole-word masking (WWM), and adversarial pretraining. Finally, we fix a pretrained BERT model and compare various modeling choices for task-specific fine-tuning. Domain-Specific Pretraining vs Mixed-Domain Pretraining BERTRoBERTa BioBERT SciBERT ClinicalBERT BlueBERT PubMedBERT uncased cased cased cased uncased cased cased cased uncased . Comparison of pretrained language models on the BLURB biomedical NLP benchmark. The standard task-specific models are used in the same fine-tuning process for all BERT models. The BLURB score is the macro average of average test results for each of the six tasks (NER, PICO, relation extraction, sentence similarity, document classification, question answering). See  for the evaluation metric used in each task.We compare BERT models by applying them to the downstream NLP applications in BLURB. For each task, we conduct the same fine-tuning process using the standard task-specific model as specified in subsection 2.4.  shows the results.By conducting domain-specific pretraining from scratch, PubMedBERT consistently outperforms all the other BERT models in most biomedical NLP tasks, often by a significant margin. The gains are most substantial against BERT models trained using out-domain text. Notably, while the pretraining corpus is the largest for RoBERTa, its performance on biomedical NLP tasks is among the worst, similar to the original BERT model. Models using biomedical text in pretraining generally perform better. However, mixing out-domain data in pretraining generally leads to worse performance. In particular, even though clinical notes are more relevant to the biomedical domain than general-domain text, adding them does not confer any advantage, as evident by the results of ClinicalBERT and BlueBERT. Not surprisingly, BioBERT is the closest to PubMedBERT, as it also uses PubMed text for pretraining. However, by conducting domain-specific pretraining from scratch, including using the PubMed vocabulary, PubMedBERT is able to obtain consistent gains over BioBERT in most tasks. A notable exception is PubMedQA, but this dataset is small, and there are relatively high variances among runs with different random seeds.Compared to the published results for BioBERT, SciBERT, and BlueBERT in their original papers, our results are generally comparable or better for the tasks they have been evaluated on. The ClinicalBERT paper does not report any results on these biomedical applications .  . Evaluation of the impact of vocabulary and whole word masking on the performance of PubMedBERT on BLURB. Ablation Study on Pretraining TechniquesTo assess the impact of pretraining options on downstream applications, we conduct several ablation studies using PubMedBERT as a running example.  shows results assessing the effect of vocabulary and wholeword masking (WWM). Using the original BERT vocabulary derived from Wikipedia & BookCorpus (by continual pretraining from the original BERT), the results are significantly worse than using an in-domain vocabulary from PubMed. Additionally, WWM leads to consistent improvement across the board, regardless of the vocabulary in use. A significant advantage in using an in-domain vocabulary is that the input will be shorter in downstream tasks, as shown in , which makes learning easier.  shows examples of how domain-specific pretraining with in-domain vocabulary helps correct errors from mixed-domain pretraining.Furthermore, we found that pretraining on general-domain text provides no benefit even if we use the in-domain vocabulary; see . The first column corresponds to BioBERT, which conducted pretraining first on the general domain and then on PubMed. The second column adopted the same continual pretraining strategy, except that the in-domain vocabulary (from PubMed) was used, which actually led to slight degradation in performance. On the other hand, by conducting pretraining from scratch on PubMed, we attained similar performance even with half of the compute (third column), and attained significant gain with the same amount of compute (fourth column; PubMedBERT). In sum, general-domain pretraining confers no advantage here in domain-specific pretraining. . Examples of how domain-specific pretraining helps correct errors from mixed-domain pretraining. Top: attention for the leading word piece of the gene mention \"epithelial-restricted with serine box\" (abbreviation \"ESX\") in the BC2GM dataset. Bottom: attention for the [CLS] token in an instance of AGONIST relation between a pair of dummified chemical and protein. In both cases, we show the aggregate attention from the penultimate layer to the preceding layer, which tends to be most informative about the final classification. Note how BioBERT tends to shatter the relevant words by inheriting the general-domain vocabulary. The domain-specific vocabulary enables PubMedBERT to learn better attention patterns and make correct predictions.In our standard PubMedBERT pretraining, we used PubMed abstracts only. We also tried adding full-text articles from PubMed Central (PMC),  with the total pretraining text increased substantially to 16.8 billion words (107 GB). Surprisingly, this generally leads to a slight degradation in performance across the board. However, by VocabWiki  . Evaluation of the impact of pretraining corpora and time on the performance on BLURB. In the first two columns, pretraining was first conducted on Wiki & Books, then on PubMed abstracts. All use the same amount of compute (twice as long as original BERT pretraining), except for the third column, which only uses half (same as original BERT pretraining).extending pretraining for 60% longer (100K steps in total), the overall results improve and slightly outperform the standard PubMedBERT using only abstracts. The improvement is somewhat mixed across the tasks, with some  . Evaluation of the impact of pretraining text on the performance of PubMedBERT on BLURB. The first result column corresponds to the standard PubMedBERT pretrained using PubMed abstracts (\"PubMed\"). The second one corresponds to PubMedBERT trained using both PubMed abstracts and PMC full text (\"PubMed+PMC\"). The last one corresponds to PubMedBERT trained using both PubMed abstracts and PMC full text, for 60% longer (\"PubMed+PMC (longer training)\").gaining and others losing. We hypothesize that the reason for this behavior is two-fold. First, PMC inclusion is influenced by funding policy and differs from general PubMed distribution, and full texts generally contain more noise than abstracts. As most existing biomedical NLP tasks are based on abstracts, full texts may be slightly out-domain compared to abstracts. Moreover, even if full texts are potentially helpful, their inclusion requires additional pretraining cycles to make use of the extra information. Adversarial pretraining has been shown to be highly effective in boosting performance in general-domain applications . We thus conducted adversarial pretraining in PubMedBERT and compared its performance with standard pretraining . Surprisingly, adversarial pretraining generally leads to a slight degradation in performance, with some exceptions such as sentence similarity (BIOSSES). We hypothesize that the reason may be similar to what we observe in pretraining with full texts. Namely, adversarial training is most useful if the pretraining corpus is more diverse and relatively out-domain compared to the application tasks. We leave a more thorough evaluation of adversarial pretraining to future work. Ablation Study on Fine-Tuning MethodsIn the above studies on pretraining methods, we fix the fine-tuning methods to the standard methods described in subsection 2.4. Next, we will study the effect of modeling choices in task-specific fine-tuning, by fixing the underlying pretrained language model to our standard PubMedBERT (WWM, PubMed vocabulary, pretrained using PubMed abstracts).Prior to the current success of pretraining neural language models, standard NLP approaches were often dominated by sequential labeling methods, such as conditional random fields (CRF) and more recently recurrent neural networks such as LSTM. Such methods were particularly popular for named entity recognition (NER) and relation extraction.With the advent of BERT models and the self-attention mechanism, the utility of explicit sequential modeling becomes questionable. The top layer in the BERT model already captures many non-linear dependencies across the entire text span. Therefore, it's conceivable that even a linear layer on top can perform competitively. We find that this is indeed the case for NER and relation extraction, as shown in . The use of a bidirectional LSTM (Bi-LSTM) does not lead to any substantial gain compared to linear layer.We also investigate the tagging scheme used in NER. The standard tagging scheme distinguishes words by their positions within an entity. For sequential tagging methods such as CRF and LSTM, distinguishing the position within an entity is potentially advantageous compared to the minimal IO scheme that only distinguishes between inside and outside of entities. But for BERT models, once again, the utility of more complex tagging schemes is diminished. We thus conducted a head-to-head comparison of the tagging schemes using three biomedical NER tasks in BLURB. As we can see in , the difference is minuscule, suggesting that with self-attention, the sequential nature of the tags is less essential in NER modeling.  . Evaluation of the impact of entity dummification and relation encoding in relation extraction, all using PubMedBERT. With entity dummification, the entity mentions in question are anonymized using entity type tags such as $DRUG or $GENE. With entity marker, special tags marking the start and end of an entity are appended to the entity mentions in question. Relation encoding is derived from the special [CLS] token appended to the beginning of the text or the special entity start token, or by concatenating the contextual representation of the entity mentions in question.The use of neural methods also has subtle, but significant, implications for relation extraction. Previously, relation extraction was generally framed as a classification problem with manually-crafted feature templates. To prevent overfitting and enhance generalization, the feature templates would typically avoid using the entities in question. Neural methods do not need hand-crafted features, but rather use the neural encoding of the given text span, including the entities themselves. This introduces a potential risk that the neural network may simply memorize the entity combination. This problem is particularly pronounced in self-supervision settings, such as distant supervision, because the positive instances are derived from entity tuples with known relations. As a result, it is a common practice to \"dummify\" entities (i.e., replace an entity with a generic tag such as $DRUG or $GENE) .This risk remains in the standard supervised setting, such as in the tasks that comprise BLURB. We thus conducted a systematic evaluation of entity dummification and relation encoding, using two relation extraction tasks in BLURB.For entity marking, we consider three variants: dummify the entities in question; use the original text; add start and end tags to entities in question. For relation encoding, we consider three schemes. In the [CLS] encoding introduced by the original BERT paper, the special token [CLS] is prepended to the beginning of the text span, and its contextual representation at the top layer is used as the input in the final classification. Another standard approach concatenates the BERT encoding of the given entity mentions, each obtained by applying max pooling to the corresponding token representations. Finally, following prior work, we also consider simply concatenating the top contextual representation of the entity start tag, if the entity markers are in use .  shows the results. Simply using the original text indeed exposes the neural methods to significant overfitting risk. Using  with the original text is the worst choice, as the relation encoding has a hard time to distinguish which entities in the text span are in question. Dummification remains the most reliable method, which works for either relation encoding method. Interestingly, using entity markers leads to slightly better results in both datasets, as it appears to prevent overfitting while preserving useful entity information. We leave it to future work to study whether this would generalize to all relation extraction tasks. CONCLUSIONIn this paper, we challenge a prevailing assumption in pretraining neural language models and show that domainspecific pretraining from scratch can significantly outperform mixed-domain pretraining such as continual pretraining from a general-domain language model, leading to new state-of-the-art results for a wide range of biomedical NLP applications. To facilitate this study, we create BLURB, a comprehensive benchmark for biomedical NLP featuring a diverse set of tasks such as named entity recognition, relation extraction, document classification, and question answering. To accelerate research in biomedical NLP, we release our state-of-the-art biomedical BERT models and setup a leaderboard based on BLURB.\n###\n"}
{"summary": "Machine Learning Input: tabular\nExplanation Interpretability: Integrated Interpretability\nKnowledge Graph Type: Factual Knowledge Graph\nExplanation Type: Mechanistic Explanation\nKnowledge Graph Selection: manual\nKnowledge Graph Semantics: ABox\nMachine Learning Task: Classification\nMachine Learning Method: Collaborative Filtering\nExplanation Form: Text Explanation\nMachine Learning Model Integration: internal", "text": "#Properties\nMachine Learning Input, Explanation Interpretability, Knowledge Graph Type, Explanation Type, Knowledge Graph Selection, Knowledge Graph Semantics, Machine Learning Task, Machine Learning Method, Explanation Form, Machine Learning Model Integration\n#Text\nProviding model-generated explanations in recommender systems is important to user experience. State-of-the-art recommendation algorithms-especially the collaborative filtering (CF)-based approaches with shallow or deep models-usually work with various unstructured information sources for recommendation, such as textual reviews, visual images, and various implicit or explicit feedbacks. Though structured knowledge bases were considered in content-based approaches, they have been largely ignored recently due to the availability of vast amounts of data and the learning power of many complex models. However, structured knowledge bases exhibit unique advantages in personalized recommendation systems. When the explicit knowledge about users and items is considered for recommendation, the system could provide highly customized recommendations based on users' historical behaviors and the knowledge is helpful for providing informed explanations regarding the recommended items. A great challenge for using knowledge bases for recommendation is how to integrate large-scale structured and unstructured data, while taking advantage of collaborative filtering for highly accurate performance. Recent achievements in knowledge-base embedding (KBE) sheds light on this problem, which makes it possible to learn user and item representations while preserving the structure of their relationship with external knowledge for explanation. In this work, we propose to explain knowledge-base embeddings for explainable recommendation. Specifically, we propose a knowledge-base representation learning framework to embed heterogeneous entities for recommendation, and based on the embedded knowledge base, a soft matching algorithm is proposed to generate personalized explanations for the recommended items. Experimental results on real-world e-commerce datasets verified the superior recommendation performance and the explainability power of our approach compared with state-of-the-art baselines. Problem FormulationIn this paper, we focus on explainable product recommendation, where the objective of the recommender system is to recommend products to users and explain why the products are recommended.Formally, we first construct a knowledge-base as a set of triplets S = {(e h , e t , r)} for recommendation, where e h is a head entity, e t is a tail entity, and r is the relationship from e h to e t . Then the goal of explainable recommendation is twofold, 1) for each user u, find one or a set of items i that are most likely to be purchased by the user, and 2) for each retrieved user-item pair, construct a natural language sentence based on S to explain why the user should purchase the item. For simplicity, we consider 5 types of entities (i.e., e h or e t ) for explainable recommendation:\u2022 user: the users of the recommender system.\u2022 item: the products in the system to be recommended.\u2022 word: the words in product names, descriptions or reviews.\u2022 brand: the brand/producers of the product.\u2022 category: the categories that a product belongs to.Also, we consider 6 types of relationships (i.e., r) between entities:\u2022 Purchase: the relation from a user to an item, which means that the user has bought the item.\u2022 Mention: the relation from a user or an item to a word, which means the word is mentioned in the user's or item's reviews.\u2022 Belongs_to: the relation from an item to a category, which means that the item belongs to the category.\u2022 Produced_by: the relation from an item to a brand, which means that the item is produced by the brand.\u2022 Bought_together: the relation from an item to another item, which means that the items have been purchased together in a single transaction.\u2022 Also_bought: the relation from an item to another item, which means the items have been purchased by same users.\u2022 Also_viewed: the relation from an item to another item, which means that the second item was viewed before or after the purchase of the first item in a single session.Therefore, for explainable product recommendation, the first goal is to retrieve item i that are likely to have the Purchase relationship with user u, and the second goal is to provide explanations for the (u, i) pair based on the relations and entities related to them. Collaborative Filtering on Knowledge GraphsWe now describe our model for explainable recommendation. Our model is a CF model built on user-item knowledge graph. In this section, we first introduce how to model the entities and relations as a product knowledge graph, and then we discuss how to optimize the model parameters for recommendation. Relation Modeling as Entity TranslationsAs discussed previously, we assume that the product knowledge can be represented as a set of triplets S = {(e h , e t , r)}, where r is the relation from entity e h to entity e t . Because an entity can be associated with one or more other entities through a single or multiple relations, we propose to separate the modeling of entity and relation for CF. Specifically, we project each entity to a low-dimensional latent space and treat each relation as a translation function that converts one entity to another. Inspired by , we represent e h and e t as latent vectors e h \u2208 R d and e t \u2208 R d , and model their relationship r as a linear projection from e h to e t parameterized by r \u2208 R d , namely, e t = trans(e h , r) = e h + rTo learn the entity embeddings, we can construct a product knowledge graph by linking entities with the translation function in the latent space. An example generation process of such a graph is shown in .  . The construction process of knowledge graph with our model. Each entity is represented with a latent vector, and each relation is modeled as a linear translation from one entity to another entity parameterized by the relation embedding.Solving Equation (1) for all (e h , e t , r) \u2208 S, however, is infeasible in practice. On one hand, a trivial solution that constructs a single latent vector for all entities with the same type will lead to inferior recommendation performance as it ignores the differences between users and items. On the other hand, deriving a solution that assigns different latent vectors for all entities in S is mathematically impossible because an entity can be linked to multiple entities with a single relationship. For example, we cannot find a single vector for Also_viewed that translates an item to multiple items that have different latent representations.To solve the problems, we propose to relax the constrains of Equation and adopt an embedding-based generative framework to learn it. Empirically, we want the translation model trans(e h , r) \u2248 e t for an observed relation triplet (e h , e t , r) \u2208 S and trans(e h , r) = e t for an unobserved triplet (e h , e t , r) / \u2208 S. In other words, we want trans(e h , r) to assign high probability for observing e t but low probability for observing e t , which is exactly the goal of the embedding-based generative framework. The embedding-based generative framework is first proposed by Mikolov et al.  and has been widely used in word embedding , recommendation , and information retrieval tasks . Formally, for an observed relation triplet (e h , e t , r) \u2208 S, we can learn the translation model trans(e h , r) by optimizing the generative probability of e t given trans(e h , r), which is defined as:where E t is the set of all possible entities that share the same type with e t . Because Equation is a softmax function of e t over E t , the maximization of P(e t |trans(e h , r)) will explicitly increase the similarity of trans(e h , r) and e t but decease the similarity between trans(e h , r) and other entities. In this way, we convert Equation into an optimization problem that can be solved with iterative optimization algorithms such as gradient decent. Another advantage of the proposed model is that it provides a theoretically principled method to conduct soft match between tail entities and the translation model. This is important for the extraction of recommendation explanations, which will be discussed in Section 5. Optimization AlgorithmFor model optimization, we learn the representations of entities and relations by maximizing the likelihood of all observed relation triplets. Let S be the set of observed triplets (e h , e t , r) in the training data, then we can compute the likelihood of S defined as L(S) = log \u220f (e h ,e t ,r)\u2208S P(e t |trans(e h , r))where P(e t |trans(e h , r)) is the posterior probability of e t computed with Equation (2). The computation cost of L(S), however, is prohibitive in practice because of the softmax function. For efficient training, we adopt a negative sampling strategy to approximate P(e t |trans(e h , r)) . Specifically, for each observed relation triplet (e h , e t , r), we randomly sample a set of \"negative\" entities with the same type of e t . Then the log likelihood of (e h , e t , r) is approximated as log P(e t |trans(e h , r)) \u2248 log \u03c3(e t \u2022 trans(e h , r))where k is the number of negative samples, P t is a predefined noisy distribution over entities with the type of e t , and \u03c3(x) is a sigmoid function as \u03c3(x) = 1 1+e \u2212x . Therefore, L(S) can be reformulated as the sum of the log-likelihood of (e h , e t , r) \u2208 S asWe also tested the 2 -norm loss function used in TransE model and it does not provide any improvement compared to our inner product-based model with log-likelihood loss, and it is also difficulty for 2 -norm loss to generate expansions, as a result, we adopt our loss function for embedding, recommendation, and explanation in this work. To better illustrate the relationship between our model and a traditional CF method based on matrix factorization, we conduct the following analysis. Inspired by , we derive the local objective for the maximization of Equation (5) on a specific relation triplet (e h , e t , r):(e h , e t , r) = #(e h , e t , r) \u2022 log \u03c3(e t \u2022 trans(e h , r)) + k \u2022 #(e h , r) \u2022 P t (e t ) \u2022 log \u03c3(\u2212e t \u2022 trans(e h , r))  where #(e h , e t , r) and #(e h , r) are the frequency of (e h , e t , r) and (e h , r) in the training data. If we further compute the partial derivative of (e h , e t , r) with respect to x = e t \u2022 trans(e h , r), we have \u2202 (e h , e t , r)When the training process has converged, the partial derivative of (e h , e t , r) should be 0, and then we havex = e t \u2022 trans(e h , r) = log( #(e h , e t , r) #(e h , r)As we can see, the left-hand side is the product of the latent vectors for e t and trans(e h , r); and the right-hand side of Equation is a shifted version of the pointwise mutual information between e t and (e h , r). Therefore, maximizing the log likelihood of observed triplet set S with negative sampling is actually factorizing the matrix of mutual information between the head-tail entity pairs (e h , e t ) of relation r. From this perspective, our model is a variation of factorization methods that can jointly factorize multiple relation matrix on a product knowledge graph.As shown in Equation , the final objective of the proposed model is controlled by the noisy distribution P t . Similar to previous studies , we notice that the relationships with tail entities that have high frequency in the collection reveal less information about the properties of the head entity. Therefore, we define the noisy probability P t (e t ) for each relation r (except Purchase) as the frequency distribution of (e h , e t , r) in S so that the mutual information on frequent tail entities will be penalized in optimization process. For Purchase, however, we define P t as a uniform distribution to avoid unnecessary biases toward certain items. Algorithm 1: Recommendation Explanation Extractione \u2190 all entities in the entity set E t within z hops from e. 10P e \u2190 the probability of each entity in V e computed by Eq (10). 11R e \u2190 the paths from e to the space of each entity in V e . Recommendation Explanation with Knowledge ReasoningIn this section, we describe how to create recommendation explanations with the proposed model. We first introduce the concept of explanation path and describe how to generate natural language explanations with it. Then we propose a soft matching algorithm to find explanation path for any user-item pair in the latent space.An overview of our algorithm is shown in Algorithm 1. In the algorithm, we first conduct breath first search (BFS) with maximum depth z from the user e u and the item e i to find an explanation path that can potentially link them. We memorize the paths and compute the path probability with soft matching (Equation and ). Finally, we rank the explanation paths by their probabilities and return the best path to create the natural language explanation. In this work, we take z as a hyper-parameter and tune the parameter by increasing its value in the experiments to search for non-empty intersections. Explanation PathThe key to generate an explanation of the recommendation is to find a sound logic inference sequence from the user to the item in the knowledge graph. In this work, we propose to find such a sequence by constructing an explanation path between the user and the item in the latent knowledge space.Formally, let E r h and E r t be the sets of all possible head entities and tail entities for a relation r. We define an explanation path from entity e u to entity e i as two sets of relationwhereIn other words, there is an explanation path between e u and e i if there is an entity that can be inferred by both e u (with R \u03b1 ) and e i (with R \u03b2 ) with the observed relations in the knowledge graph.For better illustration, we depict a recommendation example where an item (iPad) is recommended to a user (Bob) in . As we can see, the word \"IOS\" can be inferred by iPad and Bob using the relation Mention; the brand Apple can be inferred by iPad using Produced_by, and by Bob using Purchase + Produced_by. Thus, we have two explanation paths that link Bob with iPad. To generate explanations, we can create simple templates base on the relation type and apply them to the explanation paths. For example, we can say that Bob may be interested in iPad because he often mentions \"IOS\" in his reviews, and \"IOS\"is often mentioned in the reviews of iPad; or that Bob may be interested in iPad because he often purchases products produced by Apple, and iPad is also produced by Apple. In these explanations, the italic words are entities and relations on the explanation path. Entity Soft MatchingFinding a valid explanation path with observed relations, however, is often difficult for an arbitrary user-item pair. In practice, product knowledge graphs tend to be sparse. For example, the density of user-item matrix in Amazon review datasets is usually below 0.1% . Thus, the available relation triplets in the observed data are limited. To solve the problem, we propose to conduct entity soft matching in the latent space for explanation construction.As discussed in Section 4.1, we learn the distributed representations of entities and relations by optimizing the generative probability of observed relation triplets in Equation . Thus, we can extend the softmax function to compute the probability of entity e x \u2208 E r m t given e u and the relation setwhere E r m t is the tail entity set of r m , and trans(e u , R \u03b1 ) = e u + \u2211 m \u03b1=1 r \u03b1 . Therefore, we can construct an explanation path for an arbitrary user e u and item e i with relation setst , and compute the probability of this explanation path as:To find the best explanation for (e u , e i ), we can rank all paths by P(e x |e u , R \u03b1 , e i , R \u03b2 ) and pick up the best one to generate natural language explanations with predefined templates. It should be noted that learning with single hops may not guarantee the quality of multiple hops matching, but it also significantly simplifies the design of the training algorithm and increases the generalizability of the model, and helps to generate explanations more easily. Besides, using the single-hop training strategy has already been also to compete with many of the baselines. However, we believe that model training with multiple hops directly is a promising problem and we will design new models for this problem as a future work. Experimental SetupIn this section, we introduce the test bed of our experiments and discuss our evaluation settings in details. DatasetsWe conducted experiments on the Amazon review dataset , which contains product reviews in 24 categories on Amazon.com and rich metadata such as prices, brands, etc. Specifically, we used the 5-core data of CDs and Vinyl, Clothing, Cell Phones, and Beauty, in which each user or item has at least 5 associated reviews.Statistics about entities and relations used in our experiments are shown in . Overall, the interactions between users, items and other entities are highly sparse. For each dataset, we randomly sampled 70% of user purchase as the training data and used the rest 30% as the test set. This means that each user has at least 3 reviews observed in the training process and 2 reviews hidden for evaluation purposes. Thus, the objective of product recommendation is to find and recommend items that are purchased by the user in the test set. EvaluationTo verify the effectiveness of the proposed model, we adopt six representative and state-of-the-art methods as baselines for performance comparison. Three of them are traditional recommendation methods based on matrix factorization (BPR , BPR-HFT , and VBPR ), and the other three are deep models for product recommendation (DeepCoNN , CKE , and JRL ).\u2022 BPR: The Bayesian personalized ranking  model is a popular method for top-N recommendation that learns latent representations of users and items by optimizing the pairwise preferences between different user-item pairs. In this paper, we adopt matrix factorization as the prediction component for BPR.\u2022 BPR-HFT: The hidden factors and topics (HFT) model  integrates latent factors with topic models for recommendation. The original HFT model is optimized for rating prediction tasks. For fair comparison, we learn the model parameters under the pairwise ranking framework of BPR for top-N recommendation.\u2022 VBPR: The visual Bayesian personalized ranking  model is a state-of-the-art method that incorporate product image features into the framework of BPR for recommendation.\u2022 TransRec: The translation-based recommendation approach proposed in , which takes items as entities and users as relations, and leveraged translation-based embeddings to learn the similarity between user and items for personalized recommendation. We adopted L 2 loss function, which was reported to have better performance in . Notice that TransRec is different from our model because our model treats both items and users as entities, and learns embedding representations for different types of knowledge (e.g., brands, categories) as well as their relationships.\u2022 DeepCoNN: The Deep Cooperative Neural Networks model for recommendation  is a neural model that applies a convolutional neural network (CNN) over the textual reviews to jointly model users and items for recommendation.\u2022 CKE: The collaborative KBE model is a state-of-the-art neural model  that integrates text, images, and knowledge base for recommendation. It is similar to our model as they both use text and structured product knowledge, but it builds separate models on each type of data to construct item representations while our model constructs a knowledge graph that jointly embeds all entities and relations. \u2022 JRL:The joint representation learning model  is a state-of-the-art neural recommender, which leverage multi-model information including text, images and ratings for Top-N recommendation.The performance evaluation is conducted on the test set where only purchased items are considered to be relevant to the corresponding user. Specifically, we adopt four ranking measures for top-N recommendation, which are the Normalized Discounted Cumulative Gain (NDCG), Precision (Prec.), Recall, and the percentage of users that have at least one correct recommendation (Hit-Ratio, HR). All ranking metrics are computed based on the top-10 results for each test user. Significant test is conducted based on the Fisher randomization test . Parameter SettingsOur model is trained with stochastic gradient decent on a Nvidia Titan X GPU. We set the initial learning rate as 0.5 and gradually decrease it to 0.0 during the training process. We set the batch size as 64 and clip the norm of batch gradients with 5. For each dataset, we train the model for 20 epochs and set the negative sampling number as 5. We tune the dimension of embeddings from 10 to 500 ( , 300, 400, 500]) and report the best performance of each model in Section 7.We also conduct five-fold cross-validation on the training data to tune the hyper-parameters for baselines. For BPR-HFT, the best performance is achieved when the number of topics is 10. For BPR and VBPR, the regularization coefficient \u03bb = 10 worked the best in most cases. Similar to our model, we tune the number of latent factors (the embedding size) from 10 to 500 and only report the best performance of each baseline. Recommendation PerformanceOur experiments mainly focus on two research questions:\u2022 RQ1: Does incorporating knowledge-base in our model produce better recommendation performance?\u2022 RQ2: Which types of product knowledge are most useful for top-N recommendation?\u2022 RQ3: What is the efficiency of our knowledge-enhanced recommendation model compared to other algorithms?To answer RQ1, we report the results of our model and the baseline methods in . As shown in , the deep models with rich auxiliary information (DeepCoNN, CKE, and JRL) perform better in general than the shallow methods (BPR, BPR-HFT, VBPR, TransRec) on most datasets, which is coherent with previous studies . Among different neural baselines, JRL obtains the best performance in our experiments. It produced 80% or more improvements over the matrix factorization baselines and 10% or more over the other deep recommendation models. Overall, our model outperformed all the baseline models consistently and significantly. It obtained 5.6% NDCG improvement over the best baseline (i.e., JRL) on CDs and Vinyl, 78.16% on Clothing, 23.05% on Cell Phones, and 45.56% on Beauty. This shows that the proposed model can effectively incorporate product knowledge graph and is highly competitive for top-N recommendation. . Performance of the baselines and our model on top-10 recommendation. All the values in the table are percentage numbers with '%' omitted, and all differences are significant at p < 0.05. The stared numbers ( * ) indicate the best baseline performances, and the bolded numbers indicate the best performance of each column. The last line shows the percentage improvement of our model against the best baseline (i.e., JRL), which are significant at p < 0.001. DatasetCDs   depicts the recommendation performance of our model and baseline methods with different embedding sizes on CDs & Vinyl and Beauty datasets. Observations on the other two datasets are similar. As shown in , the recommendation methods based on shallow models (BPR, BPR-HFT, and VBPR) obtain the best NDCG when the embedding size is fairly small (from 10 to 100), and larger embedding sizes usually hurt the performance of these models. In contrast to the shallow models, the results of neural models (i.e., JRL, DeepCoNN, CKE, and our model) show positive correlations with the increase of embedding sizes. For example, the NDCG of the best baseline (JRL) and our model improves when the embedding size increases from 10 to 300, and remains stable afterwards. Overall, our model is robust to the variation of embedding sizes and consistently outperformed the baselines. In , we see that the CKE model did not perform as well as we expected. Although it has incorporated reviews, images and all other product knowledge described in this paper, the CKE model did not perform as well as JRL and our model. One possible reason is that CKE only considers heterogeneous information in the construction of item representations, but it does not directly leverage the information for user modeling. Another potential reason is that CKE separately constructs three latent spaces for text, image and other product knowledge, which makes it difficult for information from different types of data to propagate among the entities. Either way, this indicates that the embedding-based relation modeling of our model is a better way to incorporate structured knowledge for product recommendation.From the results we can also see that datasets of different density result in different performance in our model. In particular, denser datasets (Cell Phone and Beauty) generally get better ranking performance than sparser datasets (CD & Vinyl and Clothing) in our model, which means more sufficient information can help to learn better models in our algorithm.To answer RQ2, we experiment the performance of our model when using different relations. Because we eventually need to provide item recommendations for users, our approach would at least need the Purchase relation to model the user purchase histories. As a result, we train and test our model built with only the Purchase relation, as well as Purchase plus one another relation separately. As shown in , the relative performance of our models built on different relations varies considerably on the four datasets, which makes it difficult to conclude which type of product knowledge is the globally most useful one. This, however, is not surprising because the value of relation data depends on the properties of the candidate products. On CDs and Vinyl, where most products are music CDs, the CD covers did not reveal much information, and people often express their tastes and preferences in the reviews they wrote. Thus Mention turns out to be the most useful relation. On Clothing, however, reviews are not as important as the appearance or picture of the clothes, instead, it is easier to capture item similarities from the items that have been clicked and viewed by the same user. Therefore, adding Also_view relation produces the largest performance improvement for our model on Clothing. Overall, it is difficult to find a product knowledge that is universally useful for recommending products from all categories. However, we see that by modeling all of the heterogenous relation types, our final model outperforms all the baselines and outperforms all the simplified versions of our model with one or two types of relation, which implies that our KBE approach to recommendation is scalable to new relation types, and it has the ability to leverage very heterogeneous information sources in a unified manner.To answer RQ3, we compare the training efficiency of different methods in our experiments on the same Nvidia Titan X GPU platform. The testing procedures for all methods are quite efficient, and generating the recommendation list for a particular user on the largest CDs & Vinyl dataset requires less than 20 milliseconds. This is because after the model has learned the embeddings of the users and items, generating the recommendation list does not require re-computation of the embeddings and only needs to calculate their similarity. In terms of training efficiency, shallow models can be much more efficient than deep neural models, which is not surprising. For example, BPR or BPR-HFT can be trained within 30 minutes on the largest CDs & Vinyl dataset, while deep models such as DeepCoNN, CKE, and JRL takes about 10 hours on the same dataset, but they also bring much better recommendation performance. Our Explainable CF over Knowledge Graph (ECFKG) approach takes comparable training time on the largest dataset (about 10 hours), while achieving better recommendation performance than other deep neural baselines, which is a good balance between efficiency and effectiveness. Case Study for Explanation GenerationTo show the ability of our model to generate knowledge-enhanced explanations, we conduct case study for a test user (i.e., A1P27BGF8NAI29) from Cell Phones, for whom we have examined that the first recommendation (i.e., B009RXU59C) provided by the system is correct. We plot the translation process of this user to other entity subspaces with Purchase, Mention, Bought_together, and Belongs_to relations, as shown in . We also show the translation of the first recommended item B009RXU59C (B9C) using the same relations. The top 5 entities retrieved by our system for each translation are listed along with their probabilities computed based on Equation (10).  As we can see in , there are three explanation paths between the user A1P27BGF8NAI29 and the item B9C. The first and second paths are constructed by Purchase and Bought_together. According to our model, the user is linked to B008RDI0TU (BTU) and B00HNGB1YS (BYS) through Purchase+Bought_together with probabilities as 27.80% and 5.48%. The item B9C is linked to BTU and BYS through Bought_together directly with probabilities as 26.57% and 3.41%. The third path is constructed by Purchase and Belongs_to. The user is linked to the category Chargers with probability as 2.53% and B9C is linked to Chargers with probability as 30.90%. Therefore, we can create three natural language explanations for the recommendation of B9C by describing these explanation paths with simple templates. Example sentences and the corresponding confidences are listed below:\u2022 B9C is recommended because the user often purchases items that are bought with BTU together, and B9C is also frequently bought with BTU together (27.80% \u00d7 26.57% = 7.39%).\u2022 B9C is recommended because the user often purchases items that are bought with BYS together, and B9C is also frequently bought with BYS together (5.48% \u00d7 3.41% = 0.19%).\u2022 B9C is recommended because the user often purchases items related to the category Chargers, and B9C belongs to the category Chargers (2.53% \u00d7 30.90% = 0.78%).Among the explanation sentences, the best explanation should be the one with the highest confidence, which is the first sentence in this case.To better evaluate the quality of these recommendation explanations, we look at the details of each product shown in . On Amazom.com, B9C is a High-speed Wall Charger by New Trend for tablets, BTU is an iPad mini Keyboard Case, and BYS is an iPad Air Keyboard Case. If the generated explanations are reasonable, this means that the user has purchased some items that were frequently co-purchased with iPad accessories. Also, this indicates that there is a high probability that the user has an iPad. For validation proposes, we list the five training reviews written by the user in .As we can see in , the user has purchased several tablet accessories such as Bluetooth headsets and portable charger. The second review even explicitly mentions that the user has possessed an iPad and expresses concerns about the \"running-out-of-juice\" problem. Therefore, it is reasonable to believe that the user is likely to purchase stuff that are frequently co-purchased with iPad accessories such as iPad mini Keyboard Case (BTU) or iPad Air Keyboard Case (BYS), which are recommended as top items in our algorithm, and are also well-explained by the explanation paths in the knowledge graph. . The reviews written by the user A1P27BGF8NAI29 in the training data of Cell Phones. Review of Jabra VOX Corded Stereo Wired Headsets... I like to listen to music at work, but I must wear some sort of headset so that I do not create a disturbance. So, I have a broad experience in headsets ... Review of OXA Juice Mini M1 2600mAh... I recently had an experience, where I was about town and need to recharge my iPad, and so I tried this thing out. I plugged in the iPad, and it quickly charged it up, and at my next destination it was ready to go ... Review of OXA 8000mAh Solar External Battery Pack Portable... This amazing gadget is a solar powered charger for your small electronic device. This charger is (according to my ruler) 5-1/4 inches by 3 inches by about 6 inches tall. So, it is a bit big to place in the pocket ... Review of OXA Bluetooth Wristwatch Bracelet... I was far from thrilled with Bluetooth headset that I had, so I decided to give this device a try. Pros: The bracelet is not bad looking, ... Review of OXA Mini Portable Wireless Bluetooth Speaker... This little gadget is a Bluetooth speaker. It's fantastic! This speaker fits comfortably in the palm of your hand, ... Conclusions and OutlookIn this paper, we propose to learn over heterogenous KBE for personalized explainable recommendation. To do so, we construct the user-item knowledge graph to incorporate both user behaviors and our knowledge about the items. We further learn the KBE with the heterogenous relations collectively, and leverage the user and item embeddings to generate personalized recommendations. To explain the recommendations, we devise a soft matching algorithm to find explanation paths between a user and the recommended items in the latent KBE space. Experimental results on real-world datasets verified the superior performance of our approach, as well as its flexibility to incorporate multiple relation types.After years of success at integrating machine learning into recommender systems, we believe that equipping the systems with knowledge (again) is important to the future of recommender systems -or information systems in a broader sense -which can help to improve both the performance and explainability in the future.\n###\n"}
{"summary": "has benchmark: Benchmark ImageNet\nhas research problem: Image Classification\nhas model: Wide ResNet-50 (edge-popup)\nsame as: https://en.wikipedia.org/wiki/Contextual_image_classification", "text": "#Properties\nhas benchmark, has research problem, has model, same as\n#Text\nTraining a neural network is synonymous with learning the values of the weights. In contrast, we demonstrate that randomly weighted neural networks contain subnetworks which achieve impressive performance without ever modifying the weight values. Hidden in a randomly weighted Wide ResNet-50 [32] we find a subnetwork (with random weights) that is smaller than, but matches the performance of a ResNet-34 [9] trained on ImageNet [4]. Not only do these \"untrained subnetworks\" exist, but we provide an algorithm to effectively find them. We empirically show that as randomly weighted neural networks with fixed weights grow wider and deeper, an \"untrained subnetwork\" approaches a network with learned weights in accuracy. Our code and pretrained models are available at: https://github.com/allenai/hidden-networks. Neural Architecture Search (NAS)The advent of modern neural networks has shifted the focus from feature engineering to feature learning. However, researchers may now find themselves manually engineering the architecture of the network. Methods of Neural Architecture Search (NAS)  instead provide a mechanism for learning the architecture of neural network jointly with the weights. Models powered by NAS have recently obtained state of the art classification performance on ImageNet .As highlighted by Xie et al. , the connectivity patterns in methods of NAS remain largely constrained. Surprisingly, Xie et al. establish that randomly wired neural networks can achieve competitive performance. Accordingly, Wortsman et al.  propose a method of Discovering Neural Wirings (DNW) -where the weights and structure are jointly optimized free from the typical constraints of NAS. We highlight DNW as we use a similar method of analysis and gradient estimator to optimize our supermasks. In DNW, however, the subnetwork is chosen by taking the weights with the highest magnitude. There is therefore no way to learn supermasks with DNW as the weights and connectivity are inextricably linked -there is no way to separate the weights and the structure. Weight Agnostic Neural NetworksIn Weight Agnostic Neural Networks (WANNs) , Gaier and Ha question if an architecture alone may encode the solution to a problem. They present a mechanism for building neural networks that achieve high performance when each weight in the network has the same shared value. Importantly, the performance of the network is agnostic to the value itself. They are able to obtain \u223c 92% accuracy on MNIST .We are quite inspired by WANNs, though we would like to highlight some important distinctions. Instead of each weight having the same value, we explore the setting where each weight has a random value. In Section A.2.2 of their appendix, Gaier and Ha mention that they were not successful in this setting. However, we find a good subnetwork for a given random initialization -the supermasks we find are not agnostic to the weights. Finally,  . In the edge-popup Algorithm, we associate a score with each edge. On the forward pass we choose the top edges by score. On the backward pass we update the scores of all the edges with the straight-through estimator, allowing helpful edges that are \"dead\" to re-enter the subnetwork. We never update the value of any weight in the network, only the score associated with each weight. Linear Classifiers and Pruning at InitializationLinear classifiers on top of randomly weighted neural networks are often used as baselines in unsupervised learning . This work is different in motivation, we search for untrained subnetworks which achieve high performance without changing any weight values. This also differs from methods which prune at initialization and modify the weights of the discovered subnetwork  or methods which modify a subset of the weights . MethodIn this section we present our optimization method for finding effective subnetworks within randomly weighted neural networks. We begin by building intuition in an unusual setting -the infinite width limit. Next we motivate and present our algorithm for finding effective subnetworks. Intuition The Existence of Good SubnetworksModern neural networks have a staggering number of possible subnetworks. Consequently, even at initialization, a neural network should contain a subnetwork which performs well.To build intuition we will consider an extreme case -a neural network N in the infinite width limit (for a convolutional neural networks, the width of the network is the number of channels). As in , let \u03c4 be a network with the same structure of N that achieves good accuracy. If the weights of N are initialized using any standard scaling of a normal distribution, e.g. xavier  or kaiming , then we may show there exists a subnetwork of N that achieves the same performance as \u03c4 without training. Let q be the probability that a given subnetwork of N has weights that are close enough to \u03c4 to obtain the same accuracy. This probability q is extremely small, but it is still nonzero. Therefore, the probability that no subnetwork of N is close enough to \u03c4 is effectively (1 \u2212 q) S where S is the number of subnetworks. S grows very quickly with the width of the network, and this probability becomes arbitrarily small. How Should We Find A Good SubnetworkEven if there are good subnetworks in randomly weighted neural networks, how should we find them?Zhou et al. learn an associated probability p with each weight w in the network. On the forward pass they include weight w with probability p (where p is the output of a sigmoid) and otherwise zero it out. The infinite width limit provides intuition for a possible shortcoming of the algorithm presented by Zhou et al. . Even if the parameters p are fixed, the algorithm will likely never observe the same subnetwork twice. As such, the gradient estimate becomes more unstable, and this in turn may make training difficult.Our algorithm for finding a good subnetwork is illustrated by . With each weight w in the neural network we learn a positive, real valued popup score s. The subnetwork is then chosen by selecting the weights in each layer corresponding to the top-k% highest scores. For simplicity we use the same value of k for all layers.How should we update the score s uv ? Consider a single edge in a fully connected layer which connects neuron u to neuron v. Let w uv be the weight of this edge, and s uv the associated score. If this score is initially low then w uv is not selected in the forward pass. But we would still like a way to update its score to allow it to pop back up. Informally, with backprop  we compute how the loss \"wants\" node v's input to change (i.e. the negative gradient). We then examine the weighted output of node u. If this weighted output is aligned with the negative gradient, then node u can take node v's output where the loss \"wants\" it to go. Accordingly, we should increase the score. If this alignment happens consistently, then the score will continue to increase and the edge will re-enter the chosen subnetwork (i.e. popup).More formally, if w uv Z u denotes the weighted output of neuron u, and I v denotes the input of neuron v, then we update s uv asThis argument and the analysis that follows is motivated and guided by the work of . In their work, however, they do not consider a score and are instead directly updating the weights. In the forward pass they use the top k% of edges by magnitude, and therefore there is no way of learning a subnetwork without learning the weights. Their goal is to train sparse neural networks, while we aim to showcase the efficacy of randomly weighted neural networks. The edge-popup Algorithm and AnalysisWe now formally detail the edge-popup algorithm. For clarity, we first describe our algorithm for a fully connected neural network. In Section B.2 we provide the straightforward extension to convolutions along with code in PyTorch .A fully connected neural network consists of layers 1, ..., L where layer has n nodesWe let I v denote the input to node v and let Z v denote the output, where Z v = \u03c3(I v ) for some non-linear activation function \u03c3 (e.g. ReLU ). The input to neuron v in layer is a weighted sum of all neurons in the preceding layer. Accordingly, we write I v aswhere w uv are the network parameters for layer . The output of the network is taken from the final layer while the input data is given to the very first layer. Before training, the weights w uv for layer are initialized by independently sampling from distribution D . For example, if we are using kaiming normal initialization  with ReLU activations, then D = N 0, 2/n \u22121 where N denotes the normal distribution. Normally, the weights w uv are optimized via stochastic gradient descent. In our edge-popup algorithm, we instead keep the weights at their random initialization, and optimize to find a subnetwork G = (V, E). We then compute the input of node v in layer aswhere G is a subgraph of the original fully connected network  . As mentioned above, for each weight w uv in the original network we learn a popup score s uv . We choose the subnetwork G by selecting the weights in each layer which have the top-k% highest scores. Equation 3 may therefore be written equivalently aswhere h(s uv ) = 1 if s uv is among the top k% highest scores in layer and h(s uv ) = 0 otherwise. Since the gradient of h is 0 everywhere it is not possible to directly compute the gradient of the loss with respect to s uv . We instead use the straight-through gradient estimator , in which h is treated as the identity in the backwards pass -the gradient goes \"straight-through\" h. Consequently, we approximate the gradient to s uv a\u015dwhere L is the loss we are trying to minimize. The scores s uv are then updated via stochastic gradient descent with learning rate \u03b1. If we ignore momentum and weight decay  then we update s uv aswheres uv denotes the score after the gradient step 2 .As the scores change certain edges in the subnetwork will be replaced with others. Motivated by the analysis of  we show that when swapping does occur, the loss decreases for the mini-batch. Theorem 1: When edge (i, \u03c1) replaces (j, \u03c1) and the rest of the subnetwork remains fixed then the loss decreases for the mini-batch (provided the loss is sufficiently smooth). Proof. Lets uv denote the score of weight w uv after the gradient update. If edge (i, \u03c1) replaces (j, \u03c1) then our algorithm dictates that s i\u03c1 < s j\u03c1 buts i\u03c1 >s j\u03c1 . Accordingly,which implies thatby the update rule given in Equation 6. Let\u0128 \u03c1 denote the input to node k after the swap is made and I \u03c1 denote the original input. Note that\u0128 \u03c1 \u2212 I \u03c1 = w i\u03c1 Z i \u2212 w j\u03c1 Z j by Equation 3. We now wish to show that L(\u0128 \u03c1 ) < L (I \u03c1 ).If the loss is smooth and\u0128 \u03c1 is close to I \u03c1 and ignore second-order terms in a Taylor expansion:and from equation 8 we have that \u2202L \u2202I\u03c1 (w i\u03c1 Z i \u2212w j\u03c1 Z j ) < 0 and so L(\u0128 \u03c1 ) < L (I \u03c1 ) as needed. We examine a more general case of Theorem 1 in Section B.1 of the appendix. . As the network becomes deeper, we are able to find subnetworks at initialization that perform as well as the dense original network when trained. The baselines are drawn as a horizontal line as we are not varying the % of weights. When we write Weights \u223c D we mean that the weights are randomly drawn from distribution D and are never tuned. Instead we find subnetworks with size (% of Weights)/100 * (Total # of Weights). ExperimentsWe demonstrate the efficacy of randomly weighted neural networks for image recognition on standard benchmark datasets CIFAR-10  and ImageNet . This section is organized as follows: in Section 4.1 we discuss the experimental setup and hyperparameters. We perform a series of ablations at small scale: we examine the effect of k, the % of Weights which remain in the subnetwork, and the effect of width. In Section 4.4 we compare against the algorithm of Zhou et al., followed by Section 4.5 in which we study the effect of the distribution used to sample the weights. We conclude with Section 4.6, where we optimize to find subnetworks of randomly weighted neural networks which achieve good performance on ImageNet. Experimental SetupWe use two different distributions for the weights in our network:\u2022 Kaiming Normal , which we denote N k . Following the notation in section 3.2 the Kaiming Normal distribution is defined as N k = N 0, 2/n \u22121 where N denotes the normal distribution.\u2022 Signed Kaiming Constant which we denote U k . Here we set each weight to be a constant and randomly choose its sign to be + or \u2212. The constant we choose is the standard deviation of Kaiming Normal, and as a result the variance is the same. We use the notation U k as we are sampling uniformly from the set {\u2212\u03c3 k , \u03c3 k } where \u03c3 k is the standard deviation for Kaiming Normal (i.e. 2/n \u22121 ).In Section 4.5 we reflect on the importance of the random distribution and experiment with alternatives.  . For completeness we provide the architecture of the simple VGG-like  architectures used for CIFAR-10 , which are identical to those used by Frankle and Carbin  and Zhou et al. . However, the slightly deeper Conv8 does not appear in the previous work. Each model first performs convolutions followed by the fully connected (FC) layers, and pool denotes max-pooling.On CIFAR-10 [13] we experiment with simple VGG-like architectures of varying depth. These architectures are also used by Frankle and Carbin  and Zhou et al.  and are provided in . On ImageNet we experiment with ResNet-50 and ResNet-101 , as well as their wide variants . In every experiment (for all baselines, datasets, and our algorithm) we optimize for 100 epochs and report the last epoch accuracy on the validation set. When we optimize with Adam  we do not decay the learning rate. When we optimize with SGD we use cosine learning rate decay . On CIFAR-10 [13] we train our models with weight decay 1e-4, momentum 0.9, batch size 128, and learning rate 0.1. We also often run both an Adam and SGD baseline where the weights are learned. The Adam baseline uses the same learning rate and batch size as in   . For the SGD baseline we find that training does not converge with learning rate 0.1, and so we use 0.01. As standard we also use weight decay 1e-4, momentum 0.9, and batch size 128. For the ImageNet experiments we use the hyperparam-  . This discussion has encompassed the extent of the hyperparameter tuning for our models. We do, however, perform hyperparameter tuning for the Zhou et al.  baseline and improve accuracy significantly. We include further discussion of this in Section 4.4.In all experiments on CIFAR-10 [13] we use 5 different random seeds and plot the mean accuracy \u00b1 one standard deviation. Moreover, on all figures, Learned Dense Weights denotes the standard training the full model (all weights remaining). Varying the % of WeightsOur algorithm has one associated parameter: the % of weights which remain in the subnetwork, which we refer to as k.  illustrates how the accuracy of the subnetwork we find varies with k, a trend which we will now dissect. We consider k \u2208  and plot the dense model when it is trained as a horizontal line (as it has 100% of the weights).We recieve the worst accuracy when k approaches 0 or 100. When k approaches 0, we are not able to perform well as our subnetwork has very few weights. On the other hand, when k approaches 100, our network outputs are random.The best accuracy occurs when k \u2208 , and we make a combinatorial argument for this trend. We are choosing kn weights out of n, and there are n kn ways of doing so. The number of possible subnetworks is therefore maximized when k \u2248 0.5, and at this value our search space is at its largest.  In  we vary the width of Conv4 and Conv6. The width of a linear layer is the number of \"neurons\", and the width of a convolution layer is the number of channels. The width multiplier is the factor by which the width of all layers is scaled. A width multiplier of 1 corresponds to the models tested in . As the width multiplier increases, the gap shrinks between the accuracy a subnetwork found with edge-popup and the dense model when it is trained. Notably, when Conv6 is wide enough, a subnetwork of the randomly weighted model (with %Weights = 50) performs just as well as the dense model when it is trained. Varying the WidthMoreover, this boost in performance is not solely from the subnetwork having more parameters. Even when the # of parameters is fixed, increasing the width and therefore the search space leads to better performance. In  we fix the number of parameters and while modifying k and the width multiplier. Specifically, we test k \u2208  for subnetworks of constant size c 1 , c 2 and c 3 . On  we use |E| denote the size of the subnetwork. Comparing with Zhou et al. [33]In  we compare the performance of edge-popup with Zhou et al. Their work considers distributions N x and U x , which are identical to those presented in Section 4.1 but with xavier normal  instead of kaiming normal  -the factor of \u221a 2 is omitted from the standard deviation. By running their algorithm with N k and U k we witness a significant improvement. However, even the N x and U x results exceed those in the paper as we perform some hyperparameter tuning. As in our experiments on CIFAR-10, we use SGD with weight decay 1e-4, momentum 0.9, batch size 128, and a cosine scheduler . We double the learning rate until we see the performance become worse, and settle on 200 Effect of The DistributionThe distribution that the random weights are sampled from is very important. As illustrated by , the performance of our algorithm vastly decreases when we switch to using xavier normal  or kaiming uniform .Following the derivation in , the variance of the forward pass is not exactly 1 when we consider a subnetwork with only k% of the weights. To reconcile for this we could scale standard deviation by 1/k. This distribution is referred to as \"Scaled Kaiming Normal\" on . We may also consider this scaling for the Signed Kaiming Constant distribution which is described in Section 4.1. . Testing our Algorithm on ImageNet . We use a fixed k = 30%, and find subnetworks within a randomly weighted ResNet-50 , Wide ResNet-50 , and ResNet-101. Notably, a randomly weighted Wide ResNet-50 contains a subnetwork which is smaller than, but matches the performance of ResNet-34. Note that for the non-dense models, # of Parameters denotes the size of the subnetwork. ImageNet [4] ExperimentsOn ImageNet we observe similar trends to CIFAR-10. As ImageNet is a much harder dataset, computationally feasible models are not overparameterized to the same degree. As a consequence, the performance of a randomly weighted subnetwork does not match the full model with learned weights. However, we still witness a very encouraging trend -the performance increases with the width and depth of the network.As illustrated by , a randomly weighted Wide ResNet-50 contains a subnetwork that is smaller than, but matches the accuracy of ResNet-34 when trained on Ima-geNet . As strongly suggested by our trends, better and larger \"parent\" networks would result in even stronger performance on ImageNet . A table which reports the numbers in  may be found in Section A of the appendix.  illustrates the effect of k, which follows an almost identical trend: k \u2208  performs best though 30 now provides the best performance.  also demonstrates that we significantly outperform Zhou et al. at scale (in their original work they do not consider ImageNet). For Zhou et al. on ImageNet we report the best top-1 accuracy as we find their performance degrades towards the end of training. This is the only case where we do not report last epoch accuracy.The choice of the random distribution matters more for ImageNet. The \"Scaled\" distribution we discuss in Section 4.5 did not show any discernable difference on CIFAR-10. However,  illustrates that on ImageNet it is much  better. Recall that the \"Scaled\" distribution adds a factor of 1/k, which has less of an effect when k approaches 100% = 1. This result highlights the possibility of finding better distributions which work better for this task. ConclusionHidden within randomly weighted neural networks we find subnetworks with compelling accuracy. This work provides an avenue for many areas of exploration. Finally, we hope that our findings serve as a useful step in the pursuit of understanding the optimization and initialization of neural networks. A. Table of ImageNet ResultsIn  we provide a table of the results for image classification with ImageNet . These results correspond exactly to . B. Additional Technical DetailsIn this section we first prove a more general case of Theorem 1 then provide an extension of edge-popup for convolutions along with code in PyTorch , found in Algorithm 1. B.1. A More General Case of Theorem 1Theorem 1 (more general): When a nonzero number of edges are swapped in one layer and the rest of the network remains fixed then the loss decreases for the mini-batch (provided the loss is sufficiently smooth). Proof. As before, we letsuv denote the score of weight wuv after the gradient update. Additionally, let\u0128v denote the input to node v after the gradient update whereas Iv is the input to node v before the update. Finally, let i1, ..., in denote the n nodes in layer \u2212 1 and j1, ..., jm denote the m notes in layer . Our goal is to show thatwhere the loss is written as a function of layer 's input for brevity. If the loss is smooth and\u0128j k is close to Ij k we may ignore secondorder terms in a Taylor expansion:And so, in order to show Equation 12 it suffices to show thatIt is helpful to rewrite the sum to be over edges. Specifically, we will consider the sets Eold and Enew where Enew contains all edges that entered the network after the gradient update and Eold consists of edges which were previously in the subnetwork, but have now exited. As the total number of edges is conserved we know that |Enew| = |Eold|, and by assumption |Enew| > 0.Using the definition of I k and\u0128 k from Equation 3 we may rewrite Equation 16 aswhich, by Equation 6 and factoring out 1/\u03b1 becomesWe now show thatfor any pair of edges (ia, j b ) \u2208 Enew and (ic, j d ) \u2208 Eold. Since |Enew| = |Eold| > 0 we are then able to conclude that Equation 18 holds. As (ia, j b ) was not in the edge set before the gradient update, but (ic, j d ) was, we can concludeLikewise, since (ia, j b ) is in the edge set after the gradient update, but (ic, j d ) isn't, we can conclud\u1ebdBy adding Equation 21 and Equation 20 we find that Equation 19 is satisfied as needed. B.2. Extension to Convolutional Neural NetworksIn order to show that our method extends to convolutional layers we recall that convolutions may be written in a form that resembles Equation 2. Let \u03ba be the kernel size which we assume is odd for simplicity, then for w \u2208 {1, ..., W } and h \u2208 {1, ..., H} we havewhere instead of \"neurons\", we now have \"channels\". The input Iv and output Zv are now two dimensional and so Zis a scalar. As before, Zv = \u03c3 (Iv) where \u03c3 is a nonlinear function. However, in the convolutional case \u03c3 is often batch norm  followed by ReLU (and then implicitly followed by zero padding).Instead of simply having weights wuv we now have weights w which mirrors the formulation of edge-popup in Equation 4. In fact, when \u03ba = W = H = 1 (i.e. a 1x1 convolution on a 1x1 feature map) then Equation 23 and Equation 4 are equivalent. The update for the scores is quite similar, though we must now sum over all spatial (i.e. w and h) locations as given below:  . ImageNet  classification results corresponding to . Note that for the non-dense models, # of Parameters denotes the size of the subnetwork. # self.k is the % of weights remaining, a real number in [0,1] # self.popup_scores is a Parameter which has the same shape as self.weight # Gradients to self.weight, self.bias have been turned off. def forward(self, x):# Get the subnetwork by sorting the scores. adj = GetSubnet.apply( self.popup_scores.abs(), self.k) # Use only the subnetwork in the forward pass. w = self.weight * adj x = F.conv2d(x, w, self.bias, self.stride, self.padding, self.dilation, self.groups ) return xIn summary, we now have \u03ba 2 edges between each u and v. The PyTorch  code is given by Algorithm 1, where h is GetSubnet. The gradient goes straight through h in the backward pass, and PyTorch handles the implementation of these equations. C. Additional Experiments C.1. Resnet18 on CIFAR-10In  we experiment with a more advanced network architecture on CIFAR-10. C.2. Are these subnetworks lottery tickets?What happens when we train the weights of the subnetworks form  and  on ImageNet? They do not train to the same accuracy as a dense network, and do not perform substantially better than training a random subnetwork. This suggests that the good performance of these subnetworks at initialization does not explain the lottery phenomena described in . The results can be found in , where we again use the hyperparameters found on NVIDIA's public github example repository for training .\n###\n"}
{"summary": "has benchmark: Benchmark Atari-57/Benchmark Atari 2600 Robotank/Benchmark Atari 2600 Gravitar/Benchmark Atari 2600 Star Gunner/Benchmark Atari 2600 Chopper Command/Benchmark Atari 2600 Video Pinball/Benchmark Atari 2600 Q*Bert/Benchmark Atari 2600 HERO/Benchmark Atari 2600 James Bond/Benchmark Atari 2600 Space Invaders/Benchmark Atari 2600 Boxing\nhas research problem: Atari Games\nsame as: https://en.wikipedia.org/wiki/Atari_Games", "text": "#Properties\nhas benchmark, has research problem, same as\n#Text\nIn this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting. Contraction of the policy evaluation Bellman operator.Basing ourselves on results by  we show that, for a fixed policy, the Bellman operator over value distributions is a contraction in a maximal form of the Wasserstein (also called Kantorovich or Mallows) metric. Our particular choice of metric matters: the same operator is not a contraction in total variation, Kullback-Leibler divergence, or Kolmogorov distance.Instability in the control setting. We will demonstrate an instability in the distributional version of Bellman's optimality equation, in contrast to the policy evaluation case. Specifically, although the optimality operator is a contraction in expected value (matching the usual optimality result), it is not a contraction in any metric over distributions. These results provide evidence in favour of learning algorithms that model the effects of nonstationary policies.Better approximations. From an algorithmic standpoint, there are many benefits to learning an approximate distribution rather than its approximate expectation. The distributional Bellman operator preserves multimodality in value distributions, which we believe leads to more stable learning. Approximating the full distribution also mitigates the effects of learning from a nonstationary policy. As a whole, arXiv:1707.06887v1 LG] 21 Jul 2017 we argue that this approach makes approximate reinforcement learning significantly better behaved.We will illustrate the practical benefits of the distributional perspective in the context of the Arcade Learning Environment . By modelling the value distribution within a DQN agent , we obtain considerably increased performance across the gamut of benchmark Atari 2600 games, and in fact achieve stateof-the-art performance on a number of games. Our results echo those of , who obtained extremely fast learning by predicting Monte Carlo returns.From a supervised learning perspective, learning the full value distribution might seem obvious: why restrict ourselves to the mean? The main distinction, of course, is that in our setting there are no given targets. Instead, we use Bellman's equation to make the learning process tractable; we must, as  put it, \"learn a guess from a guess\". It is our belief that this guesswork ultimately carries more benefits than costs. SettingWe consider an agent interacting with an environment in the standard fashion: at each step, the agent selects an action based on its current state, to which the environment responds with a reward and the next state. We model this interaction as a time-homogeneous Markov Decision Process (X , A, R, P, \u03b3). As usual, X and A are respectively the state and action spaces, P is the transition kernel P (\u2022 | x, a), \u03b3 \u2208 [0, 1] is the discount factor, and R is the reward function, which in this work we explicitly treat as a random variable. A stationary policy \u03c0 maps each state x \u2208 X to a probability distribution over the action space A. Bellman's EquationsThe return Z \u03c0 is the sum of discounted rewards along the agent's trajectory of interactions with the environment. The value function Q \u03c0 of a policy \u03c0 describes the expected return from taking action a \u2208 A from state x \u2208 X , then acting according to \u03c0:x t \u223c P (Fundamental to reinforcement learning is the use of Bellman's equation  to describe the value function:In reinforcement learning we are typically interested in acting so as to maximize the return. The most common ap- proach for doing so involves the optimality equationThis equation has a unique fixed point Q * , the optimal value function, corresponding to the set of optimal policiesWe view value functions as vectors in R X \u00d7A , and the expected reward function as one such vector. In this context, the Bellman operator T \u03c0 and optimality operator T areThese operators are useful as they describe the expected behaviour of popular learning algorithms such as SARSA and Q-Learning. In particular they are both contraction mappings, and their repeated application to some initial Q 0 converges exponentially to Q \u03c0 or Q * , respectively . The Distributional Bellman OperatorsIn this paper we take away the expectations inside Bellman's equations and consider instead the full distribution of the random variable Z \u03c0 . From here on, we will view Z \u03c0 as a mapping from state-action pairs to distributions over returns, and call it the value distribution.Our first aim is to gain an understanding of the theoretical behaviour of the distributional analogues of the Bellman operators, in particular in the less well-understood control setting. The reader strictly interested in the algorithmic contribution may choose to skip this section. Distributional EquationsIt will sometimes be convenient to make use of the probability space (\u2126, F, Pr). The reader unfamiliar with mea-sure theory may think of \u2126 as the space of all possible outcomes of an experiment . We will write u p to denote the L p norm of a vector u \u2208 R X for 1 \u2264 p \u2264 \u221e; the same applies to vectors in R X \u00d7A . The L p norm of a random vector U :, and for p = \u221e we have U \u221e = ess sup U (\u03c9) \u221e (we will omit the dependency on \u03c9 \u2208 \u2126 whenever unambiguous). We will denote the c.d.f. of a random variable U by F U (y) := Pr{U \u2264 y}, and its inverse c.d.f. by F \u22121 U (q) := inf{y :indicates that the random variable U is distributed according to the same law as V . Without loss of generality, the reader can understand the two sides of a distributional equation as relating the distributions of two independent random variables. Distributional equations have been used in reinforcement learning by ;  among others, and in operations research by . The Wasserstein MetricThe main tool for our analysis is the Wasserstein metric d p between cumulative distribution functions (see e.g. , where it is called the Mallows metric). For F , G two c.d.fs over the reals, it is defined aswhere the infimum is taken over all pairs of random variables (U, V ) with respective cumulative distributions F and G. The infimum is attained by the inverse c.d.f. transform of a random variable U uniformly distributed on [0, 1]:For p < \u221e this is more explicitly written asWe will find it convenient to conflate the random variables under consideration with their versions under the inf, writingwhenever unambiguous; we believe the greater legibility justifies the technical inaccuracy. Finally, we extend this metric to vectors of random variables, such as value distributions, using the corresponding L p norm.Consider a scalar a and a random variable A independent of U, V . The metric d p has the following properties:We will need the following additional property, which makes no independence assumptions on its variables. Its proof, and that of later results, is given in the appendix.Lemma 1 (Partition lemma). Let A 1 , A 2 , . . . be a set of random variables describing a partition of \u2126, i.e. A i (\u03c9) \u2208 {0, 1} and for any \u03c9 there is exactly one A i with A i (\u03c9) = 1. Let U, V be two random variables. ThenLet Z denote the space of value distributions with bounded moments. For two value distributions Z 1 , Z 2 \u2208 Z we will make use of a maximal form of the Wasserstein metric:We will used p to establish the convergence of the distributional Bellman operators.Lemma 2.d p is a metric over value distributions. Policy EvaluationIn the policy evaluation setting  we are interested in the value function V \u03c0 associated with a given policy \u03c0. The analogue here is the value distribution Z \u03c0 . In this section we characterize Z \u03c0 and study the behaviour of the policy evaluation operator T \u03c0 . We emphasize that Z \u03c0 describes the intrinsic randomness of the agent's interactions with its environment, rather than some measure of uncertainty about the environment itself.We view the reward function as a random vector R \u2208 Z, and define the transition operator P \u03c0 : Z \u2192 Zwhere we use capital letters to emphasize the random nature of the next state-action pair (X , A ). We define the distributional Bellman operator T \u03c0 : Z \u2192 Z asWhile T \u03c0 bears a surface resemblance to the usual Bellman operator (2), it is fundamentally different. In particular, three sources of randomness define the compound distribution T \u03c0 Z:a) The randomness in the reward R, b) The randomness in the transition P \u03c0 , and c) The next-state value distribution Z(X , A ).In particular, we make the usual assumption that these three quantities are independent. In this section we will show that (5) is a contraction mapping whose unique fixed point is the random return Z \u03c0 . CONTRACTION INd pConsider the process Z k+1 := T \u03c0 Z k , starting with some Z 0 \u2208 Z. We may expect the limiting expectation of {Z k } to converge exponentially quickly, as usual, to Q \u03c0 . As we now show, the process converges in a stronger sense: T \u03c0 is a contraction ind p , which implies that all moments also converge exponentially quickly.Lemma 3. T \u03c0 : Z \u2192 Z is a \u03b3-contraction ind p .Using Lemma 3, we conclude using Banach's fixed point theorem that T \u03c0 has a unique fixed point. By inspection, this fixed point must be Z \u03c0 as defined in (1). As we assume all moments are bounded, this is sufficient to conclude that the sequenceTo conclude, we remark that not all distributional metrics are equal; for example,  have shown that T \u03c0 is not a contraction in total variation distance. Similar results can be derived for the Kullback-Leibler divergence and the Kolmogorov distance. CONTRACTION IN CENTERED MOMENTSObserve that d 2 (U, V ) (and more generally,As a result, we cannot directly use d 2 to bound the variance differenceis in fact a contraction in variance , see also appendix). In general, T \u03c0 is not a contraction in the p th centered moment, p > 2, but the centered moments of the iterates {Z k } still converge exponentially quickly to those of Z \u03c0 ; the proof extends the result of . ControlThus far we have considered a fixed policy \u03c0, and studied the behaviour of its associated operator T \u03c0 . We now set out to understand the distributional operators of the control setting -where we seek a policy \u03c0 that maximizes value -and the corresponding notion of an optimal value distribution. As with the optimal value function, this notion is intimately tied to that of an optimal policy. However, while all optimal policies attain the same value Q * , in our case a difficulty arises: in general there are many optimal value distributions.In this section we show that the distributional analogue of the Bellman optimality operator converges, in a weak sense, to the set of optimal value distributions. However, this operator is not a contraction in any metric between distributions, and is in general much more temperamental than the policy evaluation operators. We believe the convergence issues we outline here are a symptom of the inherent instability of greedy updates, as highlighted by e.g.  and most recently .Let \u03a0 * be the set of optimal policies. We begin by characterizing what we mean by an optimal value distribution.Definition 1 (Optimal value distribution). An optimal value distribution is the v.d. of an optimal policy. The set of optimal value distributions is Z * := {Z \u03c0 * : \u03c0 * \u2208 \u03a0 * }.We emphasize that not all value distributions with expectation Q * are optimal: they must match the full distribution of the return under some optimal policy.Definition 2. A greedy policy \u03c0 for Z \u2208 Z maximizes the expectation of Z. The set of greedy policies for Z isRecall that the expected Bellman optimality operator T isThe maximization at x corresponds to some greedy policy. Although this policy is implicit in (6), we cannot ignore it in the distributional setting. We will call a distributional Bellman optimality operator any operator T which implements a greedy selection rule, i.e.As in the policy evaluation setting, we are interested in the behaviour of the iterates Z k+1 := T Z k , Z 0 \u2208 Z. Our first result is to assert that E Z k behaves as expected.and in particular E Z k \u2192 Q * exponentially quickly.By inspecting Lemma 4, we might expect that Z k converges quickly ind p to some fixed point in Z * . Unfortunately, convergence is neither quick nor assured to reach a fixed point. In fact, the best we can hope for is pointwise convergence, not even to the set Z * but to the larger set of nonstationary optimal value distributions.Definition 3. A nonstationary optimal value distribution Z * * is the value distribution corresponding to a sequence of optimal policies. The set of n.o.Theorem 1 (Convergence in the control setting). Let X be measurable and suppose that A is finite. ThenIf X is finite, then Z k converges to Z * * uniformly. Furthermore, if there is a total ordering \u227a on \u03a0 * , such that for anyThen T has a unique fixed point Z * \u2208 Z * .Comparing Theorem 1 to Lemma 4 reveals a significant difference between the distributional framework and the usual setting of expected return. While the mean of Z k converges exponentially quickly to Q * , its distribution need not be as well-behaved! To emphasize this difference, we now provide a number of negative results concerning T .Proposition 1. The operator T is not a contraction.Consider the following example , left). There are two states, x 1 and x 2 ; a unique transition from x 1 to x 2 ; from x 2 , action a 1 yields no reward, while the optimal action a 2 yields 1 + or \u22121 + with equal probability. Both actions are terminal. There is a unique optimal policy and therefore a unique fixed point Z * . Now consider Z as given in  (right), and its distance to Z * :where we made use of the fact that Z = Z * everywhere except at (x 2 , a 2 ). When we apply T to Z, however, the greedy action a 1 is selected and T Z(for a sufficiently small . This shows that the undiscounted update is not a nonexpansion:With \u03b3 < 1, the same proof shows it is not a contraction. Using a more technically involved argument, we can extend this result to any metric which separates Z and T Z.Proposition 2. Not all optimality operators have a fixed pointTo see this, consider the same example, now with = 0, and a greedy operator T which breaks ties by picking a 2 if Z(x 1 ) = 0, and a 1 otherwise. Then the sequence . Undiscounted two-state MDP for which the optimality operator T is not a contraction, with example. The entries that contribute tod1(Z, Z * ) andd1(T Z, Z * ) are highlighted.Proposition 3. That T has a fixed point Z * = T Z * is insufficient to guarantee the convergence of {Z k } to Z * .Theorem 1 paints a rather bleak picture of the control setting. It remains to be seen whether the dynamical eccentricies highlighted here actually arise in practice. One open question is whether theoretically more stable behaviour can be derived using stochastic policies, for example from conservative policy iteration . Approximate Distributional LearningIn this section we propose an algorithm based on the distributional Bellman optimality operator. In particular, this will require choosing an approximating distribution. Although the Gaussian case has previously been considered , to the best of our knowledge we are the first to use a rich class of parametric distributions. Parametric DistributionWe will model the value distribution using a discrete distribution parametrized by N \u2208 N and V MIN , V MAX \u2208 R, and whose support is the set of atomsIn a sense, these atoms are the \"canonical returns\" of our distribution. The atom probabilities are given by a parametric model \u03b8 :  j e \u03b8j (x,a) .The discrete distribution has the advantages of being highly expressive and computationally friendly (see e.g. Van den Oord et al., 2016). Projected Bellman UpdateUsing a discrete distribution poses a problem: the Bellman update T Z \u03b8 and our parametrization Z \u03b8 almost always have disjoint supports. From the analysis of Section 3 it would seem natural to minimize the Wasserstein metric (viewed as a loss) between T Z \u03b8 and Z \u03b8 , which is also conveniently robust to discrepancies in support. However, a second issue prevents this: in practice we are typically restricted to learning from sample transitions, which is not possible under the Wasserstein loss (see Prop. 5 and toy results in the appendix).Instead, we project the sample Bellman updateT Z \u03b8 onto the support of Z \u03b8 , Algorithm 1), effectively reducing the Bellman update to multiclass classification. Let \u03c0 be the greedy policy w.r.t. E Z \u03b8 . Given a sample transition (x, a, r, x ), we compute the Bellman updateT z j := r + \u03b3z j for each atom z j , then distribute its probability p j (x , \u03c0(x )) to the immediate neighbours ofT z j . The i th component of the projected update \u03a6T Z \u03b8 (x, a) iswhere [\u2022] b a bounds its argument in the range [a, b]. 1 As is usual, we view the next-state distribution as parametrized by a fixed parameter\u03b8. The sample loss L x,a (\u03b8) is the cross-entropy term of the KL divergencewhich is readily minimized e.g. using gradient descent. We call this choice of distribution and loss the categorical algorithm. When N = 2, a simple one-parameter alternative0 ; we call this the Bernoulli algorithm. We note that, while these algorithms appear unrelated to the Wasserstein metric, recent work  hints at a deeper connection. Algorithm 1 Categorical Algorithm Evaluation on Atari 2600 GamesTo understand the approach in a complex setting, we applied the categorical algorithm to games from the Ar-1 Algorithm 1 computes this projection in time linear in N .cade Learning Environment (ALE; . While the ALE is deterministic, stochasticity does occur in a number of guises: 1) from state aliasing, 2) learning from a nonstationary policy, and 3) from approximation errors. We used five training games  and 52 testing games.For our study, we use the DQN architecture , but output the atom probabilities p i (x, a) instead of action-values, and chose V MAX = \u2212V MIN = 10 from preliminary experiments over the training games. We call the resulting architecture Categorical DQN. We replace the squared loss (r + \u03b3Q(x , \u03c0(x )) \u2212 Q(x, a)) 2 by L x,a (\u03b8) and train the network to minimize this loss. 2 As in DQN, we use a simple -greedy policy over the expected actionvalues; we leave as future work the many ways in which an agent could select actions on the basis of the full distribution. The rest of our training regime matches Mnih et al.'s, including the use of a target network for\u03b8.  illustrates the typical value distributions we observed in our experiments. In this example, three actions (those including the button press) lead to the agent releasing its laser too early and eventually losing the game. The corresponding distributions reflect this: they assign a significant probability to 0 (the terminal value). The safe actions have similar distributions (LEFT, which tracks the invaders' movement, is slightly favoured). This example helps explain why our approach is so successful: the distributional update keeps separated the low-value, \"losing\" event from the high-value, \"survival\" event, rather than average them into one (unrealizable) expectation.  One surprising fact is that the distributions are not concentrated on one or two values, in spite of the ALE's determinism, but are often close to Gaussians. We believe this is due to our discretizing the diffusion process induced by \u03b3. Varying the Number of AtomsWe began by studying our algorithm's performance on the training games in relation to the number of atoms ( ). For this experiment, we set = 0.05. From the data, it is clear that using too few atoms can lead to poor behaviour, and that more always increases performance; this is not immediately obvious as we may have expected to saturate the network capacity. The difference in performance between the 51-atom version and DQN is particularly striking: the latter is outperformed in all five games, and in SEAQUEST we attain state-of-the-art performance. As an additional point of the comparison, the single-parameter Bernoulli algorithm performs better than DQN in 3 games out of 5, and is most notably more robust in ASTERIX.  One interesting outcome of this experiment was to find out that our method does pick up on stochasticity. PONG exhibits intrinsic randomness: the exact timing of the reward depends on internal registers and is truly unobservable. We see this clearly reflected in the agent's prediction ( ): over five consecutive frames, the value distribution shows two modes indicating the agent's belief that it has yet to receive a reward. Interestingly, since the agent's state does not include past rewards, it cannot even extinguish the prediction after receiving the reward, explaining the relative proportions of the modes. State-of-the-Art ResultsThe performance of the 51-atom agent (from here onwards, C51) on the training games, presented in the last section, is particularly remarkable given that it involved none of the other algorithmic ideas present in state-of-the-art agents.We next asked whether incorporating the most common hyperparameter choice, namely a smaller training , could lead to even better results. Specifically, we set = 0.01 (instead of 0.05); furthermore, every 1 million frames, we evaluate our agent's performance with = 0.001.We compare our algorithm to DQN ( = 0.01), Double , the Dueling architecture , and Prioritized Replay , comparing the best evaluation score achieved during training. We see that C51 significantly outperforms these other algorithms . In fact, C51 surpasses the current state-of-the-art by a large margin in a number of games, most notably SEAQUEST. One particularly striking fact is the algorithm's good performance on sparse reward games, for example VENTURE and PRIVATE EYE. This suggests that value distributions are better able to propagate rarely occurring events. Full results are provided in the appendix.We also include in the appendix ) a comparison, averaged over 3 seeds, showing the number of games in which C51's training performance outperforms fullytrained DQN and human players. These results continue to show dramatic improvements, and are more representative of an agent's average performance. Within 50 million frames, C51 has outperformed a fully trained DQN agent on 45 out of 57 games. This suggests that the full 200 million training frames, and its ensuing computational cost, are unnecessary for evaluating reinforcement learning algorithms within the ALE.The most recent version of the ALE contains a stochastic execution mechanism designed to ward against trajectory overfitting.Specifically, on each frame the environment rejects the agent's selected action with probability p = 0.25. Although DQN is mostly robust to stochastic execution, there are a few games in which its performance is reduced. On a score scale normalized with respect to the random and DQN agents, C51 obtains mean and median score improvements of 126% and 21.5% respectively, confirming the benefits of C51 beyond the deterministic setting. Why does learning a distribution matter?It is surprising that, when we use a policy which aims to maximize expected return, we should see any difference in performance. The distinction we wish to make is that learning distributions matters in the presence of approximation. We now outline some possible reasons.Reduced chattering. Our results from Section 3.4 highlighted a significant instability in the Bellman optimality operator. When combined with function approximation, this instability may prevent the policy from converging, what  called chattering. We believe the gradient-based categorical algorithm is able to mitigate these effects by effectively averaging the different distri- \u2020 The UNREAL results are not altogether comparable, as they were generated in the asynchronous setting with per-game hyperparameter tuning .butions, similar to conservative policy iteration . While the chattering persists, it is integrated to the approximate solution.State aliasing. Even in a deterministic environment, state aliasing may result in effective stochasticity. , for example, showed the importance of coupling representation learning with policy learning in partially observable domains. We saw an example of state aliasing in PONG, where the agent could not exactly predict the reward timing. Again, by explicitly modelling the resulting distribution we provide a more stable learning target.A richer set of predictions. A recurring theme in artificial intelligence is the idea of an agent learning from a multitude of predictions . The distributional approach naturally provides us with a rich set of auxiliary predictions, namely: the probability that the return will take on a particular value. Unlike previously proposed approaches, however, the accuracy of these predictions is tightly coupled with the agent's performance.Framework for inductive bias. The distributional perspective on reinforcement learning allows a more natural framework within which we can impose assumptions about the domain or the learning problem itself. In this work we used distributions with support bounded inTreating this support as a hyperparameter allows us to change the optimization problem by treating all extremal returns (e.g. greater than V MAX ) as equivalent. Surprisingly, a similar value clipping in DQN significantly degrades performance in most games. To take another example: interpreting the discount factor \u03b3 as a proper probability, as some authors have argued, leads to a different algorithm.Well-behaved optimization. It is well-accepted that the KL divergence between categorical distributions is a reasonably easy loss to minimize. This may explain some of our empirical performance. Yet early experiments with alternative losses, such as KL divergence between continuous densities, were not fruitful, in part because the KL divergence is insensitive to the values of its outcomes. A closer minimization of the Wasserstein metric should yield even better results than what we presented here.In closing, we believe our results highlight the need to account for distribution in the design, theoretical or otherwise, of algorithms. Audrunas Gruslys, Tom Stepleton, Aaron van den Oord; and particularly Chris Maddison for his comprehensive review of an earlier draft. Thanks also to Marek Petrik for pointers to the relevant literature, and Mark Rowland for fine-tuning details in the final version. ErratumThe camera-ready copy of this paper incorrectly reported a mean score of 1010% for C51. The corrected figure stands at 701%, which remains higher than the other comparable baselines. The median score remains unchanged at 178%.The error was due to evaluation episodes in one game (Atlantis) lasting over 30 minutes; in comparison, the other results presented here cap episodes at 30 minutes, as is standard. The previously reported score on Atlantis was 3.7 million; our 30-minute score is 841,075, which we believe is close to the achievable maximum in this time frame. Capping at 30 minutes brings our human-normalized score on Atlantis from 22824% to a mere (!) 5199%, unfortunately enough to noticeably affect the mean score, whose sensitivity to outliers is well-documented. B. ProofsLemma 1 (Partition lemma). Let A 1 , A 2 , . . . be a set of random variables describing a partition of \u2126, i.e. A i (\u03c9) \u2208 {0, 1} and for any \u03c9 there is exactly one A i with A i (\u03c9) = 1. Let U, V be two random variables. ThenProof. We will give the proof for p < \u221e, noting that the same applies to p = \u221e.It follows that we can choose Y i , Z i so that also |Y i \u2212 Z i | p = 0 whenever A i = 0, without increasing the expected norm. HenceNext, we claim that Hence the distribution F U is equivalent, in an almost sure sense, to one that first picks an element A i of the partition, then picks a value for U conditional on the choice A i . On the other hand, the c.Thus the right-hand side infimum in (9) has the additional constraint that it must preserve the conditional c.d.fs, in particular when y \u2265 0. Put another way, instead of having the freedom to completely reorder the mapping U : \u2126 \u2192 R, we can only reorder it within each element of the partition. We now writewhere (a) follows because A 1 , A 2 , . . . is a partition. Using (9), this impliesbecause in (b) the individual components of the sum are independently minimized; and (c) from (8).Lemma 2.d p is a metric over value distributions.Proof. The only nontrivial property is the triangle inequality. For any value distribution Y \u2208 Z, writ\u0113\u2264 supx,awhere in (a) we used the triangle inequality for d p .Lemma 3. T \u03c0 : Z \u2192 Z is a \u03b3-contraction ind p .Proof. Consider Z 1 , Z 2 \u2208 Z. By definition, ).(10) By the properties of d p , we have dp(T , T \u03c0 Z2(x, a)) = dp(R(x, a) + \u03b3P \u03c0 Z1(x, a), R(x, a) + \u03b3P \u03c0 Z2(x, a)) \u2264 \u03b3dp(P \u03c0 Z1(x, a), P \u03c0 Z2(x, a))where the last line follows from the definition of P \u03c0 (see (4)). Combining with (10) we obtainProposition 1 . Consider two value distributions Z 1 , Z 2 \u2208 Z, and write V(Z i ) to be the vector of variances ofProof. The first statement is standard, and its proof follows from E T \u03c0 Z = T \u03c0 E Z, where the second T \u03c0 denotes the usual operator over value functions. Now, by independence of R and P \u03c0 Z i :And nowand in particular E Z k \u2192 Q * exponentially quickly.Proof. The proof follows by linearity of expectation. Write T D for the distributional operator and T E for the usual operator. ThenTheorem 1 (Convergence in the control setting). Let Z k := T Z k\u22121 with Z 0 \u2208 Z. Let X be measurable and suppose that A is finite. ThenIf X is finite, then Z k converges to Z * * uniformly. Furthermore, if there is a total ordering \u227a on \u03a0 * , such that for anythen T has a unique fixed point Z * \u2208 Z * .The gist of the proof of Theorem 1 consists in showing that for every state x, there is a time k after which the greedy policy w.r.t. Q k is mostly optimal. To clearly expose the steps involved, we will first assume a unique (and therefore deterministic) optimal policy \u03c0 * , and later return to the general case; we will denote the optimal action at x by \u03c0 * (x). For notational convenience, we will write Q k := E Z k and G k := G Z k . Let B := 2 sup Z\u2208Z Z \u221e < \u221e and let k := \u03b3 k B. We first define the set of states X k \u2286 X whose values must be sufficiently close to Q * at time k:(11) Indeed, by Lemma 4, we know that after k iterationsFor x \u2208 X , write a * := \u03c0 * (x). For any a \u2208 A, we deduce thatfor all a = \u03c0 * (x): for these states, the greedy policy \u03c0 k (x) := arg max a Q k (x, a) corresponds to the optimal policy \u03c0 * .Lemma 5. For each x \u2208 X there exists a k such that, for all k \u2265 k, x \u2208 X k , and in particular arg max a Q k (x, a) = \u03c0 * (x).Proof. Because A is finite, the gapis attained for some strictly positive \u2206(x) > 0. By definition, there exists a k such thatand hence every x \u2208 X must eventually be in X k .This lemma allows us to guarantee the existence of an iteration k after which sufficiently many states are wellbehaved, in the sense that the greedy policy at those states chooses the optimal action. We will call these states \"solved\". We in fact require not only these states to be solved, but also most of their successors, and most of the successors of those, and so on. We formalize this notion as follows: fix some \u03b4 > 0, let X k,0 := X k , and define for i > 0 the setAs the following lemma shows, any x is eventually contained in the recursively-defined sets X k,i , for any i.Lemma 6. For any i \u2208 N and any x \u2208 X , there exists a k such that for all k \u2265 k, x \u2208 X k ,i .Proof. Fix i and let us suppose that X k,i \u2191 X . By Lemma 5, this is true for i = 0. We infer that for any probability measure P on X , P (X k,i ) \u2192 P (X ) = 1. In particular, for a given x \u2208 X k , this implies thatTherefore, for any x, there exists a time after which it is and remains a member of X k,i+1 , the set of states for whichWe conclude that X k,i+1 \u2191 X also. The statement follows by induction.Proof of Theorem 1. The proof is similar to policy iteration-type results, but requires more care in dealing with the metric and the possibly infinite state space. We will write W k (x) := Z k (x, \u03c0 k (x)), define W * similarly and with some overload of notation write. Fix i > 0 and x \u2208 X k+1,i+1 \u2286 X k . We begin by using Lemma 1 to separate the transition from x into a solved term and an unsolved term:where X is the random successor from taking action \u03c0 k (x) := \u03c0 * (x), and we write S k i = S k i (X ),S k i = S k i (X ) to ease the notation. Similarly,where in (a) we used Properties P1 and P2 of the Wasserstein metric, and in (b) we separate states for which \u03c0 k = \u03c0 * from the rest using Lemma 1 ({S k i ,S k i } form a partition of \u2126). LetFrom property P3 of the Wasserstein metric, we haveRecall that B < \u221e is the largest attainable Z \u221e . Since also \u03b4 i < \u03b4 by our choice of x \u2208 X k+1,i+1 , we can upper bound the second term in (12) by \u03b3\u03b4B. This yieldsBy induction on i > 0, we conclude that for x \u2208 X k+i,i and some random state X i steps forward,Hence for any x \u2208 X , > 0, we can take \u03b4, i, and finally k large enough to make d p (W k (x), W * (x)) < . The proof then extends to Z k (x, a) by considering one additional application of T .We now consider the more general case where there are multiple optimal policies. We expand the definition of X k,i as follows:Because there are finitely many actions, Lemma 6 also holds for this new definition. As before, take x \u2208 X k,i , but now consider the sequence of greedy policies \u03c0 k , \u03c0 k\u22121 , . . . selected by successive applications of T , and writeNow denote by Z * * the set of nonstationary optimal policies. If we take any Z * \u2208 Z * , we deduce thatsince Z * corresponds to some optimal policy \u03c0 * and\u03c0 k is optimal along most of the trajectories from (x, a). In effect, T\u03c0 k Z * is close to the value distribution of the nonstationary optimal policy\u03c0 k \u03c0 * . Now for this Z * ,using the same argument as before with the newly-defined X k,i . It follows thatWhen X is finite, there exists a fixed k after which X k = X . The uniform convergence result then follows.To prove the uniqueness of the fixed point Z * when T selects its actions according to the ordering \u227a, we note that for any optimal value distribution Z * , its set of greedy policies is \u03a0 * . Denote by \u03c0 * the policy coming first in the ordering over \u03a0 * . Then T = T \u03c0 * , which has a unique fixed point (Section 3.3).Proposition 4. That T has a fixed point Z * = T Z * is insufficient to guarantee the convergence of {Z k } to Z * .We provide here a sketch of the result. Consider a single state x 1 with two actions, a 1 and a 2 ). The first action yields a reward of 1/2, while the other either yields 0 or 1 with equal probability, and both actions are optimal. Now take \u03b3 = 1/2 and write R 0 , R 1 , . . . for the received rewards. Consider a stochastic policy that takes action a 2 with probability p. For p = 0, the return isFor p = 1, on the other hand, the return is random and is given by the following fractional number (in binary): . A simple example illustrating the effect of a nonstationary policy on the value distribution.As a result, Z p=1 is uniformly distributed between 0 and 2! In fact, note thatFor some intermediary value of p, we obtain a different probability of the different digits, but always putting some probability mass on all returns in [0, 2].Now suppose we follow the nonstationary policy that takes a 1 on the first step, then a 2 from there on. By inspection, the return will be uniformly distributed on the interval [1/2, 3/2], which does not correspond to the return under any value of p. But now we may imagine an operator T which alternates between a 1 and a 2 depending on the exact value distribution it is applied to, which would in turn converge to a nonstationary optimal value distribution.Lemma 7 (Sample Wasserstein distance). Let {P i } be a collection of random variables, I \u2208 N a random index independent from {P i }, and consider the mixture random variable P = P I . For any random variable Q independent of I,and in general the inequality is strict andProof. We prove this using Lemma 1. Let A i := I [I = i]. We writewhere in the penultimate line we used the independence of I from P i and Q to appeal to property P3 of the Wasserstein metric.To show that the bound is in general strict, consider the mixture distribution depicted in . We will simply consider the d 1 metric between this distribution P and another distribution Q. The first distribution isIn this example, i \u2208 {1, 2}, P 1 = 0, and P 2 = 1. Now consider the distribution with the same support but that puts probability p on 0:The distance between P and Q isThis is d 1 (P, Q) = 1 2 for p \u2208 {0, 1}, and strictly less than 1 2 for any other values of p. On the other hand, the corresponding expected distance (after sampling an outcome x 1 or x 2 with equal probability) isHence. This shows that the bound is in general strict. By inspection, it is clear that the two gradients are different. . Example MDP in which the expected sample Wasserstein distance is greater than the Wasserstein distance.Proposition 5. Fix some next-state distribution Z and policy \u03c0. Consider a parametric value distribution Z \u03b8 , and and define the Wasserstein lossLet r \u223c R(x, a) and x \u223c P (\u2022 | x, a) and consider the sample lossIts expectation is an upper bound on the loss L W :in general with strict inequality.The result follows directly from the previous lemma. C. Algorithmic DetailsWhile our training regime closely follows that of DQN , we use Adam  instead of RMSProp  for gradient rescaling. We also performed some hyperparameter tuning for our final results. Specifically, we evaluated two hyperparameters over our five training games and choose the values that performed best. The hyperparameter values we considered were V MAX \u2208 {3, 10, 100} and adam \u2208 {1/L, 0.1/L, 0.01/L, 0.001/L, 0.0001/L}, where L = 32 is the minibatch size. We found V MAX = 10 and adam = 0.01/L performed best. We used the same step-size value as DQN (\u03b1 = 0.00025).Pseudo-code for the categorical algorithm is given in Algorithm 1. We apply the Bellman update to each atom separately, and then project it into the two nearest atoms in the original support. Transitions to a terminal state are handled with \u03b3 t = 0. D. Comparison of Sampled Wasserstein Loss and Categorical ProjectionLemma 3 proves that for a fixed policy \u03c0 the distributional Bellman operator is a \u03b3-contraction ind p , and therefore that T \u03c0 will converge in distribution to the true distribution of returns Z \u03c0 . In this section, we empirically validate these results on the CliffWalk domain shown in . The dynamics of the problem match those given by . We also study the convergence of the distributional Bellman operator under the sampled Wasserstein loss and the categorical projection (Equation 7) while fol-  . CliffWalk Environment .lowing a policy that tries to take the safe path but has a 10% chance of taking another action uniformly at random.We compute a ground-truth distribution of returns Z \u03c0 using 10000 Monte-Carlo (MC) rollouts from each state. We then perform two experiments, approximating the value distribution at each state with our discrete distributions.In the first experiment, we perform supervised learning using either the Wasserstein loss or categorical projection (Equation 7) with cross-entropy loss. We use Z \u03c0 as the supervised target and perform 5000 sweeps over all states to ensure both approaches have converged. In the second experiment, we use the same loss functions, but the training target comes from the one-step distributional Bellman operator with sampled transitions. We use V MIN = \u2212100 and V MAX = \u22121. 4 For the sample updates we perform 10 times as many sweeps over the state space. Fundamentally, these experiments investigate how well the two training regimes (minimizing the Wasserstein or categorical loss) minimize the Wasserstein metric under both ideal (supervised target) and practical (sampled one-step Bellman target) conditions.In  we show the final Wasserstein distance d 1 (Z \u03c0 , Z \u03b8 ) between the learned distributions and the ground-truth distribution as we vary the number of atoms. The graph shows that the categorical algorithm does indeed minimize the Wasserstein metric in both the supervised and sample Bellman setting. It also highlights that minimizing the Wasserstein loss with stochastic gradient descent is in general flawed, confirming the intuition given by Proposition 5. In repeat experiments the process converged to different values of d 1 (Z \u03c0 , Z \u03b8 ), suggesting the presence of local minima (more prevalent with fewer atoms).  provides additional insight into why the sampled Wasserstein distance may perform poorly. Here, we see the cumulative densities for the approximations learned under these two losses for five different states along the safe path in CliffWalk. The Wasserstein has converged to a fixedpoint distribution, but not one that captures the true (Monte Carlo) distribution very well. By comparison, the categorical algorithm captures the variance of the true distribution much more accurately. E. Supplemental Videos and ResultsIn  we provide links to supplemental videos showing the C51 agent during training on various Atari 2600 games.  shows the relative performance of C51 over the course of training.  . Number of Atari games where an agent's training performance is greater than a baseline (fully trained DQN & human). Error bands give standard deviations, and averages are over number of games. GAMES VIDEO URLFreeway  Pong  Q*Bert  Seaquest  Space Invaders\n###\n"}
{"summary": "Pre/Post-conditions: F\nhas research problem: Fuzzy Service Matching\nProtocols: F\nQoS: F\nSpecification Languages: WSDL\nMatching Result Degrees: subsumption degrees\nIncomplete Knowledge: F\nVariational Scope: + (req.)\nHeuristics & Simplif.: T\nCalculation Concepts: BPGM + SR", "text": "#Properties\nPre/Post-conditions, has research problem, Protocols, QoS, Specification Languages, Matching Result Degrees, Incomplete Knowledge, Variational Scope, Heuristics & Simplif., Calculation Concepts\n#Text\nIn this work we propose a new approach for semantic web matching to improve the performance of Web Service replacement. Because in automatic systems we should ensure the self-healing, self-configuration, self-optimization and self-management, all services should be always available and if one of them crashes, it should be replaced with the most similar one. Candidate services are advertised in Universal Description, Discovery and Integration (UDDI) all in Web Ontology Language (OWL). By the help of bipartite graph, we did the matching between the crashed service and a Candidate one. Then we chose the best service, which had the maximum rate of matching. In fact we compare two services` functionalities and capabilities to see how much they match. We found that the best way for matching two web services, is comparing the functionalities of them. Finding Most Similar ServiceWe have a repository of services that are all described in the same ontology language, OWL. So we can easily compare the capability and functionality of them. Each service does a special work by the means of some functions. These functions have inputs and outputs. Therefore, in order to compare two web services` functionalities, we must compare inputs and outputs. Now we describe the two phases of matching. Computing the Matching RateThe first phase is computing the similarity rate of two services. If the crashed service is C and the advertised service is ADV the inputs of each one is shown by C in and ADV in and the outputs are shown by C out and ADV out . So we compare C in with ADV in and C out with ADV out . Four results will be achieved that the algorithm is as follows:Case (C out , ADV out ):If C out = ADV out then return exact If C out ,subclass of ADV out then return exact If ADV out subsumes C out then return plugin If C out subsumes ADV out then return subsumes Otherwise return failBetween all services in the repository and crashed service, this comparison should be done. An Equivalent algorithm also used for inputs. Using Bipartite GraphWe do the matching by the help of bipartite graph. A Bipartite Graph is a graph G = (V,E) in which the vertex set can be partitioned into two disjoint sets ,such that every edge e in E has one vertex in V 0 and another in V 1 . The matching is complete if and only if, all vertices in V 0 are matched. It means that all vertices in V 0 , as well as V 1 , should have an edge. Let C out and ADV out be the set of output concepts in C and ADV respectively. These constitute the two vertex sets of our bipartite graph. Construct graph G=(V 0 + V 1 , E), where, V 0 = C out and V 1 = ADV out . Consider two concepts a in V 0 and b in V 1 It means that a is one of the output parameters of C and b is one of the output parameters of ADV. Let R be the result of CASE (in our algorithm, which can be Exact=E, Plugin=P, Subsume=S, Fail) between concepts a and b. It is obvious that E > P > S > F. We define an edge (a, b) in the graph and label this edge as R. Therefore if matching is complete (all vertices have at least one edge), now we compute the whole matching rate for these two services. In \" \" we have an example of a bipartite graph which has the complete matching. Fig. 2. Shows an example of bipartite graph of output conceptsNow we should choose the best sub graph. So for each vertex in V 0 we should choose the edge which is labled maximum, in a way that each vertex has only one edge. Two subgraphes of \" \" are shown in \" \". To compute the weight of the graph ,we select the less degree of edges .For example the weight of graph G1 in \" \", is S and the weight of graph G2 in \" \" is P. The result of matching two services is the graph which has the higher weight (in our example graph G2 that has the weight P). We do all these works for inputs, too. . Shows the matching subgraph G1 . Shows the matching subgraph G2 Our Matching AlgorithmIf the result of matching two services` outputs is OUTSIM and the result of matching two services inputs is INSIM, the whole result of matching two services is \"result\" that obtains by the following algorithm . The input of our algorithm is C which is the crashed service. The outputs of our algorithm are \"bestsrv\" and \"best result\" which are the best service and the matching rate, respectively. FIND MATCH (C , bestsrv, best result ); bestsrv = first service; best result = F; for all services in repository (ADV) do for all output parameters of C do Case ( C out , ADV out ) ; make a bipartite graph for outputs ; OUTSIM = minimum edge in bipartite graph;for all input parameters of C do Case ( C in , ADV in ); make a bipartite graph for inputs ; INSIM= minimum edge in bipartite graph ; reselt = E ; if (OUTSIM=F or INSIM=F ) then reselt = F else if (OUTSIM=S or INSIM=S ) then reselt = S else if (OUTSIM=P or INSIM=P ) then reselt = P ;if result > best result then best result = result ; bestsrv = ADV ; if best result =E then quit ; Complexity AnalysisIn computing the complexity of semantic web service matching, a lot of factors have Interference such as number of services in the repository, number of input parameters and number of output parameters. In equal situation with other algorithms, we don`t consider the number of input and output parameters. So for computing time complexity of an algorithm, just the number of advertised services is important. Now if N is the number of advertised services in the repository, we choose the first service as the best one. After computing each advertised service`s similarity, If this one`s similarity rate is higher than the best one, then this service is chosen as the best and so on.Whenever it finds the service by the similarity rate E, the work is finishing. Therefore the complexity of our algorithm is of O (N). An ExampleFor better explanation of our algorithm, this section describes a simple scenario of matching web services.If service C with these inputs and outputs crashes: Inputs: (officer ID, company name) Outputs: (name, address, phone number)This service takes the name of a company and one of its officer`s ID. Now we want to find a service for substituting it. For example, one of the services in UDDI is ADV that has these inputs and outputs:Inputs: (customer name, member ID) Output: (name, mobile number, add) The bipartite graph for inputs is shown in \" \". According to our ontology, the INSIM will be P.The bipartite graph for outputs is shown in \" \". According to our ontology, the OUTSIM will be E.According to these two graphs, the result is P. Conclusion and Future WorkIn this paper we have identified the problem of semantic web matching and proposed a new algorithm to solve the problem. By the help of UDDI registry, which advertises services that all are described in OWL language, we could compare the functionality of a crashed service with others. We conclude that functionality matching gives better result than QOS or policy matching. We also proposed a new architecture for our work that shows the simplicity and accuracy of our work.The bipartite graph played the main role of matching two semantic web services in our work. At the end We had computed the time complexity of our proposed algorithm ( O(N) ) which is minimum among other algorithms.Some of web services have preconditions to do a work and then have some effects. By considering these two factors, we can obtain a better result, certainly.Our future work is focused on improving the efficiency and accuracy of this algorithm by considering preconditions and effects of a service.\n###\n"}
{"summary": "has research problem: Scholarly event analysis\nanalysis aspect: Collaboration patterns\nDataset used: DBLP\non research field: Computer Science\nhas size: 382 MB\nDescription: No citation data contained/Uniquely flagged author names/3.8 mil. publications\nFields: Computer Science\nHas url: https://dblp.uni-trier.de/", "text": "#Properties\nhas research problem, analysis aspect, Dataset used, on research field, has size, Description, Fields, Has url\n#Text\nIt is popular nowadays to bring techniques from bibliometrics and scientometrics into the world of digital libraries to analyze the collaboration patterns and explore mechanisms which underlie community development. In this paper we use the DBLP data to investigate the author's scientific career and provide an in-depth exploration of some of the computer science communities. We compare them in terms of productivity, population stability and collaboration trends. Besides we use these features to compare the sets of topranked conferences with their lower ranked counterparts. DATA COLLECTIONWe use computer science bibliographic database DBLP to conduct our investigation. The database is publicly available in XML format at  We downloaded the file in August 2009 and used conference publications for corpus construction. While DBLP covers 50 years of publications the data before 1970 is rather irregular. This is the reason why we consider publications from 1970 on.The complete list accounts for 4449 distinct conference names. Manual examination of the conference pages in DBLP has shown that some venues have changed their names one or more times since they had been established. This observation suggests that we cannot treat conference names as unique because there is no guarantee of capturing the entire history of a venue. Fortunately all instances of the the same conference can be automatically identified with the XML tags in the original file. We use this feature and integrate all events of a venue with multiple names under the name of a component with the longest history.  illustrates the idea. Due to the name unification, the number of conferences is brought down to 2626. Publications from these conferences constitute the most general data set we use for our experiments. It is denoted CS dataset and represents the entire DBLP in the context of this paper.As we are interested in a comparative analysis of different scientific communities and venues we have to split the entire set of publications into topical subareas. One of the ways to do so is to specify sets of conferences that correspond to every subarea we want to analyze. Thus we select 14 subareas 1 each of which is represented by a set of relevant top ranked conferences with at least 10 years time span for the sake of data stability 2 . The idea of relying on the top ranked conferences is inspired by works of , and is grounded on the assumption that high quality conferences are clearly defined in terms of topics they cover. While every area has a modest number of commonly agreed upon top ranked venues, the assignment remains subjective. This is the reason why we validate the choice of venues by consulting several hand-made conference ranking sources  and considered the estimated venue impact provided by  . To enable a fair comparison we represent each subarea by the same or nearly the same number of conferences  .  shows the resulting data set which is denoted TOP dataset.As one of our goals is to identify a set of features that would help to distinguish between top and non-top conferences, we need a selection of conferences that do not belong to the set of top ranked venues. Using the same humanmade sources we select 6 areas with 5 representative conferences each. They are given in table 3, and constitute the NONTOP dataset.Note that there are some differences between the two sets in terms of topical partitioning and number of covered subareas. This is explained by the fact that the data about the lower ranked conferences is less consistent and agreeable, and we have preferred to construct smaller though more reliable sets.In these three sets above we exclude all publications that have incomplete bibliographic data such as missing authors, title or year. These constitute 0.052% of the records. The remaining publications are used to build co-authorship graphs GCS, GT op, and GnonT op, where GT op, GnonT op \u2208 GCS. These are undirected graphs where the authors constitute the set of vertices {V }, and two vertices vi, v k \u2208 {V } are connected by an edge e \u2208 {E} iff vi and v k have coauthored at least one paper. Our experiments are based on these graphs along with other bibliographic data such as number of records, venue, year. GENERAL RESEARCHER PROFILINGThe authors in co-author network are typically investigated from the point of view of their contribution to the research. Thus particular attention is paid to the members of program committees , \"fathers\" of the influential research directions , authors with high citation index  or yet those researchers who get often acknowledged . Such an approach yields an interesting but narrow image of the researchers community. In this section we aim at providing a broader view on the authors in entire DBLP and the areas described above by looking at their typical  While it is widely accepted to treat AI as a separate area we have preferred to decompose it into a few components, such as DMML and NLIR. We admit that these constitute only a subset of the highly interdisciplinary topic of AI.  We have had to relax the \"min 10 years time span\" requirement when dealing with conferences in Computational Biology and World Wide Web because these are young areas that have started off at the end of 90s.  In a few cases renowned conferences with less than 10 years history have been chosen to maintain consistency of the sets' size.  career length, interdisciplinary interests, individual performance pattern and publication distribution with respect to the top and non-top venues. Since our NONTOP dataset covers only a small part of the lower ranked venues listed in DBLP, we do not compare the TOP and NONTOP datasets to each other in this setting. Rather we contrast the data in TOP dataset to the global author statistics in DBLP. Author career lengthDBLP contains to hundreds of thousands distinct authors. But how many of them pursue a long scientific career?  give a full account on the authors career length distribution among the various research areas in the TOP set, CS dataset, and DBLP as a whole. The first chart represents percentage of authors with \u2264 5 career length, while the second one covers periods from 6 to 20 years. It turns out that top-ranked venues are dominated by authors with \u2264 5 years experience, and only \u2248 2% stay publishing at top ranked conferences for more than 10 years. This is consistent with the figures obtained on the whole DBLP set: \u2248 1.4% of authors have a longer than 10 years career. We hypothesize that the main component of DBLP authors is represented by PhD students who, after having finished their studies, leave the active scientific career. With respect to the research subareas, AT and CRYPTO have the lowest percentage of researchers with a short career and the highest percentage of people whose career length ranges between 10 and 15 years. The explanation lays probably in that fact that these domains require substantial mathematical background and thus time to obtain it which makes them harder to get in for the short time scientists, and more difficult for switching for those who spent so much time on it. Some characteristics of \"experienced\" scientistsWe now turn our attention to the authors with \u2265 10 years experience since they are more probable to influence scientific community than \"short time\" researchers. There are 16192 (\u2248 3%) such authors in the whole DBLP set, and 2623 researchers have \u2265 10 years publication record in the   P_authors with 5<y<=10 P_authors with 10<y<=15 P_authors with 15<y<=20  TOP set. We characterize this latter group in terms of interdisciplinarity of interests and productivity distribution. Interdisciplinarity of InterestsResearchers do not necessarily stay in one and the same field throughout the whole career. But how many areas and at what time of their career do they typically join? What is the probability for a researcher to join one more area given that he is already publishing in some field.There are 2623 authors in the TOP dataset whose career is \u2265 10 years. Out of them only \u2248 29% work in one area only. The remaining 71% join multiple areas with the average value of \u2248 2.2. We have analyzed the data distribution and found that they typically publish in more than one area from the very beginning of the career with a small spike between the 5th and tenth years. It is logical to assume that the interdisciplinarity of the researcher interests serves as an indicator of the area relatedness which can be calculated. For this purpose, let Astart be an area in which the author ai started to publish  . Next, build a transition matrix PA i with probabilities Ptransition = PA j |Pstart such that 1 \u2264 j \u2264 14, and j = start. Note that there exist two basic scenarios: ai publishes in more than one area in one year, and ai publishes in one area in a given year while overall he is active in multiple areas. We treat these two cases equally when computing P .The diagram in  shows the most probable transitions between the areas. Each circle represents an area, and its size is defined by the number of people working in it. The thickest arrows connect the most related areas, the thinner but solid arrows correspond to the second choice and the dotted ones (when present) to the third. The diagram shows clearly that the area relatedness is asymmetric. For example, Data Mining and Machine Learning (DMML) is primarily related to the Data Bases (DB). At the same time information retrieval (NLIR), computational biology (CBIO), graphics (GV), and WWW have their closest relationship to the DMML, indicating that the authors from these domains publish actively at DMML conferences. It is natural since these more practical areas constitute a field of application for the data mining and machine learning algorithms.It is also interesting to note that our rather global results that capture the state of interdisciplinarity in computer science in the last 40 years, are comparable to the yearly snapshots of the area overlap, found in . For example, both claim that there is a considerable authors' overlap between CRYPTO, Security (SEC), and theory (AT); Programming Languages (PL), Software Engineering (SE), and Distributed Computing (DP); Networks (NET) and DP. The similarity of findings that result from static and dynamic computations might point to the long-term relatedness between the areas. Individual Performance PatternLet us now focus on the author publication distribution over time and venues. For the temporal distribution analysis we distinguish between the following three groups of authors:\u2022 Authors with \u2265 10 years experience of publishing in TOPset conferences and focusing on one area only;\u2022 Authors with \u2265 10 years experience of publishing in TOPset conferences and focusing on multiple areas;\u2022 Authors \u2208 the TOPset with \u2265 10 years experience of publishing in the CS dataset, irrespective of the number of areas and conference rank.The average number of publications produced by each category of authors per 5-years periods are plotted at . The data reveals an interesting pattern: researchers in all three categories are much more active in the 2nd period of their career, and the single-area authors are even more active in the 3rd period. After that the productivity drops in the fourth period and remains stable with some minor fluctuations. Based on it we can try to reconstitute the principle milestones in the scientists' life: the first 5 years correspond roughly to the PhD. studies during which one typically produces a certain (not necessarily high) number of publications. The next 5 \u2212 10 years (2nd period) are of great importance to those who stay in research. In that time authors are evaluated on the international scale and their academic position depends heavily on their productivity. Recall also from the Subsection 4.2.1 that the small raise in the number of areas joined by researchers falls into this period, as well. The later stages correspond to the scientific maturity when scientific output stabilizes on average. With respect to the publication rate values, they are much higher for the single-area authors during the spike periods. There is no additional evidence that would help to explain this phenomenon. We might hypothesize that by working in one field only it is easier to get more papers published, since the author knows better the research criteria of his community.To analyze the author -publication distribution over venues we calculate for each author ai \u2208 TOP dataset the percentage of his publications in the top-ranked conferences relative to all his publications recorded in DBLP. Next we combine the results into the 10%-intervals and match them against the corresponding percentage of authors.The results are shown at . It turns out that only about 1.5% of authors in the TOP dataset publish exclusively or mostly at the top-ranked venues. Typically the topranked conference publications constitute from 30% to 60% of the author's conference production. It suggests that the majority of researchers appears in the mixed set of venues.To look closer at the publication distribution over venues in the topical sets we first assign each author ai \u2208 TOP dataset to the area he contributes at most (frequency based majority voting), and perform the same computation as before  .  presents the results. Notice that majority of areas are dominated by people who publish between 40 \u2212 50% of their publications in the top ranked conferences, and in DP  : Author -venue distribution in various areas.and DMML the prevailing range is 30 \u2212 40%. These values confirm the general tendency of publishing in the mixed set of venues. On the contrary, authors from DB, CRYPTO, AT and NLIR show more adherence to the top-ranked venues as proportion of researchers who publish 50 \u2212 70% of papers at top-ranked conferences outranks the other categories. SCIENTIFIC COMMUNITY ANALYSISThe previous section dealt with the author characteristic with respect to DBLP and the research areas defined in Section 3. In this section we take a closer look at the areas themselves and investigate them in terms of the publication growth rate, collaboration trends, and population stability. Selection of the evaluation criteria is not random. We believe that it may help to highlight the peculiarities of the individual domains and compare them to each other. We apply the same set of features to the subset of the non-top ranked conferences and eventually find out the differences between the top and non-top venues. Publication Growth RatePublication growth rate provides an evidence for the area \"well-being\" and sheds light on how much interest there is in it at the given moment. It is a dynamic measure that traces yearly changes in the area productivity. We distinguish between the relative and absolute growth rates.The absolute growth rate AbsGrA i,y of an area Ai in year y is a ratio of publications in Ai within two consecutive years yi and yi\u22121 such that AbsGrA i,y = P ubl A i,y P ubl A i,y\u22121 . We have calculated the values for all areas and found that except for the fluctuations corresponding typically to the beginning years, the fields differ considerably from each other. For example, Computer Architecture (ARCH) and Computer Networks (NET) have stabilized at early 90s, their absolute growths rate values oscillate around 1 \u00b1 0.1. On the contrary, Natural Language Processing and Information Retrieval (NLIR) productivity may vary three times as much from year to year, up to nowadays. Such a diversity could probably result from within-venue conventions that define the number of yearly accepted papers. We therefore compare the conferences in our TOP and NONTOP data sets with regard to the absolute publication growth rate. It turns out to be systematically higher in the non-top conferences. We can translate this result in terms of publication acceptance rates (information that is typically not present in the bibliographic databases though it is one of the important parameters for conference evaluation ), and conclude that they are lower for the top venues.   9 1 9 8 0 1 9 8 1 1 9 8 2 1 9 8 3 1 9 8 4 1 9 8 5 1 9 8 6 1 9 8 7 1 9 8 8 1 9 8 9 1 9 9 0 1 9 9 1 1 9 9 2 1 9 9 3 1 9 9 4 1 9 9 5 1 9 9 6 1 9 9 7 1 9 9 8 1 9 9 9 2 0 0 0 2 0 0 1 2 0 0 2 2 0 0 3 2 0 0 4 2 0 0 5 2 0 0 6 2 0 0 7 2 0 0 8 growth rate (in %) year CS (Tier-1)AT DMML : Relative growth rates of AT and DMML vs absolute growth rate of CSThe relative growth rate of an area Ai in year y, RGrA i ,y is a measure of its activity compared to the overall activity in Computer Science (CS)  . It is calculated as a ratio between the area absolute growth rate and the computer science absolute growth rate in the given year:Thus RGrA i ,y > 1, indicates a raise of interest to the area Ai in some year y.  illustrates the idea. As of CS, we observe considerable fluctuations in its growth rate with the overall tendency to raise in the 70s -1st half of 80s. One possible explanation is that many areas had started off in that period. At the same time the diapason in conference productivity is large in the beginning, and this is the reason why the curve goes up and down rather than increasing steadily. An additional explanation of the unstable behavior of the curve is the incompleteness of the DBLP data for the corresponding period. On the contrary, influx of the new disciplines becomes much smaller from the 2nd half of the 80s on, and we notice only two modest spikes -at the end of 90s and in the first years of 2000 which reflect most probably the contribution of the new-born Computational Biology, and World Wide Web.We chose DMML and AT to visualize the concept of the relative growth rate. On the background of the global development of CS, the bursts of activity in DMML can be seen in the beginning of 90s, and several times in the 2000s, though on the smaller rate. It corresponds well to the evolution of the area which has become very popular in the late 80s -beginning of 90s and attracts a great deal of attention nowadays. On the contrary, relative growth rate in AT remains most of the time bellow one. We suppose that the same considerations that we have mentioned in Subsection 4.1 prevent the area becoming \"trendy\". Collaboration trendsAnalysis of collaborations shows how much community is connected. One might expect that a highly interdisciplinary area such as Data Mining will exhibit lower connectivity than for example Information Retrieval which is focused on a much smaller number of topics and thus facilitates the collaboration. In addition to the between-area comparison we investigate the difference in collaboration pattern in communities described by the TOP and NONTOP data sets. The collaboration pattern is analyzed in terms of an average number of coauthors per paper and per author, and cluster-ing coefficient which quantifies how close the direct neighbors of a vertex are to form a complete graph . We use co-authorship graphs GCS, GT op, and GnonT op defined in Section 3 along with publication statistics to perform these computations.Previous analysis of the co-author network in ACM data set has shown that the number of collaborators per author increases steadily over the years . It has been confirmed by  who used CiteSeer as the experimental testbed. Our results obtained from the DBLP show that the increasing average number of co-authors per authors as well as the average number of authors per paper characterize all the subareas we deal with. Tables 4 and 5 summarizes our findings.In the TOP set data, CBIO and WWW have the highest average number of authors per paper along with the highest clustering coefficient which implies intensive collaborations throughout the entire community. On the contrary, AT, CRYPTO and PL (Programming Languages) have the smallest number of authors per paper, highest percentage of singleton authors and the lowest clustering coefficient among all 14 disciplines. It follows that in these three areas authors have a strong preference for working in small groups when collaborating. Moreover these groups turn to be weakly connected which results in a network composed of rather isolated cliques. It is worth mentioning that  found that among other CS areas, CRYPTO has the highest collaborative assortativity. Assortativity  quantifies how much a vertex in the network is connected to alike vertices. Collaborative assortativity reflects the tendency of authors to collaborate with those authors who have similar number of coauthors. This selectivity in collaboration pattern scales well with our assumption about sparseness of the cryptographic community. A bit surprisingly but the figures in the table do not confirm our assumption about the connectivity of DMML and NLIR. The higher percentage of coauthors per author coming from the same area (63%) in NLIR proves its lower interdisciplinarity compared to DMML where \u2248 51% coauthors per author belong to other disciplines. However it does not seem to have an impact on the connectivity pattern, and the clustering coefficient of NLIR is a little smaller than that of DMML. Alternatively it can be explained by the fraction of working alone authors (singletons) which is almost twice as much in NLIR as in DMML and naturally lows down the connectivity rate of the former. The weak relation between the interdisciplinarity of a field and its connectivity is best seen with {GV (Graphics), SEC (Security)} pair. The clustering coefficient of both is slightly above average (0.67 and 0.68 vs 0.65). At the same time GV is the most homogeneous area out of all 14 (73% of coauthors per authors belong to GV), while SEC is the most heterogeneous one: only 40% of coauthors per authors come from the same discipline.The data in  reveals that on average only 43% of coauthors per author belong to the set of authors publishing at top ranked conferences. It is in line with the author/venue distribution discussed in Subsection 4.2.2, and confirms that the same researchers publish at top and nontop ranked venues. In general, the NONTOP set  is featured by the slightly higher number of authors per pa-  per and higher clustering coefficient (DB is an exception), although the values are close in both sets. Note also that if we were to sort the areas by the clustering coefficient, the order would be the same as in the TOP set (DB and DMML switched around). However we have no sufficient evidence to conclude whether or not the non-top ranked conferences exhibit distinctive behavior in this setting compared to the top-ranked venues. Population StabilityIn Section 4 we discussed area interdisciplinarity as suggested by author transitions between the fields. In this section we concentrate on the mechanism that influence researcher dynamics. For this we analyze changes in conference populations in terms of new members that join a venue ( newcomers), and those who leave it, leavers. In the context of this section, the large communities corresponding to the research areas are decomposed into the conferences each of which is understood as an individual community.In  it has been pointed out that the membership in a community may be influenced by fact of having \"friends\" in that community. Thus some researchers are more likely to submit their paper to a conference if they have previously coauthored with someone who had already published over there. The theory has been tested on LiveJournal and DBLP (set of 84 conferences with at least 15 years history) communities. We take on this approach and investigate whether this property holds equally in different areas and venues. We therefore define:\u2022 Newcomer N ewc k,y : an author who had no publications at conference c k before year y. We define a fraction of newcomers in a conference c k in the year y asT otalAauthorsc k,y ;\u2022 Pure newcomer P newc k,y : an author who had neither publications nor has he coauthored with an author already member of c k before year y. The pure newcomers are calculated as P newComers = P newc k,y N ewComersc k,y ;\u2022 Leaver Leaverc k,y : an author who has no more publications in c k after year y. The fraction of leavers in c k,y is formalized asResults of the computations are given in . Due tot he space considerations we show only the most interesting results.Let us discuss some of the TOP set conferences. All venues in AT and CRYPTO prove stable and moreover are the most stable venues in the whole TOP set. They are characterized by low percentage of Newcomers, Pure newcomers, and Leavers, compared to the average values across the whole TOP set. Note that fraction of Pure newcomers is an important parameter as it sheds light on how \"friendship\" phenomenon affects the inflow of the new authors: the higher the fraction is, the smaller is the friendship influence. We have found that AT and CRYPTO are friendship driven as about 50% of new authors joining venues have co-authored with authors who had already published over there.Contrarily to the two fields above, WWW conferences are the most dynamic ones, featured by the high values for the Newcomers, Pure newcomers, and Leavers' fractions. Friendship does not seem to alter the influx of new authors as the Pure newcomers typically count for \u2248 60\u221280% of all the Newcomers. Note that the member conferences are youngexcept of ISWC that has started off in 1997 all other venues have appeared in 2000s. It is natural to postulate that the population stability of a venue is directly related to its age. In the given set of conferences, our assumption is immediately confirmed by the ISWC which has the lowest values for all three aspects. Note however that the above relation holds in many but not all the cases. Thus for example in Security, CSFW (1988) is less dynamic than S&P (1980), and ICCAD (1990), the most stable community in Architecture, is much younger than ISCA (1973) which scores second in terms of stability. The interpretation of these observations is that while population stability does depend to the certain extent on the conference age, it is also influenced by other, conference specific factors.The key observation concerning the NONTOP set of venues, is that all of them irrespective of time span (which ranges from 17 to 3 years) and domain, are very dynamic. (The only exceptions are ICCS and DLT (AT) whose behavior is closer to AT venues from the TOP set). Typically the Newcomers constitute about 75 \u2212 85% of all authors, and the average value of the Pure newcomers is about 75% which suggests that the friendship influence on the decision to join a venue is rather negligible. The turnover of authors is also remarkable since the fraction of Leavers is often comparable to that of Newcomers and constitutes up to 88% of all the authors. As such, population stability might be considered as a candidate feature that helps to distinguish between the top and non-top venues. CONCLUSIONS AND FUTURE WORKIn this paper we have analyzed computer science communities in different settings. We a performed statistical analysis of authors, and found that the DBLP community is dominated by the short-time researchers whose career does not exceed 5 years. We have also discovered that experienced scientists from the top-ranked venues tend to join multiple research communities and produce the highest number of publications between the 5th and 10th years of their career. Typically they publish in a mixture of top and non-top ranked venues.We have also compared communities from 14 research areas of computer science and performed the between-area comparison in terms of publication growth rate, collaboration trends and population stability. In addition, we applied the same criteria to the comparison between top and nontop ranked conferences and discovered that the publication growth rate and population stability could be among the features that help to separate the two sets.In this approach we have manually divided the broad area of computer science into 14 topics. In the future we plan to substitute this rather ad hoc approach by applying a machine learning technique such as Latent Dirichlet Allocation  for both -topic classification and learning the best number of topics into which the given data can be divided. By doing this we will avoid the subjectivity of manual classification. We also plan to elaborate on the set of features that could be used for efficient comparison and eventually automatic ranking of venues. Besides we plan to extend the notion of \"venue\" to incorporate journals into analysis. ACKNOWLEDGMENTSWe would like to thank Prof. Christoph Schommer for his critical reading, valuable comments and helpful suggestions.\n###\n"}
{"text": "#Properties\nName, Method automation, Supports reference extraction, Knowledge graph creation, Input format, Output format, User interface, Summary, has research problem, Task, Scope\n#Text\nDue to the lack of structure, scholarly knowledge remains hardly accessible for machines. Scholarly knowledge graphs have been proposed as a solution. Creating such a knowledge graph requires manual effort and domain experts, and is therefore time-consuming and cumbersome. In this work, we present a human-in-the-loop methodology used to build a scholarly knowledge graph leveraging literature survey articles. Survey articles often contain manually curated and high-quality tabular information that summarizes findings published in the scientific literature. Consequently, survey articles are an excellent resource for generating a scholarly knowledge graph. The presented methodology consists of five steps, in which tables and references are extracted from PDF articles, tables are formatted and finally ingested into the knowledge graph. To evaluate the methodology, 92 survey articles, containing 160 survey tables, have been imported in the graph. In total, 2 626 papers have been added to the knowledge graph using the presented methodology. The results demonstrate the feasibility of our approach, but also indicate that manual effort is required and thus underscore the important role of human experts. MethodologyWe now present a five-step methodology for the creation of a scholarly knowledge graph from survey tables. In order to reach sufficient quality, the methodology takes a human-in-the-loop approach in which multiple steps require human interaction. Data quality improves with human evaluation and, if needed, correction of the extracted data. The methodology is displayed in . The scripts required to perform the steps are available online. Paper SelectionIn the first step, suitable survey papers are selected based on multiple criteria. The purpose is to find survey papers from a diverse range of domains. Therefore, a protocol has been designed to determine which papers are suitable for data extraction. The structured nature of the selection process is needed to be able to make conclusions about the percentage of survey papers that present the information in such a way that extracting data is relatively straightforward.Search Strategy. lists the search engines used to find survey articles. Google Scholar is chosen to ensure that survey papers from various fields are searched. Additionally, ACM Digital Library has been selected because the ORKG currently focuses mainly on the Computer Science domain. The search is limited to 100 papers that are suitable for import. The following search criteria are used:-Google Scholar: the article title contains the term \"literature survey\".-ACM Digital Library: queries \"literature review\" and \"literature survey\".-The survey article has been published after 2002.-The results are sorted by relevance.The rationale for selecting papers published after the year 2002 is because in general more recent papers are more interesting for research and should therefore have more priority in the scholarly knowledge graph. In the end, articles published before 2002 can still be part of the graph, since this criterion only applies to the survey articles themselves, and not to the papers being reviewed in those articles.Selection Criteria. Papers that satisfy the inclusion criteria are selected for the import process. The inclusion criteria are defined as follows: provide information about one publication). The article is written in English.Inclusion criterion 1 ensures that a survey article does not only textually summarize the literature, but does also provide a semi-structured comparison (in tabular form). Although papers that are textually reviewing scientific literature are interesting for importing as well, it is out of scope for this work. Criterion 2 ensures only surveys that compare actual paper results are included. This excludes surveys researching, for instance, the growth of a field. Criterion 3 excludes tables in image format. This is because of the tabular extraction method we use, which is based on character extraction and does not use Optical Character Recognition (OCR) needed to support image extraction . Criterion 4 only selects tables that are suitable for import. Our methodology does only support paper import when one row in a table represents one paper. Although minor changes can be made manually (e.g. merging multiple tables), in case the structure of the table deviates significantly from the required format, the table is excluded. Finally, criterion 5 ensures a homogeneous semantic integration into the currently English monolingual knowledge graph. The result of this step is a set of the selected papers in PDF format. Table ExtractionThis step focuses on extracting the tables from the PDF files collected in the previous step. Not only the text within the table should be extracted, but the tabular structure should be preserved as well. As explained in the related work section, we use Tabula to perform the table extraction. Each PDF article is uploaded via the Tabula user interface. Afterwards, the regions of the tables are manually selected within the interface. Although Tabula provides a functionality to automatically detect tables, the accuracy is not sufficient for our use case. The performance is especially low for articles with a two-column layout. Additionally, not all tables within an article have to be extracted since not all of them are listing and comparing literature. Arguably, the manual selection method is most useful in this methodology since human judgment is needed in the selection process. Part of the extraction step is quality assurance after the extraction. When needed, extraction errors are manually fixed. Tabula supports two types of extraction, namely \"Stream\" and \"Lattice\". The Stream extraction method is based on white space between columns while Lattice is based on boundary lines between columns. During the extraction it is possible to switch between the different methods, which allows for selecting the best method for a particular table. The result of this step is a set of CSV files, in which each file represents one survey table from a review article. Table FormattingThe CSV files containing the extracted tables from the review articles should be formatted in a structure that is suitable for building a graph. Since the data from the CSV file is extracted automatically, all tables should have the same format. In this step, the formatting of the tables is changed when necessary. For some tables, a considerable amount of changes is required while for other tables only minor\n###\n", "summary": " Method automation: Semi-Automatic\n, Supports reference extraction: T\n, Knowledge graph creation: T\n, Input format: PDF\n, Output format: JSON/RDF\n, User interface: Yes (table viewer)\n, Summary: A semi-automatic approach of extracting survey tables for creating a scholarly knowledge graphs. Provides a user interface for displaying the tables\n, has research problem: Table extraction\n, Task: Automatic paper reference extraction/Automatically extract tables from PDFs/Knowledge graph creation from CSVs/Manual table formatting / quality control/Manually select survey papers /Manually select tables\n, Scope: Survey tables\n###"}
{"summary": "has benchmark: Benchmark Twitter/Benchmark Amazon/Benchmark BBCSport/Benchmark Reuters-21578/Benchmark 20NEWS/Benchmark Ohsumed\nhas research problem: Text Classification\nhas model: Orthogonalized Soft VSM", "text": "#Properties\nhas benchmark, has research problem, has model\n#Text\nSince the seminal work of Mikolov et al., word embeddings have become the preferred word representations for many natural language processing tasks. Document similarity measures extracted from word embeddings, such as the soft cosine measure (SCM) and the Word Mover's Distance (WMD), were reported to achieve state-of-the-art performance on the semantic text similarity and text classification. Despite the strong performance of the WMD on text classification and semantic text similarity, its super-cubic average time complexity is impractical. The SCM has quadratic worst-case time complexity, but its performance on text classification has never been compared with the WMD. Recently, two word embedding regularization techniques were shown to reduce storage and memory costs, and to improve training speed, document processing speed, and task performance on word analogy, word similarity, and semantic text similarity. However, the effect of these techniques on text classification has not yet been studied. In our work, we investigate the individual and joint effect of the two word embedding regularization techniques on the document processing speed and the task performance of the SCM and the WMD on text classification. For evaluation, we use the kNN classifier and six standard datasets: BBCSPORT, TWITTER, OHSUMED, REUTERS-21578, AMAZON, and 20NEWS. We show 39% average kNN test error reduction with regularized word embeddings compared to non-regularized word embeddings. We describe a practical procedure for deriving such regularized embeddings through Cholesky factorization. We also show that the SCM with regularized word embeddings significantly outperforms the WMD on text classification and is over 10,000\u00d7 faster. Document Distance and Similarity MeasuresThe Vector Space Model (VSM)  is a distributional semantics model that is fundamental to a number of text similarity applications including text classification. The VSM represents documents as coordinate vectors relative to a real inner-product-space orthonormal basis \u03b2, where coordinates correspond to weighted and normalized word frequencies. In the VSM, a commonly used measure of similarity for document vectors x and y is the cosine similarity: cosine similarity of x and y = x/ x 2 , y/ y 2 , where x, y = (x) \u03b2 T (y) \u03b2 and z 2 is the 2 -norm of z. The cosine similarity is highly susceptible to polysemy, since distinct words correspond to mutually orthogonal basis vectors. Therefore, documents that use different terminology will always be regarded as dissimilar. To borrow an example from , the cosine similarity of the documents \"Obama speaks to the media in Illinois\" and \"the President greets the press in Chicago\" is zero if we disregard stop words.The Word Mover's Distance (WMD) and the Soft Cosine Measure (SCM) are document distance and similarity measures that address polysemy. Because of the scope of this work, we discuss briefly the WMD and the SCM in the following subsections. Word Mover's DistanceThe Word Mover's Distance (WMD)  uses network flows to find the optimal transport between VSM document vectors. The distance of two document vectors x and y is the following:where the cost c i j is the Euclidean distance of embeddings for words i and j. We use the implementation in PyEMD  with the best known average time complexity O(p 3xy log p xy ), where p xy is the number of unique words in x and y. Soft Cosine MeasureThe soft VSM  assumes that document vectors are represented in a nonorthogonal normalized basis \u03b2. In the soft VSM, basis vectors of similar words are close and the cosine similarity of two document vectors x and y is the Soft Cosine Measure (SCM):T S(y) \u03b2 , and S is a word similarity matrix.We define the word similarity matrix S like Charlet and Damnati : s i j = max(t, e i / e i 2 , e j / e j 2 ) o , where e i and e j are the embeddings for words i and j, and o and t are free parameters. We use the implementation in the similarities.termsim module of Gensim (\u0158eh\u016f\u0159ek and Sojka 2010). The worst-case time complexity of the SCM is O(p x p y ), where p x is the number of unique words in x and p y is the number of unique words in y. Word Embedding RegularizationThe Continuous Bag-of-Words Model (CBOW) ) is a neural network language model that predicts the center word from context words. The CBOW with negative sampling minimizes the following loss function:u o is the vector of a center word with corpus position o, v i is the vector of a context word with corpus position i, and the window size w and the number of negative samples k are free parameters. Word embeddings are the sum of center word vectors and context word vectors. To improve the properties of word embeddings and the task performance of the WMD and the SCM, we apply two regularization techniques to CBOW. QuantizationFollowing the approach of Lam (2018), we quantize the center word vector u o and the context word vector v i during the forward and backward propagation stages of the training: a quantized center word vector u o = 1 /3 \u2022 sign(u o ) and a quantized context word vectorSince the quantization function is non-differentiable at certain points, we use Hinton's straight-through estimator (Hinton 2012, Lecture 15b) as the gradient:where \u2207 is the gradient operator and I is the identity function.Lam shows that quantization reduces the storage and memory cost and improves the performance of word embeddings on the word analogy and word similarity tasks. OrthogonalizationNovotn\u00fd shows that producing a sparse word similarity matrix S that stores at most C largest values from every column of S reduces the worst-case time complexity of the SCM to O(p x ), where p x is the number of unique words in a document vector x.Novotn\u00fd also claims that S improves the performance of the soft VSM on the question answering task and describes a greedy algorithm for producing S , which we will refer to as the orthogonalization algorithm. The orthogonalization algorithm has three boolean parameters: Sym, Dom, and Idf. Sym and Dom make S symmetric and strictly diagonally dominant. Idf processes columns of S in descending order of inverse document frequency :, where D are documents. In our experiment, we compute the SCM directly from the word similarity matrix S , see Equation . However, actual word embeddings must be extracted for many NLP tasks. Novotn\u00fd shows that the word similarity matrix S can be decomposed using Cholesky factorization. We will now define orthogonalized word embeddings and we will show that the Cholesky factors of S are in fact orthogonalized word embeddings. Notice how the cluster of numerals from E is separated in E due to the parameter value Idf = , which makes common words more likely to be mutually orthogonal.Definition 1 (Orthogonalized word embeddings) Let E, E be real matrices with |V | rows, where V is a vocabulary of words. Then E are orthogonalized word embeddings from E, which we denote E \u2264 \u22a5 E, iff for all i, j = 1, 2, . . . , |V | it holds that e i , e j = 0 =\u21d2 e i , e j = e i , e j , where e k and e k denote the k-th rows of E and E .Theorem 1 Let E be a real matrix with |V | rows, where V is a vocabulary of words, and for all k = 1, 2, . . . , |V | it holds that e k 2 = 1. Let S be a word similarity matrix constructed from E with the parameter values t = \u22121 and o = 1 as described in Section 3.2. Let S be a word similarity matrix produced from S using the orthogonalization algorithm with the parameter values Sym = and Dom = . Let E be the Cholesky factor of S . Then E \u2264 \u22a5 E.Proof With the parameter values Sym = , Dom = , S is symmetric and strictly diagonally dominant, and therefore also positive definite. The symmetric positive definite matrix S has a unique Cholesky factorization of the form S = E (E ) T . Therefore, the Cholesky factor E exists and is uniquely determined.From S = E (E ) T , we have that for all i, j = 1, 2, . . . , |V | such that the sparse matrix S does not contain the value s i j it holds that s i j = e i , e j = 0. Since the implication in the theorem only applies when e i , e j = 0, we do not need to consider this case.From S = E (E ) T , o = 1,t = \u22121, and e k 2 = 1, we have that for all i, j = 1, 2, . . . , |V | such that the sparse matrix S contains the value s i j , it holds that e i , e j = s i j = s i j = max(t, e i / e i 2 , e j / e j 2 ) o = e i , e j .  shows the extraction of orthogonalized word embeddings E from E: From E, we construct the dense word similarity matrix S and from S, we produce the sparse word similarity matrix S through orthogonalization. From S , we produce the orthogonalized embeddings E through Cholesky factorization.With a large vocabulary V , a |V | \u00d7 |V | dense matrix S may not fit in the main memory, and we produce the sparse matrix S directly from E. Similarly, a |V | \u00d7 |V | dense matrix E may also not fit in the main memory, and we use sparse Cholesky factorization to produce a sparse matrix E instead. If dense word embeddings are required, we use dimensionality reduction on E to produce a |V | \u00d7 D dense matrix, where D is the number of dimensions.With the parameter value Idf = , words with small inverse document frequency, i.e. common words such as numerals, prepositions, and articles, are more likely to be mutually orthogonal (i.e. e i , e j = 0) than rare words. This is why in , the numerals form a cluster in the non-regularized word embeddings E, but they are separated in orthogonalized word embeddings E . ExperimentThe experiment was conducted using six standard text classification datasets by employing both the WMD and the SCM with a k Nearest Neighbor (kNN) classifier using both regularized and non-regularized word embeddings. First, we describe briefly our datasets, then our experimental steps. Our experimental code is available online. 1 DatasetsIn our experiment, we used the following six standard text classification datasets:BBCSPORT The BBCSPORT dataset  consists of 737 sport news articles from the BBC sport website in five topical areas: athletics, cricket, football, rugby, and tennis. The period of coverage was during 2004-2005.TWITTER The TWITTER dataset (Sanders 2011) consists of 5,513 tweets hand-classified into one of four topics: Apple, Google, Twitter, and Microsoft. The sentiment of every tweet was also hand-classified as either Positive, Neutral, Negative, or Irrelevant.OHSUMED The OHSUMED dataset ) is a set of 348,566 references spanning 1987-1991 from the MEDLINE bibliographic database of important, peer-reviewed medical literature maintained by the National Library of Medicine (NLM). While the majority of references are to journal articles, there are also a small number of references to letters of the editor, conference proceedings, and other reports. Each reference contains human-assigned subject headings from the 17,000-term Medical Subject Headings (MeSH) vocabulary.REUTERS The documents in the REUTERS-21578 collection ) appeared on the Reuters newswire in 1987. The documents were assembled and indexed with categories by the personnel of Reuters Ltd. The collection is contained in 22 SGML files. Each of the first 21 files (reut2-000.sgm through reut2-020.sgm) contains 1,000 documents, while the last one (reut2-021.sgm) contains only 578 documents. 20NEWS The 20NEWS dataset ) is a collection of 18,828 Usenet newsgroup messages partitioned across 20 newsgroups with different topics. The collection has become popular for experiments in text classification and text clustering.Algorithm 1 The kNN classifier for the WMD and the SCM with regularized and non-regularized word embeddings Input: Training data A, test data B, neighborhood size k Output: Class labels for the test data, L 1:For all a j \u2208 A, compute the SCM / WMD between a j and b i . 3:Choose the k nearest neighbors of b i in A. 4:Assign the majority class of the k nearest neighbors to l i . 5: end for PreprocessingFor TWITTER, we use 5,116 out of 5,513 tweets due to unavailability, we restrict our experiment to the Positive, Neutral, and Negative classes, and we subsample the dataset to 3,108 tweets like . For OHSUMED, we use the 34,389 abstracts related to cardiovascular diseases  out of 50,216 and we restrict our experiment to abstracts with a single class label from the first 10 classes out of 23. In the case of REUTERS, we use the R8 subset . For AMAZON, we use 5-core reviews from the Books, CDs and Vinyl, Electronics, and Home and Kitchen classes. We preprocess the datasets by lower-casing the text and by tokenizing to longest non-empty sequences of alphanumeric characters that contain at least one alphabetical character. We do not remove stop words or rare words, only words without embeddings. We split each dataset into train and test subsets using either a standard split (for REUTERS and 20NEWS) or following the split size of . See  for statistics of the preprocessed datasets. Training and Regularization of Word EmbeddingsUsing Word2Vec, we train the CBOW on the first 100 MiB of the English Wikipedia (Mahoney 2011) using the same parameters as .3) and 10 training epochs. We use quantized word embeddings in 1,000 dimensions and non-quantized word embeddings in 200 dimensions to achieve comparable performance on the word analogy task. Nearest Neighbor ClassificationWe use the VSM with uniform word frequency weighting, also known as the bag of words (BOW), as our baseline. For the SCM, we use the double-logarithm inverse collection frequency word weighting (the SMART dtb weighting scheme) as suggested by ). For the WMD, we use BOW document document vectors like .We tune the parameters o \u2208 {1, 2, 3, 4} and t \u2208 {0, \u00b1 1 /2, 1} of the SCM, the parameter s \u2208 {0.0, 0.1, . . . , 1.0} of the SMART dtb weighting scheme, the parameter k \u2208 {1, 3, . . . , 19} of the kNN, and the parameters C \u2208 {100, 200, 400, 800}, and Idf, Sym, Dom \u2208 {, } of the orthogonalization. For each dataset, we hold out 20% of the train set for validation, and we use grid search to find the optimal parameter values. To classify each sample in the test set, we follow the procedure presented in Algorithm 1. Significance TestingWe use the method of  to construct 95% confidence intervals for the kNN test error. For every dataset, we use Student's t-test at 95% confidence level with q-values  for all combinations of document similarities and word embedding regularization techniques to find significant differences in kNN test error. Average Task PerformanceUnlike the soft VSM, the WMD does not benefit from word embedding quantization. This is because of two reasons: (1) the soft VSM takes into account the similarity between all words in two documents, whereas the WMD only considers the most similar word pairs, and (2) non-quantized word embeddings are biased towards positive similarity, see . With non-quantized word embeddings, embeddings of unrelated words have positive cosine similarity, which makes dissimilar documents less separable. With quantized embeddings, unrelated words have negative cosine similarity, which improves separability and reduces kNN test error. The WMD is unaffected by the bias in non-quantized word embeddings, and the reduced precision of quantized word embeddings increases kNN test error.  shows the average document processing speed using a single Intel Xeon X7560 2.26 GHz core. Although the orthogonalization reduces the worst-case time complexity of the soft VSM from quadratic to linear, it also makes the word similarity matrix sparse, and performing sparse instead of dense matrix operations causes a 2.73\u00d7 slowdown compared to the soft VSM with non-orthogonalized word embeddings. Quantization causes a 1.8\u00d7 slowdown, which is due to the 5\u00d7 increase in the word embedding dimensionality, since we use 1000-dimensional quantized word embeddings and only 200-dimensional non-quantized word embeddings. Document Processing SpeedThe super-cubic average time complexity of the WMD results in an average 819,496\u00d7 slowdown compared to the soft VSM with orthogonalized and quantized word embeddings, and a 1,505,386\u00d7 slowdown on the 20NEWS dataset, which has a large average number of unique words in a document. Although  report up to 110\u00d7 speed-up using an approximate variant of the WMD (the WCD), this still results in an average 7,450\u00d7 slowdown, and a 13,685\u00d7 slowdown on the 20NEWS dataset.  shows the optimal parameter values for the soft VSM with orthogonalized word embeddings. The most common parameter value Idf = shows that it is important to store the nearest neighbors of rare words in the word similarity matrix S . The most common parameter values C = 100, o = 4, t = \u22121, Sym = , and Dom = show that strong orthogonalization, which makes most values in S zero or close to zero, gives the best results. Parameter OptimizationDue to the low document processing speed of the WMD and the number of orthogonalization parameters, using parameter optimization on the WMD with regularized embeddings is computationally intractable. Therefore, we do not report results for the WMD with regularized embeddings in figures 8-10. ConclusionWord embeddings achieve state-of-the-art results on several NLP tasks, predominantly at the sentence level, but overfitting is a major issue, especially when applied at the document level with a large number of words. We have shown that regularization of word embeddings significantly improves their performance not only on the word analogy, word similarity, and semantic text similarity tasks, but also on the text classification task.We further show that the most effective word embedding regularization technique is orthogonalization and we prove a connection between orthogonalization, Cholesky factorization and orthogonalized word embeddings. With word embedding orthogonalization, the task performance of the soft VSM exceeds the WMD, an earlier known state-of-the-art document distance measure, while being several orders of magnitude faster. This is an important step in bringing application of word embeddings from supercomputers to mobile and embedded systems.\n###\n"}
{"summary": "has benchmark: Benchmark CNN / Daily Mail/Benchmark GigaWord\nhas research problem: Text Summarization\nhas model: Pointer + Coverage + EntailmentGen + QuestionGen", "text": "#Properties\nhas benchmark, has research problem, has model\n#Text\nAn accurate abstractive summary of a document should contain all its salient information and should be logically entailed by the input document. We improve these important aspects of abstractive summarization via multi-task learning with the auxiliary tasks of question generation and entailment generation, where the former teaches the summarization model how to look for salient questioning-worthy details, and the latter teaches the model how to rewrite a summary which is a directed-logical subset of the input document. We also propose novel multitask architectures with high-level (semantic) layer-specific sharing across multiple encoder and decoder layers of the three tasks, as well as soft-sharing mechanisms (and show performance ablations and analysis examples of each contribution). Overall, we achieve statistically significant improvements over the state-ofthe-art on both the CNN/DailyMail and Gigaword datasets, as well as on the DUC-2002 transfer setup. We also present several quantitative and qualitative analysis studies of our model's learned saliency and entailment skills. ModelsFirst, we introduce our pointer+coverage baseline model and then our two auxiliary tasks: question generation and entailment generation (and finally the multi-task learning models in Sec. 4). Baseline Pointer+Coverage ModelWe use a sequence-attention-sequence model with a 2-layer bidirectional LSTM-RNN encoder and a 2-layer uni-directional LSTM-RNN decoder, along with  style attention. Let x = {x 1 , x 2 , ..., x m } be the source document and y = {y 1 , y 2 , ..., y n } be the target summary. The output summary generation vocabulary distribution conditioned over the input source document isLet the decoder hidden state be s t at time step t and let c t be the context vector which is defined as a weighted combination of encoder hidden states. We concatenate the decoder's (last) RNN layer hidden state s t and context vector c t and apply a linear transformation, and then project to the vocabulary space by another linear transformation. Finally, the conditional vocabulary distribution at each time step t of the decoder is defined as: Pointer-Generator Networks Pointer mechanism  helps in directly copying the words from the source sequence during target sequence generation, which is a good fit for a task like summarization. Our pointer mechanism approach is similar to , who use a soft switch based on the generation probability p g = \u03c3(W g c t +U g s t +V g e w t\u22121 +b g ), where \u03c3(\u2022) is a sigmoid function, W g , U g , V g and b g are parameters learned during training. e w t\u22121 is the previous time step output word embedding. The final word distribution iswhere P v vocabulary distribution is as shown in Eq. 1, and copy distribution P c is based on the attention distribution over source document words.Coverage Mechanism Following previous work , coverage helps alleviate the issue of word repetition while generating long summaries. We maintain a coverage vector c t = t\u22121 t=0 \u03b1 t that sums over all of the previous time steps attention distributions \u03b1 t , and this is added as input to the attention mechanism. Coverage loss isFinally, the total loss is a weighted combination of cross-entropy loss and coverage loss:where \u03bb is a tunable hyperparameter. Two Auxiliary TasksDespite the strengths of the baseline model described above with attention, pointer, and coverage, a good summary should also contain maximal salient information and be a directed logical entailment of the source document. We teach these skills to the abstractive summarization model via multi-task training with two related auxiliary tasks: question generation task and entailment generation.Question Generation The task of question generation is to generate a question from a given input sentence, which in turn is related to the skill of being able to find the important salient information to ask questions about. First the model has to identify the important information present in the given sentence, then it has to frame (generate) a question based on this salient information, such that, given the sentence and the question, one has to be able to predict the correct answer (salient information in this case). A good summary should also be able to find and extract all the salient information in the given source document, and hence we incorporate such capabilities into our abstractive text summarization model by multi-task learning it with a question generation task, sharing some common parameters/representations (see more details in Sec. 4). For setting up the question generation task, we follow  and use the SQuAD dataset to extract sentencequestion pairs. Next, we use the same sequenceto-sequence model architecture as our summarization model. Note that even though our question generation task is generating one question at a time 2 , our multi-task framework (see Sec. 4) is set up in such a way that the sentence-level knowledge from this auxiliary task can help the documentlevel primary (summarization) task to generate multiple salient facts -by sharing high-level semantic layer representations. See Sec. 7 and Table 10 for a quantitative evaluation showing that the multi-task model can find multiple (and more) salient phrases in the source document. Also see Sec. 7 (and supp) for challenging qualitative examples where baseline and SotA models only recover a small subset of salient information but our multi-task model with question generation is able to detect more of the important information.Entailment Generation The task of entailment generation is to generate a hypothesis which is entailed by (or logically follows from) the given premise as input. In summarization, the generation decoder also needs to generate a summary that is entailed by the source document, i.e., does not contain any contradictory or unrelated/extraneous information as compared to the input document. We again incorporate such inference capabilities into the summarization model via multi-task learning, sharing some common representations/parameters between our summarization and entailment generation model (more details in Sec. 4). For this task, we use the entailmentlabeled pairs from the SNLI dataset (Bowman et al., 2015) and set it up as a generation task (using the same strong model architecture as our abstractive summarization model). See Sec. 7 and  for a quantitative evaluation showing that the multi-task model is better entailed by the source document and has fewer extraneous facts. Also see Sec. 7 and supplementary for qualitative examples of how our multi-task model with the entailment auxiliary task is able to generate more logically-entailed summaries than the baseline and  We also tried to generate all the questions at once from the full document, but we obtained low accuracy because of this task's challenging nature and overall less training data.  : Overview of our multi-task model with parallel training of three tasks: abstractive summary generation (SG), question generation (QG), and entailment generation (EG). We share the 'blue' color representations across all the three tasks, i.e., second layer of encoder, attention parameters, and first layer of decoder.SotA models, which instead produce extraneous, unrelated words not present (in any paraphrased form) in the source document. Multi-Task LearningWe employ multi-task learning for parallel training of our three tasks: abstractive summarization, question generation, and entailment generation. In this section, we describe our novel layerspecific, soft-sharing approaches and other multitask learning details. Layer-Specific Sharing MechanismSimply sharing all parameters across the related tasks is not optimal, because models for different tasks have different input and output distributions, esp. for low-level vs. high-level parameters. Therefore, related tasks should share some common representations (e.g., high-level information), as well as need their own individual task-specific representations (esp. low-level information). To this end, we allow different components of model parameters of related tasks to be shared vs. unshared, as described next. Encoder Layer Sharing: Belinkov et al. observed that lower layers (i.e., the layers closer to the input words) of RNN cells in a seq2seq machine translation model learn to represent word structure, while higher layers (farther from input) are more focused on high-level semantic meanings (similar to findings in the computer vision community for image features (Zeiler and Fergus, 2014)). We believe that while textual summarization, question generation, and entailment generation have different training data distributions and low-level representations, they can still benefit from sharing their models' high-level components (e.g., those that capture the skills of saliency and inference). Thus, we keep the lower-level layer (i.e., first layer closer to input words) of the 2layer encoder of all three tasks unshared, while we share the higher layer (second layer in our model as shown in ) across the three tasks. Decoder Layer Sharing: Similarly for the decoder, lower layers (i.e., the layers closer to the output words) learn to represent word structure for generation, while higher layers (farther from output) are more focused on high-level semantic meaning. Hence, we again share the higher level components (first layer in the decoder far from output as shown in ), while keeping the lower layer (i.e., second layer) of decoders of all three tasks unshared. Attention Sharing: As described in Sec. 3.1, the attention mechanism defines an attention distribution over high-level layer encoder hidden states and since we share the second, high-level (semantic) layer of all the encoders, it is intuitive to share the attention parameters as well. Soft vs. Hard Parameter SharingHard-sharing: In the most common multi-task learning hard-sharing approach, the parameters to be shared are forced to be the same. As a result, gradient information from multiple tasks will directly pass through shared parameters, hence forcing a common space representation for all the related tasks. Soft-sharing: In our soft-sharing approach, we encourage shared parameters to be close in representation space by penalizing their l 2 distances. Unlike hard sharing, this approach gives more flexibility for the tasks by only loosely coupling the shared space representations. We minimize the following loss function for the primary task in soft-sharing approach:where \u03b3 is a hyperparameter, \u03b8 represents the primary summarization task's full parameters, while \u03b8 s and \u03c8 s represent the shared parameter subset between the primary and auxiliary tasks. Fast Multi-Task TrainingDuring multi-task learning, we alternate the minibatch optimization of the three tasks, based on a tunable 'mixing ratio' \u03b1 s : \u03b1 q : \u03b1 e ; i.e., optimizing the summarization task for \u03b1 s mini-batches followed by optimizing the question generation task for \u03b1 q mini-batches, followed by entailment generation task for \u03b1 e mini-batches (and for 2way versions of this, we only add one auxiliary task at a time). We continue this process until all the models converge. Also, importantly, instead of training from scratch, we start the primary task (summarization) from a 90%-converged model of its baseline to make the training process faster. We observe that starting from a fully-converged baseline makes the model stuck in a local minimum.In addition, we also start all auxiliary models from their 90%-converged baselines, as we found that starting the auxiliary models from scratch has a chance to pull the primary model's shared parameters towards randomly-initialized auxiliary model's shared parameters. We use the standard ROUGE evaluation package  for reporting the results on all of our summarization models. Following previous work , we use ROUGE full-length F1 variant for all our results. Following See et al. (2017), we also report ME-TEOR (Denkowski and Lavie, 2014) using the MS-COCO evaluation script . Human Evaluation Criteria: We used Amazon MTurk to perform human evaluation of summary relevance and readability. We selected human annotators that were located in the US, had an ap-  : CNN/DailyMail summarization results. ROUGE scores are full length F-1 (as previous work). All the multi-task improvements are statistically significant over the state-of-the-art baseline.proval rate greater than 95%, and had at least 10,000 approved HITs. For the pairwise model comparisons discussed in Sec. 6.2, we showed the annotators the input article, the ground truth summary, and the two model summaries (randomly shuffled to anonymize model identities) -we then asked them to choose the better among the two model summaries or choose 'Not-Distinguishable' if both summaries are equally good/bad. Instructions for relevance were defined based on the summary containing salient/important information from the given article, being correct (i.e., avoiding contradictory/unrelated information), and avoiding redundancy. Instructions for readability were based on the summary's fluency, grammaticality, and coherence.   shows that our baseline model performs better than or comparable to See et al. (2017).  On Gigaword dataset, our baseline model (with pointer only, since coverage not needed for this single-sentence summarization task) performs better than all previous works, as shown in .  : Summarization results on Gigaword. ROUGE scores are full length F-1. All the multitask improvements are statistically significant over the state-of-the-art baseline. Multi-Task with Entailment GenerationWe first perform multi-task learning between abstractive summarization and entailment generation with soft-sharing of parameters as discussed in Sec. 4.  shows that this multi-task setting is better than our strong baseline models and the improvements are statistically significant on all metrics 5 on both CNN/DailyMail (p < 0.01 in ROUGE-1/ROUGE-L/METEOR and p < 0.05 in ROUGE-2) and Gigaword (p < 0.01 on all metrics) datasets, showing that entailment generation task is inducing useful inference skills to the summarization task (also see analysis examples in Sec. 7).   Multi-Task with Entailment and Question Generation Finally, we perform multi-task learning with all three tasks together, achieving the best of both worlds (inference skills and saliency).  show that our full multi-task model achieves the best scores on CNN/DailyMail and Gigaword datasets, and the improvements are statistically significant on all metrics on both CNN/DailyMail (p < 0.01 in ROUGE-1/ROUGE-L/METEOR and p < 0.02 in ROUGE-2) and Gigaword (p < 0.01 on all metrics). Finally, our 3-way multi-task model (with both entailment and question generation) outperforms the publicly-available pretrained result ( \u2020) of the previous SotA  with stat. significance (p < 0.01), as well the higher-reported results ( ) on ROUGE-1/ROUGE-2 (p < 0.01). Multi-Task with Question Generation Human EvaluationWe also conducted a blind human evaluation on Amazon MTurk for relevance and readability, based on 100 samples, for both CNN/DailyMail and Gigaword (see instructions in Sec. 5).  3 shows the CNN/DM results where we do pairwise comparison between our 3-way multi-task model's output summaries w.r.t. our baseline summaries and w.r.t.  summaries. As shown, our 3-way multi-task model achieves both higher due to adding more data, we separately trained word embeddings on each auxiliary dataset (i.e., SNLI and SQuAD) and incorporated them into the summarization model. We found that both our 2-way multi-task models perform significantly better than these models using the auxiliary wordembeddings, suggesting that merely adding more data is not enough.   , our MTL model is higher in relevance scores but a bit lower in readability scores (and is higher in terms of total aggregate scores). One potential reason for this lower readability score is that our entailment generation auxiliary task encourages our summarization model to rewrite more and to be more abstractive than See et al. -see abstractiveness results in .We also show human evaluation results on the Gigaword dataset in  (again based on pairwise comparisons for 100 samples), where we see that our MTL model is better than our state-of-theart baseline on both relevance and readability. 7 Generalizability Results (DUC-2002)Next, we also tested our model's generalizability/transfer skills, where we take the models trained on CNN/DailyMail and directly test them on DUC-2002. We take our baseline and 3way multi-task models, plus the pointer-coverage model from See et al. (2017).  We only retune the beam-size for each of these three models separately (based on DUC-2003 as the validation set). 9 As shown in , our multitask model achieves statistically significant improvements over the strong baseline (p < 0.01 in ROUGE-1 and ROUGE-L) and the pointercoverage model from See et al. (2017) (p < 0.01 in all metrics). This demonstrates that our model is able to generalize well and that the auxiliary knowledge helps more in low-resource scenarios. Auxiliary Task ResultsIn this section, we discuss the individual/separated performance of our auxiliary tasks.  Note that we did not have output files of any previous work's model on Gigaword; however, our baseline is already a strong state-of-the-art model as shown in .  We use the publicly-available pretrained model from See et al. (2017)'s github for these DUC transfer results, which produces the \u2020 results in . All other comparisons and analysis in our paper are based on their higher results.  We follow previous work which has shown that larger beam values are better and feasible for DUC corpora. However, our MTL model still achieves stat. significant improvements (p < 0.01 in all metrics) over  without beam retuning (i.e., with beam = 4). Models Entailment GenerationWe use the same architecture as described in Sec. 3.1 with pointer mechanism, and  compares our model's performance to . Our pointer mechanism gives a performance boost, since the entailment generation task involves copying from the given premise sentence, whereas the 2-layer model seems comparable to the 1-layer model. Also, the supplementary shows some output examples from our entailment generation model.Question Generation Again, we use same architecture as described in Sec. 3.1 along with pointer mechanism for the task of question generation.  compares the performance of our model w.r.t. the state-of-the-art . Also, the supplementary shows some output examples from our question generation model. Ablation and Analysis StudiesSoft-sharing vs. Hard-sharing As described in Sec. 4.2, we choose soft-sharing over hard-sharing because of the more expressive parameter sharing it provides to the model. Empirical results in  8 prove that soft-sharing method is statistically significantly better than hard-sharing with p < 0.001 in all metrics.  Comparison of Different Layer-Sharing Methods We also conducted ablation studies among various layer-sharing approaches.  shows results for soft-sharing models with decoder-only sharing (D1+D2; similar to Pasunuru et al. (2017)) as well as lower-layer sharing (encoder layer 1 + decoder layer 2, with and without attention Quantitative Improvements in EntailmentWe employ a state-of-the-art entailment classifier , and calculate the average of the entailment probability of each of the output summary's sentences being entailed by the input source document. We do this for output summaries of our baseline and 2-way-EG multi-task model (with entailment generation). As can be seen in , our multi-task model improves upon the baseline in the aspect of being entailed by the source document (with statistical significance p < 0.001). Further, we use the Named Entity Recognition (NER) module from CoreNLP  to compute the number of times the output summary contains extraneous facts (i.e., named entities as detected by the NER system) that are not present in the source documents, based on the intuition that a well-entailed summary should not contain unrelated information not followed from the input premise. We found that our 2-way MTL model with entailment generation reduces this extraneous count by 17.2% w.r.t. the baseline.The qualitative examples below further discuss this issue of generating unrelated information.   Quantitative Improvements in Saliency Detection For our saliency evaluation, we used the answer-span prediction classifier from Pasunuru and Bansal (2018) trained on SQuAD  as the keyword detection classifier.We then annotate the ground-truth and model summaries with this keyword classifier and compute the % match, i.e., how many salient words from the ground-truth summary were also generated in the model summary. The results are shown in Table 10, where the 2-way-QG MTL model (with question generation) versus baseline improvement is stat. significant (p < 0.01). Moreover, we found 93 more cases where our 2-way-QG MTL model detects 2 or more additional salient keywords than the pointer baseline model (as opposed to vice versa), showing that sentence-level question generation task is helping the document-level summarization task in finding more salient terms.Qualitative Examples on Entailment and Saliency Improvements  presents an example of output summaries generated by , our baseline, and our 3-way multitask model.  and our baseline models generate phrases like \"john hartson\" and \"hampden injustice\" that don't appear in the input document, hence they are not entailed by the input. 12 Moreover, both models missed salient information like \"josh meekings\", \"leigh griffiths\", and \"hoops\", that our multi-task model recovers.  Hence, our 3-way multi-task model generates summaries that are both better at logical entailment and contain more salient information. We refer to supplementary  for more details and similar examples for separated 2-way multi-task models (supplementary ).12 These extra, non-entailed unrelated/contradictory information are not present at all in any paraphrase form in the input document.  We consider the fill-in-the-blank highlights annotated by human on CNN/DailyMail dataset as salient information.Input Document: celtic have written to the scottish football association in order to gain an ' understanding\u00f3f the refereeing decisions during their scottish cup semi-final defeat by inverness on sunday . the hoops were left outraged by referee steven mclean\u015b failure to award a penalty or red card for a clear handball in the box by josh meekings to deny leigh griffith\u015b goal-bound shot during the first-half . caley thistle went on to win the game 3-2 after extra-time and denied rory delia\u015b men the chance to secure a domestic treble this season . celtic striker leigh griffiths has a goal-bound shot blocked by the outstretched arm of josh meekings . celtic\u015b adam matthews -lrb-right -rrb-slides in with a strong challenge on nick ross in the scottish cup semi-final . ' given the level of reaction from our supporters and across football , we are duty bound to seek an understanding of what actually happened ,\u0107eltic said in a statement . they added , ' we have not been given any other specific explanation so far and this is simply to understand the circumstances of what went on and why such an obvious error was made .however , the parkhead outfit made a point of congratulating their opponents , who have reached the first-ever scottish cup final in their history , describing caley as a ' fantastic club and saying ' reaching the final is a great achievement .\u0107eltic had taken the lead in the semi-final through defender virgil van dijk\u015b curling free-kick on 18 minutes , but were unable to double that lead thanks to the meekings controversy . it allowed inverness a route back into the game and celtic had goalkeeper craig gordon sent off after the restart for scything down marley watkins in the area . greg tansey duly converted the resulting penalty . edward ofere then put caley thistle ahead , only for john guidetti to draw level for the bhoys . with the game seemingly heading for penalties , david raven scored the winner on 117 minutes , breaking thousands of celtic hearts . celtic captain scott brown -lrb-left -rrb-protests to referee steven mclean but the handball goes unpunished . griffiths shows off his acrobatic skills during celtic\u015b eventual surprise defeat by inverness . celtic pair aleksandar tonev -lrb-left -rrb-and john guidetti look dejected as their hopes of a domestic treble end . Ground-truth: celtic were defeated 3-2 after extra-time in the scottish cup semi-final . leigh griffiths had a goal-bound shot blocked by a clear handball. however, no action was taken against offender josh meekings . the hoops have written the sfa for an 'understanding' of the decision . : john hartson was once on the end of a major hampden injustice while playing for celtic . but he can not see any point in his old club writing to the scottish football association over the latest controversy at the national stadium . hartson had a goal wrongly disallowed for offside while celtic were leading 1-0 at the time but went on to lose 3-2 . Our Baseline: john hartson scored the late winner in 3-2 win against celtic . celtic were leading 1-0 at the time but went on to lose 3-2 . some fans have questioned how referee steven mclean and additional assistant alan muir could have missed the infringement . Multi-task: celtic have written to the scottish football association in order to gain an ' understanding ' of the refereeing decisions . the hoops were left outraged by referee steven mclean 's failure to award a penalty or red card for a clear handball in the box by josh meekings . celtic striker leigh griffiths has a goal-bound shot blocked by the outstretched arm of josh meekings .  , our baseline, and 3-way multi-task model with summarization and both entailment generation and question generation. The boxed-red highlighted words/phrases are not present in the input source document in any paraphrasing form. All the unboxedgreen highlighted words/phrases correspond to the salient information. See detailed discussion in  As shown, the outputs from  and the baseline both include nonentailed words/phrases (e.g. \"john hartson\"), as well as they missed salient information (\"hoops\", \"josh meekings\", \"leigh griffiths\") in their output summaries. Our multi-task model, however, manages to accomplish both, i.e., cover more salient information and also avoid unrelated information. The boxed-red highlights are extraneously-generated words not present/paraphrased in the input document. The unboxed-green highlights show salient phrases.Abstractiveness Analysis As suggested in See et al. , we also compute the abstractiveness score as the number of novel n-grams between the model output summary and source document. As shown in , our multi-task model (EG + QG) is more abstractive than . ConclusionWe presented a multi-task learning approach to improve abstractive summarization by incorporating the ability to detect salient information and to be logically entailed by the document, via question generation and entailment generation auxiliary tasks. We propose effective soft and highlevel (semantic) layer-specific parameter sharing and achieve significant improvements over the state-of-the-art on two popular datasets, as well as a generalizability/transfer DUC-2002 setup.the task is to predict the answer span in the comprehension. However, in our question generation task, we extract the sentence from the comprehension containing the answer span and create a sentence-question pair similar to . The dataset has around 100K sentence-question pairs from 536 articles. A.2 Training DetailsThe following training details are common across all models and datasets. We use LSTM-RNN in our sequence models with hidden state size of 256 dimension. We use 128 dimension word embedding representations. We do not use dropout or any other regularization techniques, but we clip the gradient to allow a maximum gradient norm value of 2.0. We use Adam optimizer  with a learning rate of 0.001. Also, we share the word embeddings representation of both encoder and decoder in our models. All our tuning decisions (including soft/hard and layerspecific sharing decisions) were made on the appropriate validation/development set. CNN/DailyMail: For all the models involving CNN/DailyMail dataset, we use a maximum encoder RNN step size of 400 and a maximum decoder RNN step size of 100. We use a minibatch size of 16. We initialize the LSTM-RNNs with uniform random initialization in the range [\u22120.02, 0.02]. We set \u03bb to 1.0 in the joint crossentropy and coverage loss. Also, we only add coverage to the converged model with attention and pointer mechanism, and make the learning rate from 0.001 to 0.0001. During multi-task learning, we use coverage mechanism for primary (CNN/DailyMail summarization) task but not for auxiliary tasks (because they do not have traditional redundancy issues). The penalty coefficient \u03b3 for soft-sharing is set to 5 \u00d7 10 \u22125 and 1 \u00d7 10 \u22125 for 2-way and 3-way multi-task models respectively (the range of the penalty value is intuitively chosen such that we balance the crossentropy and regularization losses). In inference time, we use a beam search size of 4, following previous work . Gigaword: For all the models involving Gigaword dataset, we use a maximum encoder RNN step size of 50 and a maximum decoder RNN step size of 20. We use a mini-batch size of 256. We initialize the LSTM-RNNs with uniform random initialization in the range [\u22120.01, 0.01]. We do not use cov-Input Document: john hughes has revealed how he came within a heartbeat of stepping down from his job at inverness as the josh meekings controversy went into overdrive this week . the caley thistle boss says he felt so repulsed by the gut-wrenching predicament being endured by his young defender -before he was dramatically cleared -that he was ready to walk away from his post and the games he loves , just weeks before an historic scottish cup final date . keen cyclist hughes set off on a lonely bike ride after hearing meekings had been cited for the handball missed by officials in the semi-final against celtic , and admits his head was in a spin over an affair that has dominated the back-page headlines since last sunday . inverness defender josh meekings will be allowed to appear in scottish cup final after his ban was dismissed . only messages of support awaiting him on his return from footballing friends brought him back from the brink of quitting . hughes , who lives in the black isle just north of inverness , said : ' i came in here this morning after a day off . i turned my phone off and was away myself , away out on the bike with plenty of thinking time : a great freedom of mind . ' i was that sick of what has been going on in scottish football i was seriously contemplating my own future . i 'm serious when i say that . ' i had just had it up to here and was ready to just give it up . if it was n't for what happened when i turned my phone back on , with the phone calls and texts i received from people i really value in football , that my spirits picked up again . ' the calls and texts came in from all over the place , from some of the highest levels across the game . i 've had phone calls that have really got me back on my feet . ' i would n't like to name them all , but there were a lot of good people and a good few close friends in the football fraternity . meekings was not sent off and no penalty was given as inverness went on to beat celtic 3-2 after extra-time . ' they were saying : \" you need to lead from the front , you need to fight it . \" that restored and galvanised that focus and drive in me . and , if that was how i was feeling , how was the boy josh meekings feeling ? it should never have come to this . ' meekings was cleared to play in the final by the judicial panel yesterday , but hughes insists this ' unprecedented ' sfa wrangle must be the catalyst for change in scottish football 's governance . although those who sit on the panel are drawn from many walks of life , ranging from former players and coaches to ex-refs and members of the legal profession , hughes said he wants ' real football people ' drafted in instead of the ' suits ' he claims lack understanding of the nuances and spirit of the professional game . and he seemed to point a thinly-veiled finger of accusation at sfa chief executive stewart regan by alleging that compliance officer tony mcglennan was a mere ' patsy ' in the process . (...) Ground-truth: Inverness defender josh meekings has won appeal against one-match ban . the 22year-old was offered one-game suspension following incident . however , an independent judicial panel tribunal overturned decision . inverness reached the scottish cup final with 3-2 win over celtic . : Josh meekings has been cleared to play in the scottish cup final .The englishman admitted he was fortunate not to have conceded a penalty and been sent off by referee steven mclean for stopping leigh griffiths net-bound effort on his goal-line . Meekings was not sent off and no penalty was given as inverness went on to beat celtic 3-2 . Our Baseline: Josh meekings cleared to play in the scottish cup final on may 30 . Inverness defender josh meekings will be allowed to appear in scottish cup final . Meekings was not sent off and no penalty was given as inverness went on to beat celtic 3-2 . Multi-task: Josh meekings has been cleared to play in the scottish cup final . Inverness defender josh meekings will be allowed to appear in scottish cup final after his ban was dismissed . Inverness went on to beat celtic 3-2 after extra-time .  , our baseline, and 2-way multitask model with summarization and entailment generation. Boxed-red highlighted words/phrases are not present in the input source document in any paraphrasing form. As shown, both  and the baseline generate extraneous information that is not entailed by the source documents (\"referee steven mclean\" and \"may 30\"), but our multi-task model avoids such unrelated information to generate summaries that logically follow from the source document.Input Document: bending and rising in spectacular fashion , these stunning pictures capture the paddy fields of south east asia and the arduous life of the farmers who cultivate them . in a photo album that spans over china , thailand , vietnam , laos and cambodia , extraordinary images portray the crop 's full cycle from the primitive sowing of seeds to the distribution of millions of tonnes for consumption . the pictures were taken by professional photographer scott gable , 39 , who spent four months travelling across the region documenting the labour and threadbare equipment used to harvest the carbohydrate-rich food . scroll down for video . majestic : a farmer wades through the mud with a stick as late morning rain falls on top of dragonsbone terraces in longsheng county , china . rice is a staple food for more than one-half the world 's population , but for many consumers , its origin remains somewhat of a mystery . the crop accounts for one fifth of all calories consumed by humans and 87 per cent of it is produced in asia . it is also the thirstiest crop there is -according to the un , farmers need at least 2,000 litres of water to make one kilogram of rice . mr gable said he was determined to capture every stage of production with his rice project -from the planting to the harvesting all the way down to the shipping of the food . after acquiring some contacts from experts at cornell university in new york and conducting his own research , he left for china last may and spent the next four months traveling . he said : ' the images were taken over a four month period from april to july last year across asia . i visited china , thailand , vietnam , laos and cambodia as part of my rice project . video courtesy of www.scottgable.com . breathtaking : a paddy field worker toils on the beautiful landscape of dragonsbone terraces in longsheng county , china . farmers ' procession : a rice planting festival parade takes place near the village of pingan in guangxi province , china . ' the project is one part of a larger three part project on global food staples -rice , corn and wheat . i am currently in the process of shooting the corn segment . ' the industrialisation of our food and mono-culture food staples have interested me for some time so that 's probably what inspired me to do this project . ' i shot the whole project using a canon slr and gopros . the actual shooting took four months and then post production took another four more months . ' the reaction to my work has been incredibly positive -i was able to secure a solo gallery show and create quite a bit of interest online which has been great . ' family crop : a hani woman in traditional clothing sits on top of her family 's rice store in yunnan province , china . arduous labour : employees of taiwan 's state-run rice experimental station are pictured beating rice husks by hand as the sun shines on them . mr gable spent months learning mandarin chinese in preparation for his trip , but the language barrier was still his greatest challenge . (...) Ground-truth: the spectacular photos were taken at paddy fields in china , thailand , vietnam , laos and cambodia . photographer scott gable spent four months travelling region to document the process of harvesting the crop . rice accounts for one fifth of all calories consumed by humans but crop is often still cultivated in primitive way . : the pictures were taken by professional photographer scott gable , 39 , who spent four months travelling across the region documenting the labour and the arduous life of the farmers who cultivate them . the images were taken over a four month period from april to july last year across asia . mr gable said he was determined to capture every stage of production with his rice project . Our Baseline: rice is a staple food for more than one-half the world 's population . crop accounts for one fifth of all calories consumed by humans and 87 per cent of it is produced in asia . Multi-task: in a photo album that spans over china , thailand , vietnam , laos and cambodia , extraordinary images portray the crop 's full cycle from the primitive sowing of seeds to the distribution of millions of tonnes for consumption . the crop accounts for one fifth of all calories consumed by humans and 87 per cent of it is produced in asia .  , our baseline, and 2-way multi-task model with summarization and question generation. All the unboxed-green highlighted words/phrases correspond to the salient information (based on the cloze-blanks of the original CNN/DailyMail Q&A task/dataset (Hermann et al., 2015)). As shown, our multi-task model is able to generate most of this saliency information, while the outputs from  and baseline missed most of them, especially the country names.Input Document: celtic have written to the scottish football association in order to gain an ' understanding\u00f3f the refereeing decisions during their scottish cup semi-final defeat by inverness on sunday . the hoops were left outraged by referee steven mclean\u015b failure to award a penalty or red card for a clear handball in the box by josh meekings to deny leigh griffith\u015b goal-bound shot during the first-half . caley thistle went on to win the game 3-2 after extra-time and denied rory delia\u015b men the chance to secure a domestic treble this season . celtic striker leigh griffiths has a goal-bound shot blocked by the outstretched arm of josh meekings . celtic\u015b adam matthews -lrb-right -rrb-slides in with a strong challenge on nick ross in the scottish cup semi-final . ' given the level of reaction from our supporters and across football , we are duty bound to seek an understanding of what actually happened ,\u0107eltic said in a statement . they added , ' we have not been given any other specific explanation so far and this is simply to understand the circumstances of what went on and why such an obvious error was made .however , the parkhead outfit made a point of congratulating their opponents , who have reached the first-ever scottish cup final in their history , describing caley as a ' fantastic club and saying ' reaching the final is a great achievement .\u0107eltic had taken the lead in the semi-final through defender virgil van dijk\u015b curling free-kick on 18 minutes , but were unable to double that lead thanks to the meekings controversy . it allowed inverness a route back into the game and celtic had goalkeeper craig gordon sent off after the restart for scything down marley watkins in the area . greg tansey duly converted the resulting penalty . edward ofere then put caley thistle ahead , only for john guidetti to draw level for the bhoys . with the game seemingly heading for penalties , david raven scored the winner on 117 minutes , breaking thousands of celtic hearts . celtic captain scott brown -lrb-left -rrb-protests to referee steven mclean but the handball goes unpunished . griffiths shows off his acrobatic skills during celtic\u015b eventual surprise defeat by inverness . celtic pair aleksandar tonev -lrb-left -rrb-and john guidetti look dejected as their hopes of a domestic treble end . Ground-truth: celtic were defeated 3-2 after extra-time in the scottish cup semi-final . leigh griffiths had a goal-bound shot blocked by a clear handball. however, no action was taken against offender josh meekings . the hoops have written the sfa for an 'understanding' of the decision . : john hartson was once on the end of a major hampden injustice while playing for celtic . but he can not see any point in his old club writing to the scottish football association over the latest controversy at the national stadium . hartson had a goal wrongly disallowed for offside while celtic were leading 1-0 at the time but went on to lose 3-2 . Our Baseline: john hartson scored the late winner in 3-2 win against celtic . celtic were leading 1-0 at the time but went on to lose 3-2 . some fans have questioned how referee steven mclean and additional assistant alan muir could have missed the infringement . Multi-task: celtic have written to the scottish football association in order to gain an ' understanding ' of the refereeing decisions . the hoops were left outraged by referee steven mclean 's failure to award a penalty or red card for a clear handball in the box by josh meekings . celtic striker leigh griffiths has a goal-bound shot blocked by the outstretched arm of josh meekings .  , our baseline, and 3-way multi-task model with summarization and both entailment generation and question generation. The boxed-red highlighted words/phrases are not present in the input source document in any paraphrasing form. All the unboxedgreen highlighted words/phrases correspond to the salient information. See detailed discussion in  and  above. As shown, the outputs from  and the baseline both include nonentailed words/phrases (e.g. \"john hartson\"), as well as they missed salient information (\"hoops\", \"josh meekings\", \"leigh griffiths\") in their output summaries. Our multi-task model, however, manages to accomplish both, i.e., cover more salient information and also avoid unrelated information.Premise: People walk down a paved street that has red lanterns hung from the buildings. Entailment: People walk down the street. Premise: A young woman on a boat in a light colored bikini kicks a man wearing a straw cowboy hat. Entailment: A young woman strikes a man with her feet. Question: In what year was the college of science established ? Input: Notable athletes include swimmer sharron davies , diver tom daley , dancer wayne sleep , and footballer trevor francis . Question: What is the occupation of trevor francis ? : Output examples from our question generation model. erage mechanism to our Gigaword models. Also, we set our beam search size to 5, following previous work . DUC: For the CNN/DM to DUC domain-transfer experiments where we allow the beam sizes of all models to be individually re-tuned on DUC-2003, the chosen tuned beam values are 10, 4, 3 for the multi-task model, baseline, and See et al. , respectively. A.2.1 Multi-Task Learning DetailsMulti-Task Learning with Question Generation Two important hyperparameters tuned are the mixing ratio between summarization and entailment generation, as well as the soft-sharing coefficient. Here, we choose the mixing ratios 3:2 between CNN/DailyMail and SQuAD, 100:1 between Gigaword and SQuAD. Intuitively, these mixing ratios are close to the ratio of their dataset sizes. We set the soft-sharing coefficient \u03b3 to 5 \u00d7 10 \u22125 and 1 \u00d7 10 \u22125 for CNN/DailyMail and Gigaword, resp.Multi-Task Learning with Entailment Generation Here, we choose the mixing ratios 3:2 between CNN/DailyMail and SNLI, 20:1 between Gigaword and SNLI. We again set the soft-sharing coefficient \u03b3 to 5 \u00d7 10 \u22125 and 1 \u00d7 10 \u22125 for CNN/DailyMail and Gigaword, resp.Multi-Task Learning with Question and Entailment Generation Here, we choose the mixing ratios and soft-sharing coefficients to be 4:3:3 and 5 \u00d7 10 \u22125 for CNN/DailyMail, and 100:1:5 and 1.5 \u00d7 10 \u22126 for Gigaword respectively. A.3 Auxiliary Output AnalysisA.3.1 Entailment Generation Examples See  for interesting output examples by our entailment generation model. A.3.2 Question Generation ExamplesSee  for interesting output examples by our question generation model.\n###\n"}
{"summary": "Result: Highest MDI/All Accuracy (MI)/All Accuracy (TI)/All Accuracy (MI & TI)\nhas feature: Textual/Multimedia/l_home\nHas metric: Mean Decrease in Impurity\nMaterial: web pages\nhas research problem: Knowledge Gain Prediction/Knowledge Gain Prediction\nHas value: 0.039/36.4/37.0/38.7\nAmount of data: 110/13\nbased on: random forest model\nMethod: lab study\nhas unit: %\nHas unit : %/%", "text": "#Properties\nResult, has feature, Has metric, Material, has research problem, Has value, Amount of data, based on, Method, has unit, Has unit \n#Text\nIn informal learning scenarios the popularity of multimedia content, such as video tutorials or lectures, has significantly increased. Yet, the users' interactions, navigation behavior, and consequently learning outcome, have not been researched extensively. Related work in this field, also called search as learning, has focused on behavioral or text resource features to predict learning outcome and knowledge gain. In this paper, we investigate whether we can exploit features representing multimedia resource consumption to predict of knowledge gain (KG) during Web search from in-session data, that is without prior knowledge about the learner. For this purpose, we suggest a set of multimedia features related to image and video consumption. Our feature extraction is evaluated in a lab study with 113 participants where we collected data for a given search as learning task on the formation of thunderstorms and lightning. We automatically analyze the monitored log data and utilize state-of-the-art computer vision methods to extract features about the seen multimedia resources. Experimental results demonstrate that multimedia features can improve KG prediction. Finally, we provide an analysis on feature importance (text and multimedia) for KG prediction. User Study and Data CollectionThe participants (N=113, 22.86 \u00b1 2.92 years old, 96 females) of our lab study were asked to solve a realistic learning task, that is to understand the principles of thunderstorms and lightning. The topic of the formation of thunderstorms and lightning has been used in many studies that investigated learning with multimedia (e.g., ). This topic is related to natural sciences and has been chosen since it requires learners to gain knowledge about different physical and meteorological concepts and their interplay. The learning task itself can be classified as a causal task  in which learners need to learn about the causal chains of events. They need therefore to acquire declarative as well as procedural knowledge  about different concepts to gain comprehensive knowledge. We believe that this task is a suitable representative for a class of various and similar tasks. For example, comparable causal tasks would be learning about the greenhouse effect or photosynthesis. The acquisition of information about causal tasks can be accomplished through studying different representation formats like text, pictures, videos, or their combined presentation on Web pages.Technical Setup All search and learning activities of participants were conducted within a tracking framework that consists of two layers. The SMI (Sen-soMotoric Instruments) ExperimentCenter (3.7) software enabled us to track participants' activities during Web search through screen recordings and navigation log files. We implemented a second tracking layer in the browser using plugins. These plugins saved all visited HTML files and tracked additionally navigation and interaction data (e.g., mouse movements) in local log files. The local and external tracking was realized through JavaScript code integrated into the plugin \"Greasemonkey\" (3.11) running in the browser. To track HTML files, we used the plugin \"ScrapBook\" (1.5.14) which allowed us to automatically and simultaneously save all visited HTML pages (HTML files and folders) seen by the participants.Knowledge Test Based on previous work  we developed a 10-item multiplechoice knowledge test on the formation of thunderstorm and lightning. To measure participants' pre-knowledge state (pre-KS), this test had to be completed two days before the Web search task, and a second time after the Web search task to measure the post-knowledge state (post-KS). Pre-and post-knowledge states were represented by a score that counted the number of correct answers (out of 10). Knowledge gain (KG) is the difference between the two scores. Multimedia Feature ExtractionIn this section, we outline how we generated the multimedia features based on text, but mainly for image and video content that serve as input for knowledge gain prediction. The output of the data logging per user is the input for our feature extraction process. The data logging output consists of a screen recording (MPEG-4 video format (*.mp4)), a timeline of visited Web pages, as well as HTML and CSS files of every visited Web page. In a first step, Web pages are segmented into regions of headlines, (normal) text, images, etc. using a stateof-art method for document layout analysis (Section 3.1). The image regions are further processed through image type classification to infer the kind of seen content (Section 3.2). In Section 3.3 we describe the feature extraction process for text content. Both feature types are then utilized to predict knowledge gain using a random forest classifier. To reconstruct the visited Web pages, we exploit the screen recordings and segment them according to the timeline of the search session. The timeline should reflect the order of Web pages getting into focus, rather than the points in time URLs were opened in their respective browser tabs. In this way, we circumvent the problem of participants opening multiple links from the search result page at once in new tabs, leading to a flawed timeline. An overview of the framework is displayed in . As shown in , the first step separates the total number of F video frames into L learning relevant and N navigationrelated frames, with N + L = F . We extract a frame every second of the video (|F | = 173 787), but only keep those where the participant spent time on Web pages related to learning (and not navigating the browser or procrastinating). Thus, we excluded (study-specific) URLs containing Google, TripAdvisor and adblock. This procedure resulted in a total of 119 164 (average: 1268 frames per session) learning relevant frames which have to be segmented into pictorial, textual, and background information as described in the next section. Document Layout AnalysiseThe goal of this step is to derive features on document layout by automatically dividing each frame l \u2208 L into coherent regions that represent the structure of the page. Additionally, the regions should be classified according to their content, e.g., image, text, menu, etc. This procedure is crucial for the image content analysis later but also challenging since the layout and design of the Web pages vary heavily. To address this challenge, we utilize the Mask R-CNN  network architecture, originally implemented for instance (object) segmentation, and fine-tune the provided pre-trained weights of the network. We annotated 300 randomly chosen frames from our user study using the browser-based \"VGG Image Annotator\". Six region classes are distinguished:1. Heading: any headlines or titles that divide the page into sections; 2. Menu bar: buttons or lists of buttons displayed for navigational purposes; 3. Content list: enumerations like table of contents or bullet point lists; 4. Text: any coherent text block that is not part of headlines or button labels; 5. Images/Frames: All types of images (no size constraints) from small thumbnails to fullscreen video frames; 6. Background: everything that does not fit into the five other classes.These classes are supposed to reflect the core parts of a Web page. The JSON (JavaScript Object Notation) style output of the manual annotations was then split into 90% training and 10% test data, which we used to fine-tune the fully-connected layers after the pre-trained bounding box detector (i.e., network heads) for 30 epochs with a learning rate of lr = 0.001. This option is predefined (by the Mask R-CNN authors) by creating the model with parameter layers = \"heads\" and subsequently only retrains the region proposal network (RPN), the classifier, and the mask heads. The resulting network is able to segment our screen recording frames appropriately. An example output is depicted in .In addition to our six classes we also compute the average image size per frame since we want to differentiate if a Web page with 20% visual content contained five small images or a single large one. Another merit of this feature is its ability to also indirectly measure the viewing time of videos since it is difficult to measure this feature directly with satisfactory accuracy. For instance, embedded videos on Web pages other than YouTube cannot always be captured.  Lastly, the results per frame in equation 1 are summed up for all seen learning frames i \u2208 L and divided by |L| yielding seven features per participant p.We identified a total of 755 756 bounding boxes that belonged to the \"Images/Frames\" class, which has around five samples per frame on average. This appears to be a lot at first, but has a simple explanation. Every (Web page) frame that is recorded when watching a (non-maximized) YouTube video contains 10 thumbnails of other recommended videos. In order to not skew the results heavily towards this large number of irrelevant images, we filtered them out if their height or width is below 100 pixels (full image resolution was 1280x800). The remaining samples will be further examined regarding their shown content. Image Type ClassificationThis section briefly outlines how the images detected in the document layout analysis are examined regarding their content. We aim to predict the given type of an image. To the best of our knowledge, there is no comprehensive and taskspecific taxonomy of image types that can be directly applied to the learning task of our study. Therefore, we focused on covering all topic-relevant categories to learn which type of images a learner saw when searching for the formation of thunderstorms. As a result, our set of image type classes consists of Infographics, Indoor Photo, Maps, Outdoor Photo, Technical Drawings, and Information Visualization. The class Information Visualization has a specific role. Images that are composites or hybrids of common visualization types are hard to assign to a unique class. For this reason, we merge all forms of Information Visualizations into one class and use it as a fallback class to gather all frames that are otherwise hard to assign. The implementation was done in Keras, using a MobileNet  architecture with default parameters. We utilized a Google image crawler to gather 18 773 unique training samples which we split into 90% training and 10% test data. Three volunteers manually labeled the test data and achieved an intercoder agreement of \u03b1 = 0.85 (across all annotators, samples, and classes) according to Krippendorff's alpha . Finally, the classifier achieved an accuracy of 87.15% on this human-verified test set. The accuracy is sufficient for our task of knowledge gain prediction, as it is confirmed by the experimental results in Section 4.The features do not represent the number of images seen per class, because a consequence of our frame-wise extraction is that the same image gets extracted multiple times. Instead we analyse the image in every frame again and report the fraction of the image types as a percentage. The idea is to weight the content according to the duration the images have been seen by the learner. The feature vector v p representing the image types seen by participant p is defined as follows:Feature vector for the six image types seen per participant. p(< class >) is the pseudo-probability given by the softmax layer. N l is the number of images detected in frame l. Text FeaturesIn total, we used a set of 110 features to represent textual information 5 , taking into account document complexity, HTML structure, and linguistic aspects. Document Complexity Features. Based on the assumption that the document complexity is correlated with the user's knowledge state on a topic, we have extracted several features related to document complexity. Motivated by previous work  and our investigation of the data, we extracted the number of words (c word), length of words (c char), and length of sentences (c sentence) as features. Related work  suggests that the syntactic structure of a document, which can be represented by the ratio of the number of nouns, verbs, adjectives, or other words to the total number of words (c {noun, verb, adj, oth}) is likely to imply the complexity of its content.There are several widely used metrics for assessing the readability or complexity of a textual document, which have been studied to be correlated with user's knowledge level . We used Gunning Fog Grade 6 (c gi), SMOG  (c smog) and Flesch-Kincaid Grade  (c f k) as features. HTML Structural Features. A possible explanation of the finding, that there is a negative association between the number of hyperlinks embedded in a Web page and users' KG , is that people may not focus on the content in the presence of too many embedded links. Hence, we extract the feature h link by quantifying the number of outbound links (i.e., the < a > elements in our case). Furthermore, we extract features that might indicate the readability of a Web page based on HTML tags, namely, the average length of each paragraph (h p), the < ul > elements embedded (h oth ul), and the number of scripts (h script).Linguistic Features. Related work  suggests that the number of words on Web pages that are correlated with different psychological processes and basic sentiment can influence a learner's cognitive state. The writing style could also affect the readability of a learning resource and the engagement of readers. Motivated by the above observations, we used the 2015 Linguistic Inquiry and Word Count (LIWC) dictionaries 7 to compute linguistic features that reflect the psychological processes, sentiment, and the writing style of Web page content. The features of this type are prefixed with l in the remainder of the paper. Experimental Results for Knowledge Gain PredcitionIn this section, we report experimental results for the task of knowledge gain prediction utilizing the features from Section 3. Our experimental dataset consists of 113 search sessions. On average, users have issued 11.1 queries and browsed 25.4 Web pages in each session. There was a significant increase in learners' knowledge on average (KG = 2.15 \u00b1 1.84 for a full score of 10) after the search phase. The effect size for knowledge gain was large (Cohen's d = 1.29). The average pre-knowledge score was 5.22 \u00b1 1.76 and post-knowledge was 7.37 \u00b1 1.6. Experimental SetupThe goal of our study is to predict knowledge gain in informal search sessions and to investigate the impact of text and multimedia resource features. We model KG prediction as a classification task and use random forest as a supervised learning approach. We aim for a fair comparison with the state of the art in users' knowledge gain prediction in Web search. Thus, we follow the same experimental setup as used by Yu et al. , in particular for the assignment of labels, the applied classifier, and its parameter tuning, unless other settings are denoted.Ground Truth Data: We group a search session into one of three KG classes according to the measured knowledge gain X based on the Standard Deviation (\u03c3) Classification approach. The classes are defined as follows: 1.) Low KG, if X < X \u2212 \u03c3 2 ; 2.) Moderate KG, if X \u2212 \u03c3 2 < X < X + \u03c3 2 ; and 3.) High KG, if X > X + \u03c3 2 . According to this approach, our dataset consists of 44 low, 42 moderate, and 27 high knowledge gain sessions.Classifier: Random forest has shown to be the most effective classifier for knowledge gain prediction  and it allows for the analysis of feature importance. Hence, we adopt a random forest classifier and tune the hyperparameters for accuracy using grid search. For our experiments, we used the scikit-learn library for Python ( After tuning the hyper-parameters of each classifier, we run 10 repetitions of 10-fold cross-validation (90% train, 10% test) and evaluate the classification results of each classifier according to the following metrics: -Accuracy (Accu) across all classes: percentage of search sessions that were classified with the correct class label. Classification Results. The performance of the random forest classifier using textual features (TI), multimedia features (MI), as well as their combination is shown in . We also present the performance of the random forest classifier using behavioral features (the approach used by ) on our ground truth dataset in  for reference. The results for all four test feature types are in a comparable range. When using only textual features (TI) or multimedia features (MI), results are slightly worse than the state-of-the-art approach , which uses behavioral features. Our approach utilizing features from both categories (MI&TI) has achieved the best performance concerning overall accuracy, indicating that the combination of textual and multimedia features has the potential to improve knowledge gain prediction. Comparing the performance for the different classes, the classifier performs better on low and moderate knowledge gain classes when using features from both categories. A potential reason for this result is that the high knowledge gain class has the least amount of training data in our ground truth dataset. Please note that we have achieved comparable performance to the state of the art  with less training data (113 sessions versus 468 sessions) and more unbalanced classes. As shown in  the combination of multimedia and textual information (MI&TI) is able to outperform using behavior features at 85% confidence level in terms of accuracy. However, please note that we did not extract all of the behavioral features introduced in the related work, in particular, the features relevant to the clicking on the search results page were not recorded in our study. Since we focus on understanding the influence of textual and multimedia resource content on users' knowledge gain during the search, the classification model and the analysis of user behavior features are out of the scope of this work. We list the results of the classifier trained on user behavior features as evidence that our classification has reached satisfying performance.  : Results of knowledge gain prediction (in %) using text (TI) and multimedia features (MI), and comparing them with the state of the art .Feature Importance To analyze the usefulness of individual features, we make use of the Mean Decrease in Impurity (MDI) metric based on the random forest model. The metric MDI is defined as the total decrease in node impurity (weighted by the probability of reaching that node) averaged over all trees of the ensemble . Due to the space limitation, we only list and discuss the 20 features  having the highest and lowest MDI values in the paper.We observe that six out of 10 features with the highest importance are textual features. This is to be expected because, first, there are more textual features (110) than multimedia features , and, second, with recent advances in natural language processing techniques, we were able to design more sophisticated textual features such as the complexity of language and sentiment behind words. In contrast, it is still more challenging to analyze the semantics of multimedia data. Nevertheless, results indicate that the 13 multimedia features have shown promising importance for the classification, with Heading, imgsize, Menu Bar, Infographic, Technical Drawing and Outdoor rank at 4, 5, 8, 9, 13, 15, respectively, among the 123 features in total. None of the multimedia features falls into the 10 least important features according to MDI. Among the six textual features with the highest importance, five are linguistic-based, while the remaining one is related to document complexity (SMOG Readability). ConclusionsIn this paper, we have investigated whether features describing multimedia resource content can help predict users' knowledge gain in a search as learning task. Our results are based on a lab study with N=113 participants, where we recorded the individuals' behavior and the accessed Web resources. Afterwards, we applied computer vision methods to screen recordings to segment the seen Web pages into meaningful regions, and then further classified the image regions into task-specific image types. Finally, we used the textual and multimedia features to classify the knowledge gain of the participants. The combination of our different feature categories and detailed feature importance assessment showed that our approach can serve for knowledge gain prediction based on viewed resource content, which potentially can help improve a learning-oriented search result ranking (if content features are used accordingly). Although the classification accuracy is on a moderate level in terms of recall and precision, they suggest that knowledge gain is predictable. Particularly image and video features improved the classification notably when used jointly with text-based features. To the best of our knowledge, that was the first study that analyzed the importance of multimedia features in a SAL scenario.Although the number of participants in our study is already higher than in the majority of previous studies in controlled lab settings, our current dataset is limited by the fact that only one learning task has been studied. In the future, we aim to conduct additional studies on diverse learning topics, to receive further insights into the relationship between features of learning resources used and knowledge gain.\n###\n"}
{"text": "#Properties\nimplementation, metric, has research problem, evaluation, Approach type, Document type, Summary usage, Summary characteristics\n#Text\nUsers of social media sites, such as Twitter, rapidly generate large volumes of text content on a daily basis. Visual summaries are needed to understand what groups of people are saying collectively in this unstructured text data. Users will typically discuss a wide variety of topics, where the number of authors talking about a specific topic can quickly grow or diminish over time, and what the collective is saying about the subject can shift as a situation develops. In this paper, we present a technique that summarises what collections of Twitter users are saying about certain topics over time. As the correct resolution for inspecting the data is unknown in advance, the users are clustered hierarchically over a fixed time interval based on the similarity of their posts. The visualisation technique takes this data structure as its input. Given a topic, it finds the correct resolution of users at each time interval and provides tags to summarise what the collective is discussing. The technique is tested on a large microblogging corpus, consisting of millions of tweets and over a million users. Dynamic Text VisualisationA number of systems have looked at how to represent dynamically evolving textual data, often in the context of news stories or social media networks. ThemeRiver encodes the frequency of terms as horizontal streams than grow and shrink over time. present a method for depicting the evolution of tag clouds, using animation. The tags selected for animation have high \"interestingness\": a value computed based on tag frequency and variability. presented a method that characterises tags and their evolution in terms of frequency, by overlaying spark lines on each tag. visualise conversations in Twitter data using \"topic streams\" that visually represented as stacked graphs. Their system scales to data sets of over a million tweets and successfully identified conversations in the data. (a) Search for \"Obama\" (b) Tracking the \"Libya\" crowd : Overview of usage of ThemeCrowds. (a) A general search for the query term \"obama\". The search reveals many topics across time with some clusters of users, or crowds, pertaining to the uprising in Libya and others pertaining to the 2012 Presidential Election. These distinct topics are uncovered by reading frequently used terms around Obama. (b) The user selects a crowd on 4 March 2011 pertaining to Libya and finds the best match to it over time. Crowds matching the presidential election are filtered out and crowds pertaining to Libya become more prominent. The time step dated 3 March 2011 finds a cluster that may be less focused on Obama but contains enough discussion about Libya to be relevant.These techniques illustrate the evolution of tags in dynamic text data. However, in our problem, we need to illustrate the dynamic evolution of clusters of Twitter authors at the correct resolution. This problem requires the simultaneous visualisation of cluster content and tag frequency which these systems do not directly support. Clustered Text VisualisationA number of systems have looked at the problem of visualising clusters of documents in order to better understand the content of clustered document collections. IN-SPIRE creates landscapes of documents using dimensionality reduction based on document statistics. FacetAtlas describes a technique to encode entities, their relationships, and classes or clusters of entities. The approach is multilevel and allows users of the system to understand complex correlations across groups of documents at various levels of resolution. use animation to depict dynamically evolving document clusters and has facilities to take snapshots of the data over time. summarize news stories by clustering them in their most highly associated theme and depicts keywords and stories as they evolve over time. combine trend graphs with tag clouds to visualise cluster content and size as it evolves over time.Although many of these systems handle the evolution of temporal clusters of documents, they do not support the visualisation of topics at multiple levels of resolution. FacetAtlas does support this sort of visualisation, but it is unclear how to extend it to depict the evolution of topics over time. In our problem, we are concerned with summarizing the tweets of groups of users at an appropriate resolution, along with the evolution of their content over time. Social Media AnalysisMany researchers have become interested in content diffusion and network structure within the Twitter microblogging services, given the potential for Twitter to facilitate the rapid spread of information. provided some evidence of Twitter user communities, where the members share common interests as reflected by the terms appearing in their tweets. studied a sample of 41.7 million users and 106 million tweets. The authors studied aspects such as: identifying influential users, information diffusion, and trending topics. performed an analysis on microblogging activity during the 2008 US Presidential Debates. The authors demonstrated that frequent terms reflected the topics being discussed, but informal vocabulary complicated topic identification. Social media analysis has extensively studied large scale Twitter data along with the trends and topics that such data sets contain. However, these works do not investigate visualisation methods to support the exploration of such large volumes of data. With ThemeCrowds, we build a visualisation system to support the display and visual analysis of the most relevant resolution of topic clusters and how they evolve over time. INTERFACEThemeCrowds is aimed at depicting the most relevant clusters of users relating to a particular topic, providing an overview of those clusters, in terms of both size and textual content, and depicting how they evolve over time. The technique must scale to millions of tweets, and therefore, some form of summarisation is needed. We choose to perform a multilevel clustering based on the similarity of user tweet profiles for each day.The proposed visualisation interface for this technique is shown in . The user enters a query term in the search box at the top of the screen. Based on the term at each time step, an appropriate resolution is found and clusters enriched in the term are highlighted in yellow. The results are depicted in the multiples view of six multilevel tag clouds. Small multiples places\n###\n", "summary": " implementation: ThemeCrowds\n, has research problem: Automatic text summarization\n, Approach type: Algebraic\n, Document type: Multiple documents\n###"}
{"text": "#Properties\nhas research problem, sequencing platform, read length in base pairs (paired-end*) , reads per run in Million, runtime in days, total data output yield in Gigabyte, data output rate in Gigabyte per day, reagents cost in $, cost per Gigabyte data output in $, cost of resequencing a human genome with 30X coverage in $, cost of machine in $, has research problem, sequencing platform, read length in base pairs (paired-end*) , reads per run in Million, runtime in days, total data output yield in Gigabyte, data output rate in Gigabyte per day, reagents cost in $, cost per Gigabyte data output in $, cost of resequencing a human genome with 30X coverage in $, cost of machine in $, has research problem, sequencing platform, read length in base pairs (paired-end*) , reads per run in Million, runtime in days, total data output yield in Gigabyte, data output rate in Gigabyte per day, reagents cost in $, cost per Gigabyte data output in $, cost of resequencing a human genome with 30X coverage in $, cost of machine in $, has research problem, sequencing platform, read length in base pairs (paired-end*) , reads per run in Million, runtime in days, total data output yield in Gigabyte, data output rate in Gigabyte per day, reagents cost in $, cost per Gigabyte data output in $, cost of resequencing a human genome with 30X coverage in $, cost of machine in $, has research problem, sequencing platform, read length in base pairs (paired-end*) , reads per run in Million, runtime in days, total data output yield in Gigabyte, data output rate in Gigabyte per day, reagents cost in $, cost per Gigabyte data output in $, cost of resequencing a human genome with 30X coverage in $, cost of machine in $, has research problem, sequencing platform, read length in base pairs (paired-end*) , reads per run in Million, runtime in days, total data output yield in Gigabyte, data output rate in Gigabyte per day, reagents cost in $, cost per Gigabyte data output in $, cost of resequencing a human genome with 30X coverage in $, cost of machine in $, has research problem, sequencing platform, read length in base pairs (paired-end*) , reads per run in Million, runtime in days, total data output yield in Gigabyte, data output rate in Gigabyte per day, reagents cost in $, cost per Gigabyte data output in $, cost of resequencing a human genome with 30X coverage in $, cost of machine in $, has research problem, sequencing platform, read length in base pairs (paired-end*) , reads per run in Million, runtime in days, total data output yield in Gigabyte, data output rate in Gigabyte per day, reagents cost in $, cost per Gigabyte data output in $, cost of resequencing a human genome with 30X coverage in $, cost of machine in $, has research problem, sequencing platform, read length in base pairs (paired-end*) , reads per run in Million, runtime in days, total data output yield in Gigabyte, data output rate in Gigabyte per day, reagents cost in $, cost per Gigabyte data output in $, cost of resequencing a human genome with 30X coverage in $, cost of machine in $, has research problem, sequencing platform, read length in base pairs (paired-end*) , reads per run in Million, runtime in days, total data output yield in Gigabyte, data output rate in Gigabyte per day, reagents cost in $, cost per Gigabyte data output in $, cost of resequencing a human genome with 30X coverage in $, cost of machine in $, has research problem, sequencing platform, read length in base pairs (paired-end*) , reads per run in Million, runtime in days, total data output yield in Gigabyte, data output rate in Gigabyte per day, reagents cost in $, cost per Gigabyte data output in $, cost of resequencing a human genome with 30X coverage in $, cost of machine in $, has research problem, sequencing platform, read length in base pairs (paired-end*) , reads per run in Million, runtime in days, total data output yield in Gigabyte, data output rate in Gigabyte per day, reagents cost in $, cost per Gigabyte data output in $, cost of resequencing a human genome with 30X coverage in $, cost of machine in $\n#Text\n35 Sanger sequencing is based on the chain termination method which relies on separating DNA by size and the incorporation of labelled modified nucleotides. 5 454 pyrosequencing measures the amount of light produced by the incorporation of nucleotides in a cascade of enzymatic reactions under the presence of a luciferase. 5 Reversible terminator sequencing is a sequencing-by-synthesis approach where the incorporation of modified nucleotides is detected stepwise. 5 Ion semiconductor sequencing analyses changes of hydrogen ion concentration during the incorporation of nucleotides into the DNA strand. 5 Single-molecule real-time (SMRT) sequencing monitors without interruption the incorporation of differently fluorescent-tagged nucleotides by the polymerase activity. 5 Nanopore sequencing detects the identity of nucleotides within the DNA strand while it is passing through a nanopore. 5 The availability of next-generation sequencing (NGS) platforms transformed the field of genomics and led to a dramatic decrease in sequencing costs. Sanger SequencingTwo DNA sequencing techniques were developed in the mid-1970s. Allan Maxam and Walter Gilbert proposed a chemical cleavage method, which was initially widely used around molecular laboratories . Around the same time, Frederick Sanger and colleagues developed a chain termination method . After the chemistry needed for this method became commercially available, Sanger sequencing got the standard sequencing technique for all applications. For nearly three decades, Sanger sequencing was synonym to DNA sequencing. DNA sequencing revolutionized many fields of biological and medical sciences, and fittingly Frederick Sanger and Walter Gilbert were awarded with a Nobel Prize in Chemistry in 1980, which they shared with Paul Berg ). Sanger's chain termination method is basically based on two principles: (I) DNA can be separated by size and (II) DNA polymerases is able to incorporate modified nucleotides. As DNA is negatively charged, gel electrophoresis allows the separation of DNA strands by size as larger DNA strands migrate slower to a positive electrode. By using polyacrylamide gels, it is even possible to detect minute differences of single base pairs between two different DNA strands. When separating DNA into single strands, it is possible to replicate one strand by the use of DNA polymerase II which will add one nucleotide after another complementary to the template DNA strand. Requirements are a DNA primer (oligonucleotide) that fits to the template and the availability of nucleotides (dNTPs) for incorporation. Nucleotides bear a 3\u2032 OH group and a 5\u2032 phosphate group which will be connected by the polymerase while removing two of the phosphates of the 5\u2032 end. However, if the 3\u2032 OH group is missing, it is impossible to connect another nucleotide, and the elongation of the template strand is terminated. Sanger used exactly such modified nucleotides, which have an H at the 3\u2032 end instead of the OH group (dideoxynucleotides, ddNTPs) for his sequencing reaction. The sequencing reaction mix is composed of DNA polymerase, a primer for the target region, and a mix of dNTPs and ddNTPs. The ratio of dNTPs/ddNTPs is usually around 100 to 1, so that termination could be obtained at least once for every position of the DNA template during the amplification process. Whenever a ddNTP is incorporated, the elongation of the template DNA stops, thereby generating DNA molecules of different sizes. Using electrophoresis, these differently sized DNA molecules can be separated, and the identity of the incorporated ddNTP will allow reconstructing the nucleotide sequence (. .1). Initially, ddNTPs were labelled radioactively, and reactions were performed for each of the four bases separately. Huge gels had to be inspected by the eye to reconstruct the sequence order, a reason why DNA output from sequencers is still known as reads. Later on, unique fluorescent labels for all four different bases (adenosine, thymine, guanine and cytosine) were introduced which could be detected by a laser while migrating through the gel. This innovation strongly decreased analysis time, as all reactions could run at the same time and increased the accuracy of base detection . Computer-based analyses allow that the detected fluorescence will be automatically associated with the corresponding nucleotide to generate a chromatogram for every sequence read (. .1).Current machines for high-throughput analyses like the ABI 3730xl are equipped with 96 capillaries. The time for a single sequencing run will take, according to the envisaged quality, 2-3 h, with up to 1000 bp sequence reads of high quality. The output for a single run will be around 100 Kb and during 24 h sequencing of 1 Mb might be realistic . After trimming the ends of sequencing reads, an accuracy of 99.999% can be achieved, and sequence errors are mainly due to errors in preceding amplification steps . As such, Sanger sequencing remains a good option for high-quality sequence reads. The first human genome sequence has been sequenced with this technique. 454 PyrosequencingThe first next-generation sequencing (NGS) technique that was commercially available has been 454 sequencing. This technique presented solutions for each of the bottlenecks typically found in large-scale classical Sanger sequencing: library preparation, template preparation and sequencing itself (Rothberg and Leamon 2008). The principle for 454 sequencing goes back to a real-time sequencing approach called pyrosequencing developed by , and a highly parallelized high-throughput automatization was presented by . . The great advantage of this method compared to Sanger sequencing is that sequences are read out while being synthesized, therefore omitting electrophoresis steps for size separation of DNA fragments. In this approach, nucleotides are released one after another and washed over the template DNA strand. A cascade of enzymatic reactions leads to the emission of detectable light signal which is in its strength proportional to the number of nucleotides incorporated during this step. For example, if C's are washed over the template DNA, the polymerase incorporates as much consecutive C's as are present in the target sequence. When being incorporated by the polymerase, a pyrophosphate is released for each nucleotide, which is subsequently converted to ATP by ATP sulphurylase (. luciferases can use this energy to oxidize luciferin, which leads to the generation of light . After detection of the signal, superfluous nucleotides are removed in a washing\n###\n", "summary": " has research problem: comparison of DNA sequencing techniques\n, sequencing platform: Illumina Next Seq 500\n, read length in base pairs (paired-end*) : 150*\n, reads per run in Million: 400\n, runtime in days: 1.2\n, total data output yield in Gigabyte: 120\n, data output rate in Gigabyte per day: 100\n, reagents cost in $: 4410\n, cost per Gigabyte data output in $: 36.75\n, cost of resequencing a human genome with 30X coverage in $: 4410\n, cost of machine in $: 250000\n, has research problem: comparison of DNA sequencing techniques\n, sequencing platform: Illumina HiSeq 2500 RR\n, read length in base pairs (paired-end*) : 100*\n, reads per run in Million: 600\n, runtime in days: 1.125\n, total data output yield in Gigabyte: 120\n, data output rate in Gigabyte per day: 106.6\n, reagents cost in $: 6145\n, cost per Gigabyte data output in $: 51.2\n, cost of resequencing a human genome with 30X coverage in $: 6144\n, cost of machine in $: 740000\n, has research problem: comparison of DNA sequencing techniques\n, sequencing platform: Illumina HiSeq 4000\n, read length in base pairs (paired-end*) : 150*\n, reads per run in Million: 5000\n, runtime in days: 3.5\n, total data output yield in Gigabyte: 1500\n, data output rate in Gigabyte per day: 400\n, reagents cost in $: 29900\n, cost per Gigabyte data output in $: 20\n, cost of resequencing a human genome with 30X coverage in $: 2400\n, cost of machine in $: 900000\n, has research problem: comparison of DNA sequencing techniques\n, sequencing platform: Illumina HiSeq X\n, read length in base pairs (paired-end*) : 150*\n, reads per run in Million: 6000\n, runtime in days: 3\n, total data output yield in Gigabyte: 1800\n, data output rate in Gigabyte per day: 600\n, reagents cost in $: 12750\n, cost per Gigabyte data output in $: 7\n, cost of resequencing a human genome with 30X coverage in $: 840\n, cost of machine in $: 1000000\n, has research problem: comparison of DNA sequencing techniques\n, sequencing platform: Illumina MiSeq\n, read length in base pairs (paired-end*) : 300*\n, reads per run in Million: 25\n, runtime in days: 2\n, total data output yield in Gigabyte: 15\n, data output rate in Gigabyte per day: 7.5\n, reagents cost in $: 1000\n, cost per Gigabyte data output in $: 93\n, cost of resequencing a human genome with 30X coverage in $: 11160\n, cost of machine in $: 99000\n, has research problem: comparison of DNA sequencing techniques\n, sequencing platform: Ion Proton P1\n, read length in base pairs (paired-end*) : 200\n, reads per run in Million: 165\n, total data output yield in Gigabyte: 10\n, reagents cost in $: 1000\n, cost per Gigabyte data output in $: 100\n, cost of resequencing a human genome with 30X coverage in $: 12000\n, cost of machine in $: 243000\n, has research problem: comparison of DNA sequencing techniques\n, sequencing platform: Ion Torrent 318 HiQ 520\n, read length in base pairs (paired-end*) : 200-400\n, reads per run in Million: 3-5\n, runtime in days: 0.37\n, total data output yield in Gigabyte: 1.2-2\n, data output rate in Gigabyte per day: 5.5\n, reagents cost in $: 600\n, cost of machine in $: 50000\n, has research problem: comparison of DNA sequencing techniques\n, sequencing platform: MinIoN MK1\n, read length in base pairs (paired-end*) : 10000\n, reads per run in Million: 0.05\n, runtime in days: 2\n, total data output yield in Gigabyte: 2.75\n, data output rate in Gigabyte per day: 1.375\n, reagents cost in $: 500\n, cost per Gigabyte data output in $: 180\n, cost of resequencing a human genome with 30X coverage in $: 21800\n, cost of machine in $: 1000\n, has research problem: comparison of DNA sequencing techniques\n, sequencing platform: PacBio RS P6-C4\n, read length in base pairs (paired-end*) : 15000\n, reads per run in Million: 5.5\n, runtime in days: 4.3\n, total data output yield in Gigabyte: 12\n, data output rate in Gigabyte per day: 2.8\n, reagents cost in $: 2400\n, cost per Gigabyte data output in $: 200\n, cost of resequencing a human genome with 30X coverage in $: 24000\n, cost of machine in $: 695000\n, has research problem: comparison of DNA sequencing techniques\n, sequencing platform: PacBio Sequel\n, read length in base pairs (paired-end*) : 12000\n, reads per run in Million: 38.5\n, runtime in days: 4.3\n, total data output yield in Gigabyte: 84\n, data output rate in Gigabyte per day: 19.5\n, reagents cost in $: 11200\n, cost per Gigabyte data output in $: 80\n, cost of resequencing a human genome with 30X coverage in $: 9600\n, cost of machine in $: 350000\n, has research problem: comparison of DNA sequencing techniques\n, sequencing platform: PromethION\n, read length in base pairs (paired-end*) : 10000\n, total data output yield in Gigabyte: 3100\n, reagents cost in $: 2500\n, cost per Gigabyte data output in $: 20\n, cost of resequencing a human genome with 30X coverage in $: 2400\n, cost of machine in $: 75000\n, has research problem: comparison of DNA sequencing techniques\n, sequencing platform: Roche 454 FLX plus\n, read length in base pairs (paired-end*) : 700\n, reads per run in Million: 1.25\n, runtime in days: 0.9\n, total data output yield in Gigabyte: 0.7\n, data output rate in Gigabyte per day: 0.75\n, reagents cost in $: 6200\n, cost per Gigabyte data output in $: 8000\n, cost of machine in $: 500000\n###"}
{"summary": "has benchmark: Benchmark ADE Corpus/Benchmark ACE 2004/Benchmark CoNLL04\nhas research problem: Relation Extraction\nhas model: multi-head + AT\nsame as: https://en.wikipedia.org/wiki/Relationship_extraction", "text": "#Properties\nhas benchmark, has research problem, has model, same as\n#Text\nAdversarial training (AT) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data. We show how to use AT for the tasks of entity recognition and relation extraction. In particular, we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations, allows improving the state-of-the-art effectiveness on several datasets in different contexts (i.e., news, biomedical, and real estate data) and for different languages (English and Dutch). Model Joint learning as head selectionThe baseline model, described in detail in , is illustrated in . It aims to detect (i) the type and the boundaries of the entities and (ii) the relations between them. The input is a sequence of tokens (i.e., sentence) w = w 1 , ..., w n . We use character level embeddings to implicitly capture morphological features (e.g., prefixes and suffixes), representing each character by a vector (embedding). The character embeddings are fed to a bidirectional LSTM (BiLSTM) to obtain the character-based representation of the word. We also use pre-trained word embeddings. Figure 1: Our model for joint entity and relation extraction with adversarial training (AT) comprises (i) a word and character embedding layer, (ii) a BiLSTM layer, (iii) a CRF layer and (iv) a relation extraction layer. In AT, we compute the worst-case perturbations \u03b7 of the input embeddings.Word and character embeddings are concatenated to form the final token representation, which is then fed to a BiLSTM layer to extract sequential information.For the NER task, we adopt the BIO (Beginning, Inside, Outside) encoding scheme. In , the B-PER tag is assigned to the beginning token of a 'person' (PER) entity. For the prediction of the entity tags, we use: (i) a softmax approach for the entity classification (EC) task (assuming entity boundaries given) or (ii) a CRF approach where we identify both the type and the boundaries for each entity. During decoding, in the softmax setting, we greedily detect the entity types of the tokens (i.e., independent prediction). Although independent distribution of types is reasonable for EC tasks, this is not the case when there are strong correlations between neighboring tags. For instance, the BIO encoding scheme imposes several constraints in the NER task (e.g., the B-PER and I-LOC tags cannot be sequential). Motivated by this intuition, we use a linear-chain CRF for the NER task . For decoding, in the CRF setting, we use the Viterbi algorithm. During training, for both EC (softmax) and NER tasks (CRF), we minimize the cross-entropy loss L NER . The entity tags are later fed into the relation extraction layer as label embeddings (see ), assuming that knowledge of the entity types is beneficial in predicting the relations between the involved entities.We model the relation extraction task as a multi-label head selection problem . In our model, each word w i can be involved in multiple relations with other words. For instance, in the example illustrated in , \"Smith\" could be involved not only in a Lives in relation with the token \"California\" (head) but also in other relations simultaneously (e.g., Works for, Born In with some corresponding tokens). The goal of the task is to predict for each word w i , a vector of heads\u0177 i and the vector of corresponding relationsr i . We compute the score s(w j , w i , r k ) of word w j to be the head of w i given a relation label r k using a single layer neural network. The corresponding probability is defined as:where \u03c3(.) is the sigmoid function. During training, we minimize the cross-entropy loss L rel as:where m is the number of associated heads (and thus relations) per word w i . During decoding, the most probable heads and relations are selected using threshold-based prediction. The final objective for the joint task is computed as L JOINT (w; \u03b8) = L NER + L rel where \u03b8 is a set of parameters. In the case of multi-token entities, only the last token of the entity can serve as head of another token, to eliminate redundant relations. If an entity is not involved in any relation, we predict the auxiliary \"N\" relation label and the token itself as head. Adversarial training (AT)We exploit the idea of AT  as a regularization method to make our model robust to input perturbations. Specifically, we generate examples which are variations of the original ones by adding some noise at the level of the concatenated word representation . This is similar to the concept introduced by  to improve the robustness of image recognition classifiers. We generate an adversarial example by adding the worst-case perturbation \u03b7 adv to the original embedding w that maximizes the loss function:where\u03b8 is a copy of the current model parameters. Since Eq. (2) is intractable in neural networks, we use the approximation proposed in  defined as: \u03b7 adv = g/ g , with g = \u2207 w L JOINT (w;\u03b8), where is a small bounded norm treated as a hyperparameter. Similar to , we set to be \u03b1 \u221a D (where D is the dimension of the embeddings). We train on the mixture of original and adversarial examples, so the final loss is computed as: L JOINT (w;\u03b8) + L JOINT (w + \u03b7 adv ;\u03b8). Experimental setupWe evaluate our models on four datasets, using the code as available from our github codebase. 1 Specifically, we follow the 5-fold crossvalidation defined by  for the ACE04  dataset. For the CoNLL04  EC task (assuming boundaries are given), we use the same splits as in ; . We also evaluate our models on the NER task similar to  in the same dataset using 10-fold cross validation. For the Dutch Real Estate Classifieds, DREC (Bekoulis et al., 2017) dataset, we use train-test splits as in . For the Adverse Drug Events, ADE , we perform 10-fold cross-validation similar to . To obtain comparable results that are not affected by the input embeddings, we use the embeddings of the previous works. We employ early stopping in all of the experiments. We use the Adam optimizer  and we fix the hyperparameters (i.e., \u03b1, dropout values, best epoch, learning rate) on the validation sets. The scaling parameter \u03b1 is selected from {5e\u22122, 1e\u22122, 1e\u22123, 1e\u22124}. Larger values of \u03b1 (i.e., larger perturbations) lead to consistent performance decrease in our early experiments. This can be explained from the fact that adding more noise can change the content of the sentence as also reported by .We use three types of evaluation, namely: (i) S(trict): we score an entity as correct if both the entity boundaries and the entity type are correct (ACE04, ADE, CoNLL04, DREC), (ii) B(oundaries): we score an entity as correct if only the entity boundaries are correct while the entity type is not taken into account (DREC) and (iii) R(elaxed): a multi-token entity is considered correct if at least one correct type is assigned to the tokens comprising the entity, assuming that the  : Comparison of our method with the stateof-the-art in terms of F 1 score. The proposed models are: (i) baseline, (ii) baseline EC (predicts only entity classes) and (iii) baseline (EC) + AT (regularized by AT). The and symbols indicate whether the models rely on external NLP tools. We include different evaluation types (S, R and B).boundaries are known (CoNLL04), to compare to previous works. In all cases, a relation is considered as correct when both the relation type and the argument entities are correct.  shows our experimental results. The name of the dataset is presented in the first column while the models are listed in the second column. The proposed models are the following: (i) baseline: the baseline model shown in  with the CRF layer and the sigmoid loss, (ii) baseline EC: the proposed model with the softmax layer for EC, (iii) baseline (EC) + AT: the baseline regularized using AT. The final three columns present the F 1 results for the two subtasks and their average performance. Bold values indicate the best results among models that use only automatically extracted features. For ACE04, the baseline outperforms Katiyar and Cardie (2017) by \u223c2% in both tasks. This improvement can be explained by the use of: (i) multi-label head selection, (ii) CRF-layer and (iii) character level embeddings. Compared to , who rely on NLP tools, the baseline performs within a reasonable margin (less than 1%) on the joint task. On the other hand,  use the same model for the ADE biomedical dataset, where we report a 2.5% overall improvement. This indicates that NLP tools are not always accurate for various contexts. For the CoNLL04 dataset, we use two evaluation settings. We use the relaxed evaluation similar to ; Adel and Sch\u00fctze (2017) on the EC task. The baseline model outperforms the state-of-the-art models that do not rely on manually extracted features (>4% improvement for both tasks), since we directly model the whole sentence, instead of just considering pairs of entities. Moreover, compared to the model of  that relies on complex features, the baseline model performs within a margin of 1% in terms of overall F 1 score. We also report NER results on the same dataset and improve overall F 1 score with \u223c1% compared to , indicating that our automatically extracted features are more informative than the hand-crafted ones. These automatically extracted features exhibit their performance improvement mainly due to the shared LSTM layer that learns to automatically generate feature representations of entities and their corresponding relations within a single model. For the DREC dataset, we use two evaluation methods. In the boundaries evaluation, the baseline has an improvement of \u223c3% on both tasks compared to , whose quadratic scoring layer complicates NER.  and  show the effectiveness of the adversarial training on top of the baseline model. In all of the experiments, AT improves the predictive performance of the baseline model in the joint setting. Moreover, as seen in , the performance of the models using AT is closer to maximum even from the early training epochs. Specifically, for ACE04, there is an improvement in both tasks as well as in the overall F 1 performance (0.4%). For CoNLL04, we note an improvement in the overall F 1 of 0.4% for the EC and 0.8% for the NER tasks, respectively. For the DREC dataset, in both settings, there is an overall improvement of \u223c1%.  shows that from the first epochs, the model obtains its maximum performance on the DREC validation set. Finally, for ADE, our AT model beats the baseline F 1 by 0.7%. ResultsOur results demonstrate that AT outperforms the neural baseline model consistently, considering our experiments across multiple and more diverse datasets than typical related works. The im- Number of epochs F1 PerformanceAdversarial Baseline : F 1 performance of the baseline and the AT models on the validation sets from 10-30 epochs onwards depending on the dataset. The smoothed lines (obtained by LOWESS smoothing) model the trends and the 95% confidence intervals.provement of AT over our baseline (depending on the dataset) ranges from \u223c0.4% to \u223c0.9% in terms of overall F 1 score. This seemingly small performance increase is mainly due to the limited performance benefit for the NER component, which is in accordance with the recent advances in NER using neural networks that report similarly small gains (e.g., the performance improvement in  and  on the CoNLL-2003 test set is 0.01% and 0.17% F 1 percentage points, while in the work of , a 0.07% F 1 improvement on CoNLL-2000 using AT for NER is reported). However, the relation extraction performance increases by \u223c1% F 1 scoring points, except for the ACE04 dataset. Further, as seen in , the improvement for CoNLL04 is particularly small on the evaluation set. This may indicate a correlation between the dataset size and the benefit of adversarial training in the context of joint models, but this needs further investigation in future work. ConclusionWe proposed to use adversarial training (AT) for the joint task of entity recognition and relation extraction. The contribution of this study is twofold: (i) investigation of the consistent effectiveness of AT as a regularization method over a multi-context baseline joint model, with (ii) a large scale experimental evaluation. Experiments show that AT improves the results for each task separately, as well as the overall performance of the baseline joint model, while reaching high performance already during the first epochs of the training procedure.\n###\n"}
{"summary": "has research problem: VUV spectroscopy in low pressure plasmas/VUV spectroscopy in low pressure plasmas\nresearch_target: VUV measurement  in MW-plasmas in support of simulation of planetary atmospheric photochemestry.\nvuvspec_irradiance: For Neon[73.6 nm]  vs [Pressure & Power] 0.1 - 40.\nUnit_vuvspec_irradiance: 10$$^{14}$$  cm$$^{-2}$$ s$$^{-1}$$/10$$^{14}$$  cm$$^{-2}$$ s$$^{-1}$$\nPressure: 40-170/40-170\nFeed_gases: Ne/Ne\nUnit_pressure: $$Pascal$$/$$Pascal$$\nUnit_Ne: /10$$^{11}$$ cm$$^{-3}$$\nInput_power: 40-100/40-100\nPlasma_discharge: MW/MW\nUnit_input_power: $$Watt$$\nvuvspec_irradiance [Al I, Al II, C$$_x$$F$$_y$$, Ne,I, Xe I, He I]: For Neon[73.6 nm]  vs [Pressure & Power] 0.1 - 40.;", "text": "#Properties\nhas research problem, research_target, vuvspec_irradiance, Unit_vuvspec_irradiance, Pressure, Feed_gases, Unit_pressure, Unit_Ne, Input_power, Plasma_discharge, Unit_input_power, vuvspec_irradiance [Al I, Al II, C$$_x$$F$$_y$$, Ne,I, Xe I, He I]\n#Text\nMicrowave plasma discharges working at low pressure are nowadays a welldeveloped technique mainly used to provide radiations at different wavelengths. The aim of this work is to show that those discharges are an efficient windowless VUV photon source for planetary atmospheric photochemistry experiments. To do this, we use a surfatron-type discharge with a neon gas flow in the mbar pressure range coupled to a photochemical reactor. Working in the VUV range allows to focus on nitrogen-dominated atmospheres (\u03bb<100nm). The experimental setup makes sure that no other energy sources (electrons, metastable atoms) than the VUV photons interact with the reactive medium. Neon owns two resonance lines at 73.6 and 74.3 nm which behave differently regarding the pressure or power conditions. In parallel, the VUV photon flux emitted at 73.6 nm has been experimentally estimated in different conditions of pressure and power and varies in a large range between 2\u00d710 13 ph.s-1 .cm-2 and 4\u00d710 14 ph.s-1 .cm-2 which is comparable to a VUV synchrotron photon flux. Our first case study is the atmosphere of Titan and its N2-CH4 atmosphere. With this VUV source, the production of HCN and C2N2, two major Titan compounds, is detected, ensuring the suitability of the source for atmospheric photochemistry experiments. PACS. 50 Physics of gases, plasmas, and electric discharges 52.70.Kz Optical (ultraviolet, visible, infrared) measurements 52.80.Pi High-frequency and RF discharges 52.80.Yr Discharges for spectral sources (including inductively coupled plasma) Experimental setup The VUV source: a microwave-plasma dischargeThe plasma discharge takes place in a 40-cm length quartz tube with an internal diameter of 8 mm and an external diameter of 10 mm, surrounded by a surfatron resonance cavity. The microwave power delivered by a Sairem generator goes up to 200 W. The surfatron can be moved along the quartz tube in order to settle the end of the discharge regarding the entrance of the photochemical reactor. A compressed air circulation avoids any over heat of the system. Surface temperature measurement showed that it does not go above 60\u00b0C.The gas flow is regulated with a 0-10 sccm (cm 3 .min -1 STP) range MKS mass flow controller. Moreover, the pressure is measured with a Pfeiffer capacitor gauge mounted upstream the surfatron device while the main part of the plasma column is created downstream the field applicator.Three noble gases, with intense resonance lines, can be used in order to work in the VUV range. Argon emits at 104.8 nm (no dissociation of N2), Neon at 73.59 and 74.37 nm and He at 58.4 and 53.7 nm (ionization of both molecules) . Here, we use Neon: at this energy, the dissociation and ionization of N2 are both possible (figure 1) in parallel to the ionization of CH4 (threshold at 98.52 nm). Characterization of the source with a VUV monochromatorThe characterization of the VUV source is performed by coupling it directly to a VUV 1-m focal-length McPherson NOVA 225 spectrometer, fitted with a 1200 gr/mm concave grating blazed at 45 nm and sensitive from 30 nm to 300 nm. The efficiency of the grating strongly varies with the wavelength and is around 6% at 75 nm (characterized by McPherson). In order to avoid the saturation of the acquisition system but still have a good resolution, the width of the slits is set up at 75 \u00b5m for a height of 4mm. The acquisition system consists of an Optodiode AXUV100 photodiode coupled to an amplifier. The output signal gives 1V for 1nA and saturates at 10V. Moreover, according to the data sheet, at 75 nm, the photodiode responsivity is about 0.22 A.W -1 .The monochromator is pumped with a turbomolecular pump (Agilent TV 301) and the internal pressure is measured by a Penning gauge (Oerlikon) calibrated for air. The real pressure is obtained using a neon/air correction factor. The turbomolecular pump ensures an ultimate vacuum of 10 -8 mbar inside the monochromator; while the VUV source is also pumped by this system, through the entrance slit. At the maximum gas flow injected into the VUV source, the monochromator pressure is about 10 -3 mbar.The luminous power collected by the photodiode is related to the number of photons per second:where E is the energy and Np the number of collected photons.As neon from the source is pumped in the monochromator, optical absorption happens. Then the transmission inside the monochromator at a specific pressure is given by the Beer-Lambert law:where d stands for the length of the optical path (here d=2 m); \u03c3 is the absorption cross section (for neon and around 75 nm, \u03c3=9\u00d710 -17 cm\u00b2 (17)); and finally [Ne] is the neon density linked to the pressure via the perfect gas law. In the few cm between the end of the discharge and the monochromator entrance slit, the absorption is negligible.After weighting the number of photons per second with all those factors (responses of both the photodiode and the grating, plus the neon absorption), we divide by the surface of the exposed slits in order to obtain the photon flux (ph.s -1 .cm -\u00b2). The atmospheric photoreactor (\"APSIS\")The table-top VUV source is to be coupled with a photochemical reactor named APSIS (Atmospheric Photochemistry SImulated by Synchrotron) in order to carry research on planetary upper atmospheres and their interaction with the VUV solar light. The APSIS reactor is described in details in . Briefly, it is a stainless steel chamber of dimensions 500 mm x 114 mm x 92 mm where the reactive mixture is introduced via a gas inlet (figure 2).The gas mixture is chosen in order to simulate Titan's atmospheric composition. In the present work, a 95-5% N2-CH4 mixture is injected up to 10 sccm with a MKS gas flow controller. Before each experiment, the reactor is pumped by a turbo-molecular pump down to 10 -7 mbar. During the photochemistry experiments, a rotary vane pump ensures a stable pressure on the order of 1 mbar. . Scheme of the photochemical reactor coupled window-less with the VUV source. In-situ mass spectrometry on APSISWe use mass spectrometry to monitor the neutral molecules in the experiment, with a HIDEN HPR-20 QIC mass spectrometer. The gaseous products are taken at the closest spot from the VUV source. In order maintain a low pressure inside the mass spectrometer at 10 -7 mbar, a 1m-capillary tube with an internal diameter of 1/16 e inch is used. The Multiple Ion Detection (MID) mode on the mass spectrometer is chosen, as it follows selected mass signal in function of time and shows the evolution of the intensities before and during the irradiation process. UV optical emission spectroscopy on the coupled system \"VUV source-APSIS\"In order to ensure that no neon plasma enters the APSIS reactor, and that the dissociation of nitrogen and methane only occurs from the VUV photochemistry, optical emission spectroscopy of the neon-N2-CH4 system is performed in the UV range. A Hamamatsu spectrometer TG-UV C9404CAH is positioned in front of the VUV source at the opposite side of the reactor (figure 2). The emitted spectra in the UV range from 200 nm to 450 nm is recorded through a quartz window. Results Validation of pure photochemistryIn order to work with a clean neon plasma, the discharge remains on for at least 20 min before starting any measurement. A VUV spectrum of the clean discharge is presented on figure 3 (black curve). The key parameter in our photochemistry experimental platform is the distance between the end of the discharge and the entrance of the reactor. Indeed, if the distance is too short, there is a risk that the discharge would continue inside the reactor and create a N2-CH4 plasma (electron-driven chemistry) or that the metastable neon atoms will enter the reactor. But if the distance is too long, a photon loss is to be considered. Preventing electron-driven chemistry.Thanks to optical emission spectroscopy, it is possible to monitor the N2 bands of the Second Positive System (SPS), which are recognizable because degraded to shorter wavelengths. The SPS is emitted from the 2 ( 3 ) state which cannot be populated neither by the 73.6 nm nor the 74.3nm-VUV line. This emission is then due to the energetic electrons of the plasma in the reactor: seeing those SPS bands means that the plasma discharge enters the photo-reactor and interacts with the N2-CH4 mixture.First, in order to inject the maximum amount of VUV photons, the surfatron has been placed as near as possible from the entrance of the APSIS reactor. However, in this configuration, N2 bands of the SPS appear on the spectrum , red curve).It is then relevant to balance the distance from the end of the discharge and the entrance of the reactor in order to have enough photons entering the reactive medium without exciting the nitrogen-methane mixture.  . UV spectrum with neon in the discharge tube (blue) and neon facing APSIS filled with the N2-CH4 mixture (orange). The fact that the two spectra are exactly the same shows that N2 is not excited by the discharge. This is why the surfatron is pushed back from the entrance of the reactor until the end of the discharge is 5cm away from it.  compares UV spectra of the only neon source and the APSIS reactor filled with N2-CH4 and irradiated by the surfatron source in its new position. N2 emission bands are no more observed, even for the maximum (10 sccm) gas flow of N2-CH4. From that we can conclude that N2 is not excited by electrons, and that there is no electron-driven chemistry in the reactor with this position of the surfatron.1.1.2 Preventing neon-metastable-atom-driven chemistry. Another possible source of energy could also be the neon metastable states 3 P 0 2-0 at 16.61 and 16.71 eV. Those metastable atoms are a potential source of energy for ionizing molecular nitrogen as its ionization threshold is at 15.58 eV; this is why it is crucial to determine if they can reach the reactor. As metastable atoms are embattled in the neon flow, their diffusion characteristic time \u03c4D has to be compared with their travel time \u03c4t between the end of the discharge and the reactor entrance.In our experimental conditions, the depopulation is mainly done by radial diffusion to the discharge which means that the metastable atom density depends on the Bessel function of the first kind J0 :The inverse of the diffusion characteristic time is then 1 D = 2 where \u039b, the diffusion length, equals2.405 where R is the radius of the tube. The neon metastable diffusion coefficient is D=170 cm\u00b2.s -1 at 1 Torr and 300K . In our working conditions, (P=1 mbar, T=300K, R=4mm), the diffusion time of the neon metastable atoms is =1.6\u00d710 -4 s.For a neon gas flow Q=4sccm, the velocity is V=1.3\u00d710\u00b2 cm.s -1 and the travel time on this 5cm-length is \u03c4t =4\u00d710 -\u00b2 s. As \u03c4t >> , the neon metastable atoms are destroyed before arriving to the reactive zone.We are now sure that the only energy input from our lamp are the VUV photons, making it fully compatible to the kind of experiments we intend to perform. Discharge length.In order to increase the VUV photon flux, one can increase the micro-wave power deliver to the discharge, but this will modify the discharge length, which could then enter the reactor. This is why we measured the length of the discharge downstream the surfatron-device as a function of pressure in different conditions of power ( ). The length increases with the pressure. Above 1 mbar, it depends only on the micro-wave power with a maximum of 18 cm at a pressure of 1.7 mbar and a power of 80W.Then, for further use for photochemistry, we know where the surfatron must be positioned in order to have the maximum of VUV flux without any induced plasma reactions whatever the pressure and/or the power are. Characterization of the VUV sourceIn order to fully characterize the source in the VUV range, the intensities of the two neon resonant lines  are recorded in different conditions of pressure and power for the discharge.Typical spectra recorded for the two resonance lines of neon are presented on . We notice that the two lines present different behaviours regarding the pressure, this is why we will study them separately.In order to calculate the VUV flux emitted by the surfatron source, we need the spectral responses of both the grating and the photodetector, without forgetting the absorption of the neon present inside the monochromator as described in section 1.2.  The calculations have been performed first on the 73.6 nm line for different gas pressures and microwave powers ). The order of magnitude of the source photon flux is 10 14 ph.s -1 .cm -\u00b2, with a maximum of 4\u00d710 14 ph.s -1 .cm -\u00b2 (1.7 mbar and 100W) and a minimum of 2\u00d710  ph.s -1 .cm -\u00b2 (0.4 mbar and 40W).   On , the variations of the photon flux versus the power delivered into the discharge are presented, for different values of pressures. Here, the growth looks linear.These two trends versus the pressure and the power can be explained by a collisional radiative model. The intensity of the discharge is directly proportional to the electron density ne, the neon concentration in its fundamental state , that is the inverse of the lifetime of the radiative level and an excitation coefficient k function of the electronic temperature:In our pressure range, we assume that the electron energy remains practically constant which means that the ionization degree is supposed to stay constant as well. For a given power, arising the gas pressure would then directly increase the neon density [Ne] but also the electron density ne which explains the quadratic law of the 73.6 nm-line intensity. Moreover, at a fixed pressure, increasing the delivered power would here have a linear impact on the electron density ne, while the electronic temperature Te and the neon concentration remain constant.We can now focus on the evolution of the 74.3nm-line.When the 74.3nm-photon flux is plotted versus the pressure for an arbitrary power of 70W, it looks like it does not follow the same quadratic trend as the 73.6 nm one ). In fact, its intensity varies very linearly with the pressure which means that another phenomenon counterbalances the expected quadratic rise of the photon flux. In addition to the depopulation of this energy level by radiative photon emission, one lead is to involve a quenching effect. The lifetime of the radiative level 3 P1, that emits the 74.3 nm line, was measured by  in parallel to the two metastable states 3 P2 and 3 P0 ( ). These lifetimes are quite similar (few ms). As the 74.3nm-line is resonant, its level 3 P1 is re-populated and its density is on the same order of magnitude than the metastable states as  has measured in a neon microwave discharge.The quenching coefficient of the 3 P1 radiative level (at 16.67eV) due to the transfer towards the 3 P2 metastable state, is ku= 4.2\u00d710 -14 cm 3 s -1 . The efficiency of this quenching is related to the small energy gap of 5.17\u00d710 -2 eV between those two states. After collision, this energy is shared between the two atoms and corresponds to a temperature of approximately 300K, which is more or less the gas temperature in the discharge.Equation gives the intensity of the 74.3nm-line taking into account this potential quenching effect. In this equation, corresponds to the inverse of the 3 P1 level lifetime. Then, the quenching seems to be dominant here, this is why the 74.3nm-line intensity increases linearly with the electron density and thus the pressure. . Compared photon flux evolution, in function of the pressure, of the two neon emission lines at 73.6 and 74.3 nm for an arbitrary given power of 70W and their quadratic and linear fits (respectively).For the 73.6 nm line, which corresponds to the 1 P1 level, the quenching is less efficient, because the transfer to the 3 P0 metastable state presents a higher energy gap of 0.133 eV i.e a temperature of 770 K for each atom, too hot for our experimental conditions. This level is then mainly depopulated by radiative transfer.To conclude, the pressure at which we are working will determine which line(s) will contribute to our photochemistry experiment. For high pressures (P>0.7 mbar), the radiative emission from the 73.6-nm line is clearly dominant compared to the steady contribution of the 74.33-nm one. However, when going low in pressure the contribution of both lines have to be taken into account. First photochemistry experimentWhen the source is coupled to the APSIS reactor, an N2-CH4 mixture (95% N2, 5% CH4) is injected. First, 4 sccm of neon are injected inside the discharge tube, which corresponds to a discharge pressure P=1.3 mbar. In a second time, in order not to contaminate the discharge, we inject 2 sccm of N2-CH4 in APSIS for a pressure of 0.9 mbar. This neon flow permits a relevant photon flux for the VUV source but also prevent the reactive N2-CH4 mixture from entering the discharge tube, this is why we chose it for our first photochemistry experiment. The power delivered to the discharge is Pw=80W which corresponds to a photon flux of 1.9\u00d710 14 ph.s -1 .cm\u00b2 or 0.95\u00d710 14 ph.s -1 for the tube section of 0.5cm\u00b2.Two effects have to be monitored by mass spectrometry: first, if the reactive signals are getting lower through the experiment, as it would mean that there is an effective consumption due to the photon interaction; and, in a second time, the signals of some hydrocarbon and/or N-bearing species (Titan-like molecules) are followed in the expectation of seeing a production. Here, we focus on the mass m/z=15 in order to represent the CH4 reactive, through its CH3 + fragment, because the real CH4 mass m/z=16 can overlap multiple species fragments. In addition, we look at some Titan-like molecules: m/z=27 for . Gotrian diagram of the neon energy levels taken from  HCN and m/z=52 for C2N2. On , the red dotted line marks the moment when the VUV source is turned ON. The signals of the selected products immediately start to increase, while the m/z=15 signal decreases. After approximately 20 minutes, a quasi-stationary is reached for the products.The presence of those molecules highlights the incorporation of both C and N atoms coming from the expected dissociation of N2 and CH4. Methane has been identified as a key reactant to initiate efficient organic growth, in contrast to the other major form of carbon commonly found in planetary atmospheres, carbon dioxide CO2. However, methane chemistry leads to the only production of complex hydrocarbon molecules, with no heavy heteroatoms of interest for prebiotic chemistry such as nitrogen. . N2 photolysis leads to reactive forms of nitrogen, atomic or ionized, which react with hydrocarbons to produce nitrogen containing organic compounds. These nitrogen containing species have a strong interest for prebiotic chemistry . The most abundant gas-phase nitrogen-bearing products in Titan atmosphere are nitriles molecules (R-CN) as the HCN and C2N2 molecules that we found in our experiment. The goal of the photochemistry experiment is then achieved, which ends to validate the suitability of the VUV source. ConclusionSo, in conclusion, VUV plasma source based on surfatron-driven microwave discharges have proven to be an efficient and reliable tool for VUV atmospheric photochemistry experiments working at low pressure. For the ones using neon, it has been demonstrated here that the only energy source injected inside the reactive medium is the VUV photons from its two resonant lines (73.6 and 74.3 nm). Other sources are indeed possible (neon metastable atoms and electrons from the discharge) but the system has been adapted so that they will not disturb the desired pure photochemistry. In this case, the key parameter appears to be the distance between the end of the discharge and the entrance of the photochemical reactor. Also, always for neon, the behaviours of the two resonant wavelengths have been investigated in order to characterize their contribution in several conditions of pressure and power. The 73.6-nm line is largely dominant in our working conditions, but its intensity decreases with the pressure (quadratic law) and the power (linear law). This behaviour is different from the 74.3-nm line which remains quite stable whatever the pressure or the power are, possibly because of a quenching effect. It results that at low pressure (<0.7 mbar) the 74.3-nm one becomes dominant. To further investigate these relative intensities, a complete modelling of the discharge in our conditions taking into account the electron energy distribution function would be required. In fact, according to  and their model with a 10Torr-DC discharge, the 74.3-nm line has a higher intensity than the 73.6-nm one, which is not in agreement with our experimental results.Moreover, at 73.6nm, the order of magnitude for the photon flux is 10 14 ph.s -1 .cm -\u00b2, which seems largely comparable to a VUV synchrotron beamline at specific wavelength. The working conditions of the surfatron-based VUV source are quite flexible (regarding the pressures and power), allowing the photon flux to vary from 2\u00d710 13 ph.s -1 .cm -\u00b2 up to 4\u00d710 14 ph.s -1 .cm -\u00b2 and offer measurable photochemistry results. Our new setup, without any window separating the source and the photoreactor, can change the way atmospheric photochemistry experiments have been performed so far, especially for the ones focusing of nitrogen-dominated atmospheres such as the Earth, the Early Earth, Titan or Pluto.More specifically, our setup showed its ability to simulate the formation of nitriles in Titan's atmosphere. Those are not correctly predicted by the current photochemical models . Even the simplest and most abundant nitrile, HCN, is found to be predicted by neutral photochemistry with concentrations three to ten times larger than measured by the Cassini space mission . Such disagreements between model predictions and observations reveal a poor knowledge of the chemistry involving nitrile in general and HCN in particular. Without the constraint of adapting the system to high pressure gradients between the source and the reactor or the low availability of synchrotron beamlines, our experiment will provide the opportunity to explore the chemistry of nitriles and improve our knowledge in this area.\n###\n"}
{"summary": "description: Conceptional background of the state-of-the-art comparison feature/Multilingual word embeddings created by Facebook/Benchmarks the comparison system against a naive baseline in terms of time/Similarity and Comparision framework, to compare research contributions and find simialiatiy among them/Elastic search indexing engine/Where the data is stored and processed, It contains the business logic and the Rest API code/The user interface, which contains the main three functionalities of curation, contribution, and Exploration./Programming language/Find similar research contributions inside ORKG and suggest them to the user/Term frequency, inverse document frequency technique\nutilizes: FastText embedding/ES/TF/iDF\nevaluation: DILS2018 technical evaluation\nHas formula: $$\\varphi_{i,j} =(\\Phi_{i,j})_{\\substack{c_i \\in \\mathcal{C}\\\\ p_j \\in sim(p) }}, \\Phi_{i,j} = \\begin{cases} 1 & \\text{ if } p_{j} \\in c_i \\\\  0 & \\text{ otherwise }   \\end{cases}$$\nDemo video: https://www.youtube.com/watch?v=mbe-cVyW_us\nhas research problem: Structured descriptions of research contributions/Scholarly communications representation/User interaction/Semantic representation of scholarly communication/Structured descriptions of research contributions/Similarity measures\nurl: https://fasttext.cc//https://gitlab.com/TIBHannover/orkg/orkg-similarity/https://www.elastic.co//https://gitlab.com/TIBHannover/orkg/orkg-backend/https://neo4j.com/https://gitlab.com/TIBHannover/orkg/orkg-frontend/https://reactjs.org/\nyields: DILS2018 technical evaluation of ORKG comparison service\nstructure: Data Structure Definition ESUC\ncomponent: Component Specification 8 Contributions/Component Specification 6 Contributions/Component Specification 2 Contributions/Component Specification 5 Contributions/Component Specification 7 Contributions/Component Specification 3 Contributions/Component Specification 4 Contributions/Component Specification System\nmeasure: 8 Contributions/6 Contributions/2 Contributions/5 Contributions/7 Contributions/3 Contributions/4 Contributions\norder: 8/6/2/5/7/3/4/1\ndimension: System\nRelated video: https://av.tib.eu/media/42465/https://av.tib.eu/media/16120\nimplementation: Simcomp/Backend/Frontend\nSemantic representation: ORKG\nSupports RDF: True\nArchitecture: Classical layered architecture/Classical layered architecture\nuses library: numpy/pandas/React JS\nprogramming language: Python/Kotlin/Java/Javascript\nuses framework: Flask/Spring Boot/Spring Security\nsame as: https://en.wikipedia.org/wiki/Python_(programming_language)/https://www.wikidata.org/entity/Q28865\nAcquisition: Natural Language Processing (NLP)/Crowdsourcing\nSupports research data: Partially\nMetadata: T\nScope: Summary\nHigh level claims: T\nProspective/retrospective: Retrospective/Prospective\nKnowledge representation: Metadata/RDF\nExamples: Research grants/Affiliations/Names\nhas part: Restful API layer/Persistence layer/RDF interoperability layer/Domain model layer/Versioning and provenance layer/Querying layer/Services layer/Restful API layer/Persistence layer/RDF interoperability layer/Domain model layer/Versioning and provenance layer/Querying layer/Services layer\nFigure: https://doi.org/10.6084/m9.figshare.11395362.v1/https://doi.org/10.6084/m9.figshare.11395362.v1\nUses graph store: Neo4J/Virtuoso triple store\nmodel: Linked Property Graph\nDeveloper: Oracle Corporation", "text": "#Properties\ndescription, utilizes, evaluation, Has formula, Demo video, has research problem, url, yields, structure, component, measure, order, dimension, Related video, implementation, Semantic representation, Supports RDF, Architecture, uses library, programming language, uses framework, same as, Acquisition, Supports research data, Metadata, Scope, High level claims, Prospective/retrospective, Knowledge representation, Examples, has part, Figure, Uses graph store, model, Developer\n#Text\nDespite improved digital access to scholarly knowledge in recent decades, scholarly communication remains exclusively documentbased. In this form, scholarly knowledge is hard to process automatically. In this paper, we present the first steps towards a knowledge graph based infrastructure that acquires scholarly knowledge in machine actionable form thus enabling new possibilities for scholarly knowledge curation, publication and processing. The primary contribution is to present, evaluate and discuss multi-modal scholarly knowledge acquisition, combining crowdsourced and automated techniques. We present the results of the first user evaluation of the infrastructure with the participants of a recent international conference. Results suggest that users were intrigued by the novelty of the proposed infrastructure and by the possibilities for innovative scholarly knowledge processing it could enable. PROBLEM STATEMENTWe illustrate the problem with an example from life sciences. When searching for publications on the popular Genome editing method CRISPR/Cas in scholarly search engines we obtain a vast amount of search results. Google Scholar, for example, currently returns more than 50 000 results, when searching for the search string 'CRISPR Cas'. Furthermore, research questions often require complex queries relating numerous concepts. Examples for CRISPR/Cas include: How good is CRISPR/Cas w.r.t. precision, safety, cost? How is genome editing applied to insects? Who has applied CRISPR/Cas to butterflies? Even if we include the word 'butterfly' to the search query we still obtain more than 600 results, many of which are not relevant. Furthermore, relevant results might not be included (e.g., due to the fact that the technical term for butterfly is Lepidoptera, which combined with 'CRISPR Cas' returns over 1 000 results). Finally, virtually nothing about the returned scholarly knowledge in the returned documents is machine actionable: human experts are required to further process the results.We argue that keyword-based information retrieval and documentbased results no longer adequately meet the requirements of research communities in modern scholarly knowledge infrastructures  and processing of scholarly knowledge in the digital age. Furthermore, we suggest that automated techniques to identify concepts, relations and instances in text , despite decades of research, do not and unlikely will reach a sufficiently high granularity and accuracy for useful applications. Automated techniques applied on published legacy documents need to be complemented with techniques that acquire scholarly knowledge in machine actionable form as knowledge is generated along the research lifecycle. As Mons suggested , we may fundamentally question \"Why bury [information in plain text] first and then mine it again. \"This article tackles the following research questions:\u2022 Are authors willing to contribute structured descriptions of the key research contribution(s) published in their articles using a fit-for-purpose infrastructure, and what is the user acceptance of the infrastructure?1 \u2022 Can the infrastructure effectively integrate crowdsourcing and automated techniques for multi-modal scholarly knowledge acquisition? OPEN RESEARCH KNOWLEDGE GRAPHWe propose to leverage knowledge graphs to represent scholarly knowledge communicated in the literature. We call this knowledge graph the Open Research Knowledge Graph 2 (ORKG). Crucially, the proposed knowledge graph does not merely contain (bibliographic) metadata (e.g., about articles, authors, institutions) but semantic (i.e., machine actionable) descriptions of scholarly knowledge. ArchitectureThe infrastructure design follows a classical layered architecture. As depicted in , a persistence layer abstracts data storage implemented by labeled property graph (LPG), triple store, and relational database storage technology, each serving specific purposes. Versioning and provenance handles tracking changes to stored data.The domain model specifies ResearchContribution, the core information object of the ORKG. A ResearchContribution relates the ResearchProblem addressed by the contribution with the ResearchMethod and (at least one) ResearchResult. Currently, we do not further constrain the description of these resources. Users can adopt arbitrary third-party vocabularies to describe problems, methods, and results. For instance, users could use the Ontology for Biomedical Investigations as a vocabulary to describe statistical hypothesis tests.Research contributions are represented by means of a graph data model. Similarly to the Research Description Framework  (RDF), the data model is thus centered around the concept of a statement, a triple consisting of two nodes (resources) connected by a directed edge. In contrast to RDF, the data model allows to uniquely identify instances of edges and to qualify these instances (i.e., annotate edges and statements). As metadata of statements,   provenance information is a concrete and relevant application of such annotation.RDF import and export enables data synchronization between LPG and triple store, which enables SPARQL and reasoning. Querying handles the requests by services for reading, updating, and creating content in databases. The following layer is for modules that implement infrastructure features such as authentication or comparison and similarity computation. The REST API acts as the connector between features and services for scholarly knowledge contribution, curation and exploration.ORKG users in author, researcher, reviewer or curator roles interact differently with its services. Exploration services such as State-of-the-Art comparisons are useful in particular for researchers and reviewers. Contribution services are primarily for authors who intend to contribute content. Curation services are designed for domain specialists more broadly to include for instance subject librarians who support quality control, enrichment and other content organization activities. FeaturesThe ORKG services are underpinned by numerous features that, individually or in combination, enable services. We present the most important current features next.State-of-the-Art (SOTA) comparison. SOTA comparison extracts similar information shared by user selected research contributions and presents comparisons in tabular form. Such comparisons rely on extracting the set of semantically similar predicates among compared contributions.We use FastText  word embeddings to generate a similarity matrix \u03b3with the cosine similarity of vector embeddings for predicate pairs (p i , p j ) \u2208 R, whereby R is the set of all research contributions.Furthermore, we create a mask matrix \u03a6 that selects predicates of contributions c i \u2208 C, whereby C is the set of research contributions to be compared. Formally,Next, for each selected predicate p we create the matrix \u03c6 that slices \u03a6 to include only similar predicates. Formally,where sim(p) is the set of predicates with similarity values \u03b3 [p] \u2265 T = 0.9 with predicate p. The threshold T is computed empirically. Finally, \u03c6 is used to efficiently compute the common set of predicates and their frequency.Contribution similarity. Contribution similarity is a feature used to explore related work, find or recommend comparable research contributions. The sub-graphs G(r i ) for each research contribution r i \u2208 R are converted into document D by concatenating the labels of subject s, predicate p, and object o, of all statements (s, p, o) \u2208 G(r i ). We then use T F /iDF  to index and retrieve the most similar contributions with respect to some query q. Queries are constructed in the same manner as documents D.Automated Information Extraction. The ORKG uses machine learning for automated extraction of scientific knowledge from literature. Of particular interest are the NLP tasks named entity recognition as well as named entity classification and linking.As a first step, we trained a neural network based machine learning model for named entity recognition using in-house developed annotations on the Elsevier Labs corpus of Science, Technology, and Medicine 3 (STM) for the following generic concepts: process, method, material and data. We use the Beltagy et al.  Named Entity Recognition task-specific neural architecture atop pretrained SciBERT embeddings with a CRF-based sequence tag decoder .Linking scientific knowledge to existing knowledge graphs including those from the open domain such as DBpedia  as well as domain specific graphs such as ULMS  is another important feature. Most importantly, such linking enables semi-automated enrichment of research contributions. ImplementationThe infrastructure consists of two main components: the back end with the business logic, system features and a set of APIs used by services and third party apps; and the front end, i.e. the User Interface (UI).Back end. The back end is written in Kotlin , within the Spring Boot 2 framework. The data is stored in a Neo4j Labeled Property Graph (LPG) database accessed via the Spring Data Neo4j's Object Graph Mapper (OGM). Data can be queried using Cypher, Neo4j's native query language. The back end is exposed via a JSON RESTful API accessed by applications, including the ORKG front end. A technical documentation of the current API specification is available online  .Data can be exported to RDF via the Neo4j Semantics extension  . Due to the differences between our graph model and RDF, a \"semantification\" needs to occur. Most importantly, the ORKG back end auto-generates URIs. Mapping (or changing) these URIs to an existing ontology must be done manually. The Semantics extension also allows importing RDF data and ontologies. The Web Ontology Language is, however, not fully supported.In order to enrich ORKG data, we support linking to data from other sources. Of particular interest are systems that collect, curate and publish bibliographic metadata or data about entities relevant to scholarly communication, such as people and organizations. Rather than collecting such metadata ourselves we thus link and integrate with relevant systems (e.g., DataCite, Crossref) and their data via identifiers such as DOI, ORCID or ISNI.Front end. The ORKG front end is a Web-based tool for, among other users, researchers and librarians and supports searching, exploring, creating and modifying research contributions.  depicts the wizard that guides users in creating research contributions.   features to deliver the depicted table. The table can be shared via a persistent link and exported to different formats, including L A T E X, CSV and PDF.The front end was built with the following two key requirements in mind: (1) Usability to enable a broad range of users, in particular researchers across disciplines, to contribute, curate and explore research contributions; and (2) Flexibility to enable maximum degree of freedom in describing research contributions.It is implemented according to the ES6 standard of JavaScript using the React 6 framework. The Bootstrap 7 framework is used for responsive interface components.The front end design takes great care to deliver the best possible overall experience for a broad range of users, in particular researchers not familiar with knowledge graph technology. User evaluations are a key instrument to continually validate the development and understand requirements. EVALUATIONSThe ORKG infrastructure, its services, features, performance and usability are continually evaluated to inform the next iteration and future developments. Among other preliminary evaluations and results, we present here the first front end user evaluation, which informed the second iteration of front end development, presented in this paper. Front end EvaluationFollowing a qualitative approach, the evaluation of the first iteration of front end development aimed to determine user performance, identify major (positive and negative) aspects, and user acceptance/perception of the system. The evaluation process had two components: (1) instructed interaction sessions and (2) a short evaluation questionnaire. This evaluation resulted in data relevant to our first research question. We conducted instructed interaction sessions with 12 authors of articles presented at the DILS2018 9 conference. The aim of these sessions was to get first-hand observations and feedback. The sessions were conducted with the support of two instructors. At the start of each session, the instructor briefly explained the underlying principles of the infrastructure, including how it works and what is required from authors to complete the task, i.e. create a structured description of the key research contribution in their article presented at the conference. Then, participants engaged with the system without further guidance from the instructor. However, at any time they could ask the instructor for assistance. For each participant, we recorded the time required to complete the task (to determine the mean duration of a session), the instructor's notes and the participant's comments.In addition to the instructed interaction sessions, participants were invited to complete a short evaluation questionnaire. The questionnaire is available online  . Its aim was to collect further insights into user experience. Since the quantity of collected data was insufficient to establish any correlational or causal relationship , the questionnaire was treated as a qualitative instrument. The paper-based questionnaire consisted of 11 questions. These were designed to capture participant thoughts regarding the positive and negative aspects of the system following their instructed interaction session. Participants completed their questionnaire after the instructed interaction session. All 12 participants answered the questionnaire. The interaction notes, participant comments and the time recordings were collected together with questionnaire responses and analysed in light of our research questions.A dataset summarizing the research contributions collected in the experiment is available online  . The data is grouped into four main categories. Research Problem describes the main question or issue addressed by the research contribution. Participants used a variety of properties to describe the problem, e.g. problem, addresses, subject, proposes and topic. Approach describes the solution taken by the authors. Properties used included approach, uses, prospective work, method, focus and algorithm. Implementation & Evaluation were the most comprehensively described aspects, arguably because it was easier for participants to describe technical details compared to describing the problem or the approach.In summary, 75% of the participants found the front end developed in the first iteration fairly intuitive and easy to use. Among the participants, 80% needed guidance only at the beginning while 10% did not need guidance. The time required to complete the task was 17 minutes on average, with a minimum of 13 minutes and a maximum of 22 minutes. Five out of twelve participants suggested to make the front end more keyboard-friendly to ease the creation of research contributions. As participant #3 stated: \"More description in advance can be helpful.\" Two participants commented that the navigation process throughout the system is complicated for firsttime users and suggested alternative approaches. As an example, participant #5 suggested to \"Use breadcrumbs to navigate. \"Four participants wished for a visualisation (i.e., graph chart) to be available when creating new research contributions. For instance, participant #1 commented that \"It could be helpful to show a local view of the graph while editing.\" This type of visualisation could facilitate comprehension and ease curation. Another participant suggested to integrate a document (PDF) viewer and highlight relevant passages or phrases. Participant #4 noted that \"If I could highlight the passages directly in the paper and add predicates there, it would be more intuitive and save time. \" Further details of the questionnaire, including participant ratings on main issues, are summarized in . While the cohort of participants was too small for statistically significant conclusions, these results provided a number of important suggestions that informed the second iteration of front end development, which is presented in this paper and will be evaluated in future conferences. Other EvaluationsWe have performed preliminary evaluations also of other components of the ORKG infrastructure. The experimental setup for these evaluations is an Ubuntu 18.04 machine with Intel Xeon CPUs 12 \u00d7 3.60 GHz and 64 GB memory.Comparison feature. With respect to the State-of-the-Art comparison feature, we compared our approach in ORKG with the baseline approach, which uses brute force to find the most similar predicates and thus checks every possible predicate combination.  shows the time needed to perform the comparison for the baseline approach and for the approach we implemented and presented above. As the results suggest, our approach clearly outperforms the baseline and the performance gain can be attributed to more efficient retrieval. The experiment is limited to 8 contributions because the baseline approach does not scale to larger sets.Scalability. For horizontal scalability, the infrastructure containerizes applications. We also tested the vertical scalability in terms of response time. For this, we created a synthetic dataset of papers. Each paper includes one research contribution described by three statements. The generated dataset contains 10 million papers or 100 million nodes. We tested the system with variable numbers of papers and the average response time to fetch a single paper with its related research contribution is 60 ms. This suggests that the infrastructure can handle large amounts of scholarly knowledge. NED performance. We evaluated the performance of a number of existing NED tools on scholarly knowledge, specifically Falcon , DBpedia Spotlight , TagME , EARL , TextRazor  and MeaningCloud  . These tools were used to link to entities from Wikidata and DBpedia. We used the annotated entities from the STM corpus as the experimental data. However, since there is no gold standard for the dataset, we only computed the coverage metric \u03b6 = # of linked entities divided by # of all entities.  summarizes the coverage percentage for the evaluated tools. The results suggest that Falcon is most promising. CONCLUSIONThis article described the first steps of a larger research and development agenda that aims to enhance document-based scholarly communication with semantic representations of communicated scholarly knowledge. In other words, we aim to bring scholarly communication to the technical standards of the 21st century and the age of modern digital libraries. We presented the architecture of the proposed infrastructure as well as a first implementation. The front end has seen substantial development, driven by earlier user feedback. We have reported here the results of the user evaluation to underpin the development of the current front end, which was recently released for public view and use. By integrating crowdsourcing and automated techniques in natural language processing, initial steps were also taken and evaluated that advance multi-modal scholarly knowledge acquisition using the ORKG.\n###\n"}
{"text": "#Properties\nHas value, Method, Time period, has beginning, has end, Lower confidence limit, Upper confidence limit, Located in, Basic reproduction number, Confidence interval (95%), has research problem, location, Approaches, same as, description\n#Text\n, 355 cases have been confirmed as having COVID-19 infection on the Diamond Princess cruise ship. It is of crucial importance to estimate the reproductive number (R0) of the novel virus in the early stage of outbreak and make a prediction of daily new cases on the ship. Method: We fitted the reported serial interval (mean and standard deviation) with a gamma distribution and applied \"earlyR\" package in R to estimate the R0 in the early stage of COVID-19 outbreak. We applied \"projections\" package in R to simulate the plausible cumulative epidemic trajectories and future daily incidence by fitting the data of existing daily incidence, a serial interval distribution, and the estimated R0 into a model based on the assumption that daily incidence obeys approximately Poisson distribution determined by daily infectiousness. Results: The Maximum-Likelihood (ML) value of R0 was 2.28 for COVID-19 outbreak at the early stage on the ship. The median with 95% confidence interval (CI) of R0 values was 2.28 (2.06-2.52) estimated by the bootstrap resampling method. The probable number of new cases for the next ten days would gradually increase, and the estimated cumulative cases would reach 1514 (1384-1656) at the tenth day in the future. However, if R0 value was reduced by 25% and 50%, the estimated total number of cumulative cases would be reduced to 1081 (981-1177) and 758 (697-817), respectively. Conclusion: The median with 95% CI of R0 of COVID-19 was about 2.28 (2.06-2.52) during the early stage experienced on the Diamond Princess cruise ship. The future daily incidence and probable outbreak size is largely dependent on the change of R0. Unless strict infection management and control are taken, our findings indicate the potential of COVID-19 to cause greater outbreak on the ship. Methods DefinitionsA confirmed case of COVID-19 infection was defined as a case with a positive result for viral nucleic acid testing in respiratory specimens. Suspected case was defined as a case with symptoms of COVID-19 infection but not confirmed by viral nucleic acid testing. Serial interval was defined as the duration between symptom onset of the primary case and symptom onset of the secondary in a transmission chain. R0 was defined as the expected number of secondary cases that one primary case will generate in a susceptible population . Data sourceAll the data were captured from the official website (Ministry of Health, Labour and Welfare of Japan, 2020) that reported the situation of COVID-19 infection in Japan. The data for model development were updated to February 16, 2020. Model development and statistical analysisTo evaluate the transmissibility of COVID-19 on the ship, we applied the \"earlyR\" package to estimate the R0 in the early stage of outbreak . Serial interval distribution is required for R0 estimation, and there was insufficient information about cluster cases for serial interval estimation. Therefore, we assumed the serial interval of COVID-19 on the ship was equal to that of COVID-19 in Wuhan, China, with a mean of 7.5 days and a standard deviation of 3.4 days . We fitted the value of serial interval (mean and standard deviation) with a gamma distribution, as previously described . The \"get_R\" function with Maximum-Likelihood (ML) estimation was used to obtain the distribution of R0.To derive other statistics for R0 distribution estimation, we used a bootstrap strategy with 1000 times resampling to get a large set of likely R0 values. We displayed these R0 values in histogram format and computed the median and interquantile range for these R0 values. R package of \"projections\" was used for plausible epidemic trajectories simulation and future daily incidence prediction . The simulation and prediction were generated by fitting the data of existing daily incidence, a serial interval distribution, and the estimated R0 into a model based on the assumption that daily incidence obeys approximately Poisson distribution determined by daily infectiousness, which was denoted asysw\u00f0t \u00c0 s\u00de w\u00f0t \u00c0 s\u00de was the vector of probability mass function (PMF) of serial interval distribution. ys was the real-time incidence at time S (, . We computed the prediction of daily incidence for the next ten days using a bootstrap resampling method (1000 times). We also plotted the possible cumulative incidence range for the next ten days. All statistical analyses and model development were performed using R version 3.6.2. ResultsSerial interval distribution of COVID-19 is shown in . Using the serial interval distribution described above, we estimated that the ML value of R0 was 2.28 for COVID-19 outbreak at the early stage on the ship ( ). With a bootstrap strategy, we obtained 1000 likely R0 values. The distribution of these R0 values was displayed as histogram plot in . The estimated median with 95% confidence interval (CI) of R0 values was 2.28 (2.06-2.52).We then computed the probable number of new cases for the next ten days based on existing data and estimated R0. As shown in , the daily median number with 95% confidence interval (CI) of new cases was 57 (42-75), 66 (49-84), 77 (59-96), 89 (68-111), 102 (81-125), 117 (93-142), 133 (107-164), 150 (123-184), 171 (138-208), 194 (156-235), respectively. We also simulated the range of cumulative number of cases for the next ten days, which is shown in . The daily cumulative number of cases with 95% CI was 413 (397-430), 478 (456-503), 555 (528-588), 645 (606-684), 747 (696-798), 863 (800-925), 996 (919-1071), 1148 (1055-1242), 1321 (1210-1437), 1514 (1384-1656), respectively.We considered that the crew had taken measures to control the spread of infection, which would reduce the value of R0. We assume that if R0 value was reduced by 25%, the corresponding median number with 95% CIs of new cases for the next ten days would be 43 (30-57), 49 (35-64), 56 (41-72), 63 (47-81), 69 (51-90), 75 (56-96), 83 (63-105), 89 (68-111), 96 (74-123),104 (79-133) ( ). The total number of cumulative cases would be reduced to 1081 (981-1177) at the tenth day in the future ( ). If the R0 value was further reduced by 50%, the corresponding new cases for the next ten\n###\n", "summary": " Has value: 2.28\n, Time period: Time interval\n, has beginning: 2020-02-16\n, has end: 2020-02-16\n, Lower confidence limit: 2.06\n, Upper confidence limit: 2.52\n, Basic reproduction number: Basic reproduction number estimate value specification\n, Confidence interval (95%): Confidence interval (95%)\n, has research problem: Determination of the COVID-19 basic reproduction number\n, location: the Diamond Princess cruise ship\n, same as: https://en.wikipedia.org/wiki/COVID-19_pandemic\n, description: This research problem aims at determining the basic reproduction rate of COVID-19.\n###"}
{"summary": "has benchmark: Benchmark WMT2016 English-German/Benchmark WMT2014 French-English/Benchmark WMT2016 German-English/Benchmark WMT2014 English-French/Benchmark WMT2014 German-English/Benchmark WMT2014 English-German\nhas research problem: Unsupervised Machine Translation\nhas model: SMT + NMT (tuning and joint refinement)", "text": "#Properties\nhas benchmark, has research problem, has model\n#Text\nWhile machine translation has traditionally relied on large amounts of parallel corpora, a recent research line has managed to train both Neural Machine Translation (NMT) and Statistical Machine Translation (SMT) systems using monolingual corpora only. In this paper, we identify and address several deficiencies of existing unsupervised SMT approaches by exploiting subword information, developing a theoretically well founded unsupervised tuning method, and incorporating a joint refinement procedure. Moreover, we use our improved SMT system to initialize a dual NMT model, which is further fine-tuned through onthe-fly back-translation. Together, we obtain large improvements over the previous stateof-the-art in unsupervised machine translation. For instance, we get 22.5 BLEU points in English-to-German WMT 2014, 5.5 points more than the previous best unsupervised system, and 0.5 points more than the (supervised) shared task winner back in 2014. Principled unsupervised SMTPhrase-based SMT is formulated as a log-linear combination of several statistical models: a translation model, a language model, a reordering model and a word/phrase penalty. As such, building an unsupervised SMT system requires learning these different components from monolingual corpora. As it turns out, this is straightforward for most of them: the language model is learned from monolingual corpora by definition; the word and phrase penalties are parameterless; and one can drop the standard lexical reordering model at a small cost and do with the distortion model alone, which is also parameterless. This way, the main challenge left is learning the translation model, that is, building the phrase-table.Our proposed method starts by building an initial phrase-table through cross-lingual embedding mappings (Section 3.1). This initial phrase-table is then extended by incorporating subword information, addressing one of the main limitations of previous unsupervised SMT systems (Section 3.2). Having done that, we adjust the weights of the underlying log-linear model through a novel unsupervised tuning procedure (Section 3.3). Finally, we further improve the system by jointly refining two models in opposite directions (Section 3.4). Initial phrase-tableSo as to build our initial phrase-table, we follow  and learn n-gram embeddings for each language independently, map them to a shared space through self-learning, and use the resulting cross-lingual embeddings to extract and score phrase pairs.More concretely, we train our n-gram embeddings using phrase2vec 1 , a simple extension of skip-gram that applies the standard negative sampling loss of  to bigramcontext and trigram-context pairs in addition to the usual word-context pairs. 2 Having done that, we map the embeddings to a cross-lingual space using VecMap 3 with identical initialization , which builds an initial solution by aligning identical words and iteratively improves it through self-learning. Finally, we extract translation candidates by taking the 100 nearestneighbors of each source phrase, and score them by applying the softmax function over their cosine similarities:where the temperature \u03c4 is estimated using maximum likelihood estimation over a dictionary induced in the reverse direction. In addition to the phrase translation probabilities in both directions, the forward and reverse lexical weightings are also estimated by aligning each word in the target phrase with the one in the source phrase most likely generating it, and taking the product of their respective translation probabilities. The reader is referred to  for more details. Adding subword informationAn inherent limitation of existing unsupervised SMT systems is that words are taken as atomic units, making it impossible to exploit characterlevel information. This is reflected in the known difficulty of these models to translate named entities, as it is very challenging to discriminate among related proper nouns based on distributional information alone, yielding to translation errors like \"Sunday Telegraph\" \u2192 \"The Times of London\" . So as to overcome this issue, we propose to incorporate subword information once the initial alignment is done at the word/phrase level. For that purpose, we add two additional weights to the initial phrase-table that are analogous to the lexical weightings, but use a character-level similarity function instead of word translation probabilities:where = 0.3 guarantees a minimum similarity score, as we want to favor translation candidates that are similar at the character level without excessively penalizing those that are not. In our case, we use a simple similarity function that normalizes the Levenshtein distance lev(\u2022)  by the length of the words len(\u2022): sim(f, e) = 1 \u2212 lev(f, e) max(len(f ), len(e))We leave the exploration of more elaborated similarity functions and, in particular, learnable metrics , for future work. Unsupervised tuningHaving trained the underlying statistical models independently, SMT tuning aims to adjust the weights of their resulting log-linear combination to optimize some evaluation metric like BLEU in a parallel validation corpus, which is typically done through Minimum Error Rate Training or MERT . Needless to say, this cannot be done in strictly unsupervised settings, but we argue that it would still be desirable to optimize some unsupervised criterion that is expected to correlate well with test performance. Unfortunately, neither of the existing unsupervised SMT systems do so: Artetxe et al. (2018b) use a heuristic that builds two initial models in opposite directions, uses one of them to generates a synthetic parallel corpus through back-translation , and applies MERT to tune the model in the reverse direction, iterating until convergence, whereas Lample et al. (2018b) do not perform any tuning at all. In what follows, we propose a more principled approach to tuning that defines an unsupervised criterion and an optimization procedure that is guaranteed to converge to a local optimum of it.Inspired by the previous work on CycleGANs  and dual learning , our method takes two initial models in opposite directions, and defines an unsupervised optimization objective that combines a cyclic consistency loss and a language model loss over the two monolingual corpora E and F :The cyclic consistency loss captures the intuition that the translation of a translation should be close to the original text. So as to quantify this, we take a monolingual corpus in the source language, translate it to the target language and back to the source language, and compute its BLEU score taking the original text as reference:At the same time, the language model loss captures the intuition that machine translation should produce fluent text in the target language. For that purpose, we estimate the per-word entropy in the target language corpus using an n-gram language model, and penalize higher per-word entropies in machine translated text as follows: 4We initially tried to directly minimize the entropy of the generated text, but this worked poorly in our preliminary experiments on English-Spanish (note that we used this language pair exclusively for development to be faithful to our unsupervised scenario at test time). More concretely, the behavior of the optimization algorithm was very unstable, as it tended to excessively focus on either the cyclic consistency loss or the language model loss at the cost of the other, and we found it very difficult to find the right balance between the two factors.where the length penalty LP = LP(E) \u2022 LP(F ) penalizes excessively long translations: 5So as to minimize the combined loss function, we adapt MERT to jointly optimize the parameters of the two models. In its basic form, MERT approximates the search space for each source sentence through an n-best list, and performs a form of coordinate descent by computing the optimal value for each parameter through an efficient line search method and greedily taking the step that leads to the largest gain. The process is repeated iteratively until convergence, augmenting the n-best list with the updated parameters at each iteration so as to obtain a better approximation of the full search space. Given that our optimization objective combines two translation systems T F \u2192E (T E\u2192F (E)), this would require generating an n-best list for T E\u2192F (E) first and, for each entry on it, generating a new n-best list with T F \u2192E , yielding a combined n-best list with N 2 entries. So as to make it more efficient, we propose an alternating optimization approach where we fix the parameters of one model and optimize the other with standard MERT. Thanks to this, we do not need to expand the search space of the fixed model, so we can do with an n-best list of N entries alone. Having done that, we fix the parameters of the opposite model and optimize the other, iterating until convergence. Joint refinementConstrained by the lack of parallel corpora, the procedure described so far makes important simplifications that could compromise its potential performance: its phrase-table is somewhat unnatural (e.g. the translation probabilities are estimated from cross-lingual embeddings rather than actual frequency counts) and it lacks a lexical reordering model altogether. So as to overcome this issue, existing unsupervised SMT methods generate a synthetic parallel corpus through back-translation and use it to train a standard SMT system from scratch, iterating until convergence.An obvious drawback of this approach is that the back-translated side will contain ungrammatical n-grams and other artifacts that will end up in the induced phrase-table. One could argue that this should be innocuous as long as the ungrammatical n-grams are in the source side, as they should never occur in real text and their corresponding entries in the phrase-table should therefore not be used. However, ungrammatical source phrases do ultimately affect the estimation of the backward translation probabilities, including those of grammatical phrases.  We argue that, ultimately, the backward probability estimations can only be meaningful when all source phrases are grammatical (so the probabilities of all plausible translations sum to one) and, similarly, the forward probability estimations can only be meaningful when all target phrases are grammatical.Following the above observation, we propose an alternative approach that jointly refines both translation directions. More concretely, we use the initial systems to build two synthetic corpora in opposite directions.  Having done that, we independently extract phrase pairs from each synthetic corpus, and build a phrase-table by taking their intersection. The forward probabilities are estimated in the parallel corpus with the synthetic source side, while the backward probabilities are estimated in the one with the synthetic target side. This does not only guarantee that the probability estimates are meaningful as discussed previously, but it also discards the ungrammatical phrases altogether, as both the source and the target n-grams must have occurred in the original monolingual texts to be present in the resulting phrase-table. This phrase-table is then combined with a lexical reordering model learned on the synthetic parallel corpus in the reverse direction, and we apply the unsupervised tuning method described in Section 3.3 to adjust the weights of the resulting system. We repeat this process for a total of 3 iterations. 8 6 For instance, let's say that the target phrase \"dos gatos\" has been aligned 10 times with \"two cats\" and 90 times with \"two cat\". While the ungrammatical phrase-table entry two cat-dos gatos should never be picked, the backward probability estimation of two cats -dos gatos is still affected by it (it would be 0.1 instead of 1.0 in this example).7 For efficiency purposes, we restrict the size of each synthetic parallel corpus to 10 million sentence pairs.8 For the last iteration, we do not perform any tuning and use default Moses weights instead, which we found to be more robust during development. Note, however, that using unsupervised tuning during the previous steps was still strongly beneficial. NMT hybridizationWhile the rigid and modular design of SMT provides a very suitable framework for unsupervised machine translation, NMT has shown to be a fairly superior paradigm in supervised settings, outperforming SMT by a large margin in standard benchmarks. As such, the choice of SMT over NMT also imposes a hard ceiling on the potential performance of these approaches, as unsupervised SMT systems inherit the very same limitations of their supervised counterparts (e.g. the locality and sparsity problems). For that reason, we argue that SMT provides a more appropriate architecture to find an initial alignment between the languages, but NMT is ultimately a better architecture to model the translation process.Following this observation, we propose a hybrid approach that uses unsupervised SMT to warm up a dual NMT model trained through iterative backtranslation. More concretely, we first train two SMT systems in opposite directions as described in Section 3, and use them to assist the training of another two NMT systems in opposite directions. These NMT systems are trained following an iterative process where, at each iteration, we alternately update the model in each direction by performing a single pass over a synthetic parallel corpus built through back-translation .  In the first iteration, the synthetic parallel corpus is entirely generated by the SMT system in the opposite direction but, as training progresses and the NMT models get better, we progressively switch to a synthetic parallel corpus generated by the reverse NMT model. More concretely, iteration t uses N smt = N \u2022 max(0, 1 \u2212 t/a) synthetic parallel sentences from the reverse SMT system, where the parameter a controls the number of transition iterations from SMT to NMT back-translation. The remaining N \u2212 N smt sentences are generated by the reverse NMT model. Inspired by , we use greedy decoding for half of them, which produces more fluent and predictable translations, and random sampling for the other half, which produces more varied translations. In our experiments, we use N = 1, 000, 000 and a = 30, and perform a total of 60 such iterations. At test time, we use beam search decoding with an ensemble of all check-  points from every 10 iterations. Experiments and resultsIn order to make our experiments comparable to previous work, we use the French-English and German-English datasets from the WMT 2014 shared task. More concretely, our training data consists of the concatenation of all News Crawl monolingual corpora from 2007 to 2013, which make a total of 749 million tokens in French, 1,606 millions in German, and 2,109 millions in English, from which we take a random subset of 2,000 sentences for tuning (Section 3.3). Preprocessing is done using standard Moses tools, and involves punctuation normalization, tokenization with aggressive hyphen splitting, and truecasing. Our SMT implementation is based on Moses 10 , and we use the KenLM  tool included in it to estimate our 5-gram language model with modified Kneser-Ney smoothing. Our unsupervised tuning implementation is based on Z-MERT , and we use FastAlign  for word alignment within the joint refinement procedure. Finally, we use the big transformer implementation from fairseq 11 for our NMT system, training with a total batch size of 20,000 tokens across 8 GPUs with the exact same hyperparameters as .We use newstest2014 as our test set for French-English, and both newstest2014 and new-stest2016 (from WMT 2016 12 ) for German-English. Following common practice, we report tokenized BLEU scores as computed by the multi-bleu.perl script included in Moses. In addition to that, we also report detokenized BLEU scores as computed by SacreBLEU 13 , which is equivalent to the official mteval-v13a.pl script. We next present the results of our proposed system in comparison to previous work in Section 5.1. Section 5.2 then compares the obtained results to those of different supervised systems. Finally, Section 5.3 presents some translation examples from our system.  reports the results of the proposed system in comparison to previous work. As it can be seen, our full system obtains the best published results in all cases, outperforming the previous stateof-the-art by 5-7 BLEU points in all datasets and translation directions. Main resultsA substantial part of this improvement comes from our more principled unsupervised SMT ap-  Note that it is only the test set that is from WMT 2016. All the training data comes from WMT 2014 News Crawl, so it is likely that our results could be further improved by using the more extensive monolingual corpora from WMT 2016.13 SacreBLEU signature: BLEU+case.mixed+lang.LANG +numrefs.1+smooth.exp+test.TEST+tok.13a+version.1.2.1 1, with LANG \u2208 {fr-en, en-fr, de-en, en-de} and TEST \u2208 {wmt14/full, wmt16}   proach, which outperforms all previous SMTbased systems by around 2 BLEU points. Nevertheless, it is the NMT hybridization that brings the largest gains, improving the results of this initial SMT systems by 5-9 BLEU points. As shown in , our absolute gains are considerably larger than those of previous hybridization methods, even if our initial SMT system is substantially better and thus more difficult to improve upon. This way, our initial SMT system is about 4-5 BLEU points above that of , yet our absolute gain on top of it is around 2.5 BLEU points higher. When compared to Lample et al. (2018b), we obtain an absolute gain of 5-6 BLEU points in both French-English directions while they do not get any clear improvement, and we obtain an improvement of 7-9 BLEU points in both German-English directions, in contrast with the 2.3 BLEU points they obtain. More generally, it is interesting that pure SMT systems perform better than pure NMT systems, yet the best results are obtained by initializing an NMT system with an SMT system. This suggests that the rigid and modular architecture of SMT might be more suitable to find an initial alignment between the languages, but the final system should be ultimately based on NMT for optimal results. Comparison with supervised systemsSo as to put our results into perspective,  reports the results of different supervised systems in the same WMT 2014 test set. More concretely, we include the best results from the shared task itself, which reflect the state-of-the-art in machine translation back in 2014; those of , who introduced the now predominant transformer architecture; and those of , who apply back-translation at a large scale and, to the best of our knowledge, hold the current best results in the test set.As it can be seen, our unsupervised system outperforms the WMT 2014 shared task winner in English-to-German, and is around 2 BLEU points behind it in the other translation directions. This shows that unsupervised machine translation is already competitive with the state-of-the-art in supervised machine translation in 2014. While the field of machine translation has undergone great progress in the last 5 years, and the gap between our unsupervised system and the current state-ofthe-art in supervised machine translation is still large as reflected by the other results, this suggests that unsupervised machine translation can be a usable alternative in practical settings. La NHTSA n'a pas pu examiner la lettre d'information aux propri\u00e9taires en raison de l'arr\u00eat de 16 jours des activit\u00e9s gouvernementales, ce qui a ralenti la croissance des ventes de v\u00e9hicules en octobre.NHTSA could not review the owner notification letter due to the 16-day government shutdown, which tempered auto sales growth in October.The NHTSA could not consider the letter of information to owners because of halting 16-day government activities, which slowed the growth in vehicle sales in October.NHTSA said it could not examine the letter of information to owners because of the 16-day halt in government operations, which slowed vehicle sales growth in October.Le M23 est n\u00e9 d'une mutinerie, en avril 2012, d'anciens rebelles, essentiellement tutsi, int\u00e9gr\u00e9s dans l'arm\u00e9e en 2009 apr\u00e8s un accord de paix.The M23 was born of an April 2012 mutiny by former rebels, principally Tutsis who were integrated into the army in 2009 following a peace agreement.M23 began as a mutiny in April 2012, former rebels, mainly Tutsi integrated into the national army in 2009 after a peace deal.The M23 was born into a mutiny in April 2012, of former rebels, mostly Tutsi, embedded in the army in 2009 after a peace deal.Tunks a d\u00e9clar\u00e9 au Sunday Telegraph de Sydney que toute la famille \u00e9tait \u00abextr\u00eamement pr\u00e9occup\u00e9e\u00bb du bien\u00eatre de sa fille et voulait qu'elle rentre en Australie.Tunks told Sydney's Sunday Telegraph the whole family was \"extremely concerned\" about his daughter's welfare and wanted her back in Australia.Tunks told The Times of London from Sydney that the whole family was \"extremely concerned\" of the welfare of her daughter and wanted it to go in Australia.Tunks told the Sunday Telegraph in Sydney that the whole family was \"extremely concerned\" about her daughter's well-being and wanted her to go into Australia.  .  shows some translation examples from our proposed system in comparison to those reported by . We choose the exact same sentences reported by , which were randomly taken from newstest2014, so they should be representative of the general behavior of both systems. Qualitative resultsWhile not perfect, our proposed system produces generally fluent translations that accurately capture the meaning of the original text. Just in line with our quantitative results, this suggests that unsupervised machine translation can be a usable alternative in practical settings.Compared to , our translations are generally more fluent, which is not surprising given that they are produced by an NMT system rather than an SMT system. In addition to that, the system of  has some adequacy issues when translating named entities and numerals (e.g. 34 \u2192 32, Sunday Telegraph \u2192 The Times of London), which we do not observe for our proposed system in these examples. Conclusions and future workIn this paper, we identify several deficiencies in previous unsupervised SMT systems, and propose a more principled approach that addresses them by incorporating subword information, using a theoretically well founded unsupervised tuning method, and developing a joint refinement procedure. In addition to that, we use our improved SMT approach to initialize a dual NMT model that is further improved through on-the-fly backtranslation. Our experiments show the effectiveness of our approach, as we improve the previous state-of-the-art in unsupervised machine translation by 5-7 BLEU points in French-English and German-English WMT 2014 and 2016. Our code is available as an open source project at https: //github.com/artetxem/monoses.In the future, we would like to explore learnable similarity functions like the one proposed by  to compute the characterlevel scores in our initial phrase-table. In addition to that, we would like to incorporate a language modeling loss during NMT training similar to . Finally, we would like to adapt our approach to more relaxed scenarios with multiple languages and/or small parallel corpora.\n###\n"}
{"text": "#Properties\nHas value, Global Mean Sea level Rise Projection, has lower limit for likely range, has upper limit for likely range, has  start of period, has end of period, climate scenario, has research problem, has unit\n#Text\nMechanisms such as ice-shelf hydrofracturing and ice-cliff collapse may rapidly increase discharge from marine-based ice sheets. Here, we link a probabilistic framework for sea-level projections to a small ensemble of Antarctic ice-sheet (AIS) simulations incorporating these physical processes to explore their influence on global-mean sea-level (GMSL) and relative sea-level (RSL). We compare the new projections to past results using expert assessment and structured expert elicitation about AIS changes. Under high greenhouse gas emissions (Representative Concentration Pathway [RCP] 8.5), median projected 21st century GMSL rise increases from 79 to 146 cm. Without protective measures, revised median RSL projections would by 2100 submerge land currently home to 153 million people, an increase of 44 million. The use of a physical model, rather than simple parameterizations assuming constant acceleration of ice loss, increases forcing sensitivity: overlap between the central 90% of simulations for 2100 for RCP 8.5 (93-243 cm) and RCP 2.6 (26-98 cm) is minimal. By 2300, the gap between median GMSL estimates for RCP 8.5 and RCP 2.6 reaches >10 m, with median RSL projections for RCP 8.5 jeopardizing land now occupied by 950 million people (versus 167 million for RCP 2.6). The minimal correlation between the contribution of AIS to GMSL by 2050 and that in 2100 and beyond implies current sea-level observations cannot exclude future extreme outcomes. The sensitivity of post-2050 projections to deeply uncertain physics highlights the need for robust decision and adaptive management frameworks. Plain Language Summary Recent ice-sheet modeling papers have introduced new physical mechanisms-specifically the hydrofracturing of ice shelves and the collapse of ice cliffs-that can rapidly increase ice-sheet mass loss from a marine-based ice-sheet, as exists in much of Antarctica. This paper links new Antarctic model results into a sea-level rise projection framework to examine their influence on global and regional sea-level rise projections and their associated uncertainties, the potential impact of projected sea-level rise on areas currently occupied by human populations, and the implications of these projections for the ability to constrain future changes from present observations. Under a high greenhouse gas emission future, these new physical processes increase median projected 21st century GMSL rise from \u223c80 to \u223c150 cm. Revised median RSL projections for a high-emissions future would, without protective measures, by 2100 submerge land currently home to more than 153 million people. The use of a physical model indicates that emissions matter more for 21st century sea-level change than previous projections showed. Moreover, there is little correlation between the contribution of Antarctic to sea-level rise by 2050 and its contribution in 2100 and beyond, so current sea-level observations cannot exclude future extreme outcomes. Revised Antarctic ProjectionsIn this paper, we compare two sets of projections. The first, which we label K14, follows the original methodology of K14, extended in space and time. The second, which we label DP16, replaces the AIS projections of K14 with projections based on new physical modeling . These processes include the influence of surface meltwater, driven by summer temperatures above freezing and the increasing ratio of rain to snow in a warming climate, on the penetration into ice shelves of surface crevasses that can lead to hydrofracturing. Hence, in DP16, buttressing ice shelves can thin or be lost entirely due to sub-ice ocean warming, the extensive spread of surface meltwater, or a combination of the two. In places where thick, marine-terminating grounding lines have lost their buttressing ice shelves, a wastage rate of ice is applied locally at the tidewater grounding line in places where vertical ice cliffs are tall enough to produce stresses that exceed the yield strength of the ice (see DeConto for complete formulation).Three uncertain but key model parameters relate to (1) the rate of sub-ice shelf melt rates in response to warming ocean temperatures (OCFAC), (2) the sensitivity of crevasse penetration to meltwater input (hydrofracturing) (CREVLIQ), and (3) the maximum rate of cliff collapse (VCLIF). Because, as discussed below, there are no modern analogues to widespread ice-cliff failure, model performance cannot be adequately judged relative to Holocene or recent trends in ice-sheet behavior. Instead, the new model physics were tested relative to past episodes of ice sheet retreat during the Pliocene (\u223c3 Ma) and the Last Interglacial (LIG, \u223c125 ka), when Antarctic ocean and surface air temperatures were warmer than today . The three key parameters were varied systematically. From an initial 64 versions of the ice-sheet model, 29 were found to satisfy both Pliocene and LIG sea-level targets, with Antarctic contributions to GMSL ranging between 5-15 m (Pliocene) and 3.6-7.4 m (LIG). The range of oceanic melt rate model parameters passing the Pliocene and LIG sea-level tests are comparable to those determined from a large, 625-member ensemble of the last deglacial retreat of the WAIS using the same ice-sheet model ; however, the deglacial simulations do not provide guidance on hydrofracturing and ice-cliff physics, because the background climate was too cold to trigger those processes.One challenge of formulating a parameterization of ice-cliff physics is the lack of observations of marine-terminating ice without buttressing ice shelves and of sufficient thickness (\u223c1000 m) to allow subaerial ice cliffs tall enough (\u223c100 m) to drive structural collapse . The few calving fronts of this scale that exist today (e.g., Helheim and Jakobshavn Glaciers on Greenland, and Crane Glacier on the Antarctic Peninsula) are experiencing rates of calving and structural failure at the terminus, comparable to the seaward flow of the glaciers, on the order of \u223c2 to >12 km/yr. (e.g., . Unlike several major Antarctic outlet glaciers, these Greenland outlet glaciers are in relatively narrow (5-12 km wide), restricted fjords, with substantial m\u00e9lange (a mix of ice bergs and sea ice that can provide some supporting buttressing/back pressure at the terminus), and supportive, lateral shear along the fjord walls. Hence, using observed rates of cliff collapse to constrain the model physics representing these processes could lead to underestimates.In Antarctica, there is potential for much wider ice cliffs to form along vast stretches of the coastline if floating ice tongues and shelves are lost. For example,\n###\n", "summary": " Has value: 0.23\n, Global Mean Sea level Rise Projection: Global Mean Sea Level Rise Projections\n, has lower limit for likely range: 0.16\n, has upper limit for likely range: 0.33\n, has  start of period: 2000\n, has end of period: 2050\n, climate scenario: RCP2.6\n, has research problem: Global Mean Sea Level Rise Projections\n, has unit: m\n###"}
{"text": "#Properties\nimplementation, dataset, has research problem, evaluation, Task, Test questions, Train questions, Question language, Language, On, Question analysis task, Phrase mapping task, Disambiguation task, Query construction task\n#Text\nThe Linked Data initiative comprises structured databases in the Semantic-Web data model RDF. Exploring this heterogeneous data by structured query languages is tedious and error-prone even for skilled users. To ease the task, this paper presents a methodology for translating natural language questions into structured SPARQL queries over linked-data sources. Our method is based on an integer linear program to solve several disambiguation tasks jointly: the segmentation of questions into phrases; the mapping of phrases to semantic entities, classes, and relations; and the construction of SPARQL triple patterns. Our solution harnesses the rich type system provided by knowledge bases in the web of linked data, to constrain our semantic-coherence objective function. We present experiments on both the question translation and the resulting query answering. MotivationRecently, very large, structured, and semantically rich knowledge bases have become available. Examples are Yago , DBpedia , and Freebase . DBpedia forms the nucleus of the Web of Linked Data , which interconnects hundreds of RDF data sources with a total of 30 billion subject-property-object (SPO) triples.The diversity of linked-data sources and their high heterogeneity make it difficult for humans to search and discover relevant information. As linked data is in RDF format, the standard approach would be to run structured queries in triple-pattern-based languages like SPARQL, but only expert programmers are able to precisely specify their information needs and cope with the high heterogeneity of the data (and absence or very high complexity of schema information). For less initiated users the only option to query this rich data is by keyword search (e.g., via services like sig.ma ). None of these approaches is satisfactory. Instead, the by far most convenient approach would be to search in knowledge bases and the Web of linked data by means of natural-language questions.As an example, consider a quiz question like \"Which female actor played in Casablanca and is married to a writer who was born in Rome?\". The answer could be found by querying several linked data sources together, like the IMDBstyle LinkedMDB movie database and the DBpedia knowledge base, exploiting that there are entity-level sameAs links between these collections. One can think of different formulations of the example question, such as \"Which actress from Casablanca is married to a writer from Rome?\". A possible SPARQL formulation, assuming a user familiar with the schema of the underlying knowledge base(s), could consist of the following six triple patterns (joined by shared-variable bindings): ?x hasGender female, ?x isa actor, ?x actedIn Casablanca (film), ?x marriedTo ?w, ?w isa writer, ?w bornIn Rome. This complex query, which involves multiple joins, would yield good results, but it is difficult for the user to come up with the precise choices for relations, classes, and entities. This would require familiarity with the contents of the knowledge base, which no average user is expected to have. Our goal is to automatically create such structured queries by mapping the user's question into this representation. Keyword search is usually not a viable alternative when the information need involves joining multiple triples to construct the final result, notwithstanding good attempts like that of . In the example, the obvious keyword query \"female actress Casablanca married writer born Rome\" lacks a clear specification of the relations among the different entities. ProblemGiven a natural language question q N L and a knowledge base KB, our goal is to translate q N L into a formal query q F L that captures the information need expressed by q N L .We focus on input questions that put the emphasis on entities, classes, and relations between them. We do not consider aggregations (counting, max/min, etc.) and negations. As a result, we generate structured queries of the form known as conjunctive queries or select-project-join queries in database terminology. Our target language is SPARQL 1.0, where the above focus leads to queries that consist of multiple triple patterns, that is, conjunctions of SPO search conditions. We do not use any pre-existing query templates, but generate queries from scratch as they involve a variable number of joins with apriori unknown join structure.A major challenge is in the ambiguity of the phrases occurring in a natural-language question. Phrases can denote entities (e.g., the city of Casablanca or the movie Casablanca), classes (e.g., actresses, movies, married people), or relations/properties (e.g., marriedTo between people, played between people and movies). A priori, we do not know if a phrase should be mapped to an entity, a class, or a relation. In fact, some phrases may denote any of these three kinds of targets. For example, a phrase like \"wrote score for\" in a question about film music composers, could map to the composerfilm relation wroteSoundtrackForFilm, to the class of movieSoundtracks (a subclass of music pieces), or to an entity like the movie \"The Score\". Depending on the choice, we may arrive at a structurally good query (with triple patterns that can actually be joined) or at a meaningless and non-executable query (with disconnected triple patterns). This generalized disambiguation problem is much more challenging than the more focused task of named entity disambiguation (NED). It is also different from general word sense disambiguation (WSD), which focuses on the meaning of individual words (e.g., mapping them to WordNet synsets). ContributionIn our approach, we introduce new elements towards making translation of questions into SPARQL triple patterns more expressive and robust. Most importantly, we solve the disambiguation and mapping tasks jointly, by encoding them into a comprehensive integer linear program (ILP): the segmentation of questions into meaningful phrases, the mapping of phrases to semantic entities, classes, and relations, and the construction of SPARQL triple patterns. The ILP harnesses the richness of large knowledge bases like Yago2 , which has information not only about entities and relations, but also about surface names and textual patterns by which web sources refer to them. For example, Yago2 knows that \"Casablanca\" can refer to the city or the film, and \"played in\" is a pattern that can denote the actedIn relation. In addition, we can leverage the\n###\n", "summary": " implementation: DEANNA\n, has research problem: Question answering systems\n, Question analysis task: Dependency parser/NE n-gram strategy/POS handmade\n, Phrase mapping task: Knowledge base labels\n, Disambiguation task: LIP/Local disambiguation\n, Query construction task: Using info. from the QA\n###"}
{"text": "#Properties\nHas value, Global Mean Sea level Rise Projection, has lower limit for likely range, has upper limit for likely range, has  start of period, has end of period, climate scenario, has research problem, has unit\n#Text\nThere is a growing awareness that uncertainties surrounding future sea-level projections may be much larger than typically perceived. Recently published projections appear widely divergent and highly sensitive to non-trivial model choices. Moreover, the West Antarctic ice sheet (WAIS) may be much less stable than previous believed, enabling a rapid disintegration. Here, we present a set of probabilistic sea-level projections that approximates the deeply uncertain WAIS contributions. The projections aim to inform robust decisions by clarifying the sensitivity to non-trivial or controversial assumptions. We show that the deeply uncertain WAIS contribution can dominate other uncertainties within decades. These deep uncertainties call for the development of robust adaptive strategies. These decision-making needs, in turn, require mission-oriented basic science, for example about potential signposts and the maximum rate of WAIS-induced sea-level changes. Future sea-level rise poses nontrivial risks for many coastal communities 1, 2. Managing these risks often relies on consensus projections like those provided by the IPCC 3. Yet, there is a growing awareness that the surrounding uncertainties may be much larger than typically perceived 4. Recently published sea-level projections appear widely divergent and highly sensitive to non-trivial model choices 4. Moreover, the West Antarctic ice sheet (WAIS) may be much less stable than previously believed, enabling a rapid disintegration 5, 6. In response, some agencies have already announced to update their projections accordingly 7, 8. The construction of sea-level projections is often largely motivated by scientific considerations, such as gaining a better understanding of the underlying physics 2, 9. In this process, the translation from input data to model projections and full uncertainty estimates involves a wide range of non-trivial model choices and assumptions that can result in large discrepancies between different uncertainty estimates 4. For example, many studies consider a high level of model detail indispensable for reliable projections 3 , whereas semi-empirical modeling approaches 10-12 trade complexity for the ability to calibrate the model. Semi-empirical modeling approaches often rely on strong assumptions about the prior parameter distributions, what mechanisms to include, and how to interpret and represent the data-model discrepancies. These modeling choices can be nontrivial and the associated uncertainties hard to quantify 13. On the other hand, projections based on multi-model ensembles (implicitly) focus on structural uncertainty which requires strong assumptions on which part of the overall uncertainty is covered 4. Decision makers often prefer \"robust\" over optimal decisions when faced with \"deep\" uncertainty 14-18. Deep uncertainty refers to a situation when experts cannot agree upon or are not willing to provide probabilistic uncertainty ranges 15. In the context of decision-making, robustness has many different definitions that usually involve trading some optimality for relative insensitivity to deviations from the model assumptions or relatively good performance over a wide range of futures 15-18. Here we present sea-level projections to inform the design of robust strategies to cope with the deep uncertainties surrounding sea-level change, i.e. \"solutions capable of withstanding from deviations of the conditions for which they are designed\" 17. This notion of \"robustness\" deviates from scientific robustness that builds on arguably well understood physics and empirical/robust evidence 19-21 , which may lead to overconfident uncertainty ranges 2, 4 and getting surprised by new insights and data 9 . Our sea-level projections are constructed to support robust decision frameworks by i) being explicit about the relevant uncertainties, both shallow and deep; ii) communicating plausible ranges of sea-level rise, including the deep uncertainties surrounding future climate forcings and potential WAIS collapse; and iii) tending to err on the side of underconfident versus overconfident when possible.Model design. We design the projections to be probabilistic where reasonable and explicit about deep uncertainties (e.g. resulting from non-trivial model choices) when needed. Robust decision frameworks often apply plausible rather than probabilistic ranges to represent and communicate uncertainties . In the case of sea-level projections, the bounding of the plausible range usually involves both a probabilistic interpretation of the surrounding uncertainties and estimates of which probabilities are still relevant. For example, a full disintegration of the major ice sheets is often not taken into account because the probabilities of this occurring are considered too small to be relevant . What probability is relevant is highly dependent on the decision context and therefore it makes sense to be explicit about the probabilities. Moreover, probabilities are the easiest and most unambiguous way to communicate uncertainties .Our projections are designed to highlight the relatively large deep uncertainties, notably those resulting from future climate forcings and those surrounding potential WAIS collapse (even though representations of deep uncertainty often implicitly encompass probabilistic interpretations). The future climate forcing is, to a large extent, controlled by future human decisions.The probability of a WAIS collapse is potentially much larger than previously thought due to the combined effects of Marine Ice Sheet Instability (MISI), ice cliff failure and hydrofracturing . The discovery of this new mechanism puts earlier expert elicitations in a different light as it is unclear if those were based on this combined effect. One approach when faced with deeply uncertain model structures and priors is to present a potential WAIS collapse as deeply uncertain by means of a plausible range. We stress that this range is not meant to represent an implicit probabilistic projection of the WAIS contribution to sea-level rise.We merge some small deep uncertainties into the probabilistic part of the projections. According to \"\u2026 a larger risk lies in sampling too narrow a range (thus ignoring potentially important vulnerabilities) rather than too wide a range which, at worst, will sample extreme states of the world in which all alternatives fail\". Thus, in the context of informing robust decision making, it can be preferable to be slightly under-than slightly overconfident. To minimize the risk of producing overconfident projections we only use observational data with relatively uncontroversial and well-defined error structure.Model setup. We use a relatively simple (39 free physical and statistical parameters), but a mechanistically motivated model framework to link transient sea-level rise to radiative concentration pathways applying sub-models for the global climate, thermal expansion (TE), and contributions of the\n###\n", "summary": " Has value: 0.72\n, Global Mean Sea level Rise Projection: Global Mean Sea Level Rise Projections\n, has  start of period: 2000\n, has end of period: 2100\n, climate scenario: RCP4.5\n, has research problem: Global Mean Sea Level Rise Projections\n, has unit: m\n###"}
{"text": "#Properties\nimplementation, Developer, Input format, programming language, Attributes, has research problem, Keyword, Filter, Sampling, Aggregation, Incremental, Disk, Domain, Application type\n#Text\nA wealth of information has recently become available as browsable RDF data on the Web, but the selection of client applications to interact with this Linked Data remains limited. We show how to browse Linked Data with Fenfire, a Free and Open Source Software RDF browser and editor that employs a graph view and focuses on an engaging and interactive browsing experience. This sets Fenfire apart from previous table-and outline-based Linked Data browsers. BROWSING EXPERIENCEThe Fenfire application is a generic RDF browser and editor with features useful for Linked Data browsing. The user interface employs the conventional graph representation of the RDF data model. To make the visualisations scalable in the number of nodes in the graph and to focus on one thing at a time, only one central node and its surroundings are displayed concurrently. It is possible to switch between two views implemented based on the concept: a simple list view of objects associated with the focused subject (say, a container), and the generic graph view from Fentwine .A browsing session starts from some URI which is retrieved for a document with RDF data. This URI will be the initial focus unless the document has a foaf:primaryTopic defined, in which case the primary topic URI will be chosen as the initial focus.For the surroundings, the generic graph view shows to the left of the focus all triples that have the focus as an object, and to the right all triples that have the focus as a subject. Each triple is shown as a predicate connecting the subject to the object. This view is applied recursively to each displayed node until, with distance, the graph fades away to the background. If the node has an rdfs:label, it is displayed instead of the URI.Graph navigation can be done entirely via keyboard by rotating the surrounding nodes around the centre and moving focus to the node immediately to the left or right of the centre. While there is limited space for the surrounding nodes, all nodes can be navigated to via the rotation.To enable browsing of Linked Data, Fenfire dereferences the URI of the focused node and retrieves any rdfs:seeAlso related to this node whenever instructed to do so. For example, in  a user has loaded the FOAF profile of one person, followed a foaf:knows link to another person and retrieved the FOAF profile of this person.As an important alleviation of incompletely linked data, Fenfire adds triples asserting that the graph retrieved contains information about all of its disconnected components. USE CASESWe target two audiences primarily:\u2022 Semantic Web researchers, application developers and data producers need to explore available data on the level of individual triples. Fenfire provides a convenient alternative to manually downloading graph documents, reading the serialisation formats, and trying to match URIs to discover the links in the data.\u2022 People who want to learn about or demonstrate the Semantic Web and what data is available benefit from a visual presentation that truthfully shows the networked nature of the data. Here it is highly advantageous that Linked Data documents that follow the guidelines include an rdfs:label for each node. IMPLEMENTATIONFenfire has its origins in ZigZag , which is a system for managing interlinked, distributed data and is completely independent of the World Wide Web standards. Fentwine  incorporated Semantic Web standards and became a graphview RDF browser, as did its sibling BuoyOING  (Buoy-Oriented Interface, Next Generation) user interface, which adds spatial navigation and concentrates on it.Fenfire is implemented in the Haskell programming language to achieve high programmer productivity while meeting performance requirements for real-time animation and for browsing large graphs. It uses the Raptor library of the Redland project for RDF parsing, the GTK library for the standard graphical user interface elements and the Cairo library for the animated, vector-graphics based visualisations.A major part of the architecture is the key-frame and identitybased visualisation and animation system that provides the user continuous feedback on how the navigation and the switching between alternative views change what is visible. CONCLUSIONSWe applied Fenfire to the task of browsing Linked Data with a graph view and highlighted the use cases of exploring, learning about and demonstrating Semantic Web data.Some Linked Data browsers can edit the data as well. Fenfire can edit RDF graphs, but we need to implement remote publishing interfaces in addition to the current feature of saving to local files.Information display and navigation can be enhanced if there is knowledge about the user interaction requirements for a specific domain. Thus, Fenfire should have some automatic ontology-awareness and more should be configurable with settings and plug-ins.\n###\n", "summary": " implementation: Fenfire\n, has research problem: Graph-based visualization systems\n, Keyword: F\n, Filter: F\n, Sampling: F\n, Aggregation: F\n, Incremental: F\n, Disk: F\n, Domain: Generic\n, Application type: Desktop\n###"}
{"summary": "has research problem: Optimisation problem/Optimization problem/Assembly line worker assignment and balancing problem\nHas exact solution method: Branch and bound (B&B)\nHas heuristic: Iterated genetic algorithm (IGA)/Integrated beam search (IBS)\nHas model: Mixed integer linear programming", "text": "#Properties\nhas research problem, Has exact solution method, Has heuristic, Has model\n#Text\nIn traditional assembly lines, it is reasonable to assume that task execution times are the same for each worker. However, in sheltered work centres for disabled this assumption is not valid: some workers may execute some tasks considerably slower or even be incapable of executing them. Worker heterogeneity leads to a problem called the assembly line worker assignment and balancing problem (ALWABP). For a fixed number of workers the problem is to maximize the production rate of an assembly line by assigning workers to stations and tasks to workers, while satisfying precedence constraints between the tasks. This paper introduces new heuristic and exact methods to solve this problem. We present a new MIP model, propose a novel heuristic algorithm based on beam search, as well as a task-oriented branch-and-bound procedure which uses new reduction rules and lower bounds for solving the problem. Extensive computational tests on a large set of instances show that these methods are effective and improve over existing ones. Problem DefinitionLet S be a set of stations, W be a set of workers, |W | = |S|, and T be a set of tasks. Each workstation s \u2208 S is placed along a conveyor belt and is assigned to exactly one worker w \u2208 W , which is responsible for executing a subset of tasks x w \u2286 T . The tasks are partially ordered, and we assume that the partial order is given by a transitively reduced directed acyclic graph G(T, E) on the tasks, such that for an arc (t,t \u2032 ) \u2208 E task t precedes task t \u2032 . Therefore, the station that executes task t cannot be placed later than that of task t \u2032 on the conveyor belt. The execution time of task t for worker w is p tw . If a worker w cannot execute a task t, p tw is set to \u221e.The total execution time of worker w is D w = \u2211 t\u2208x w p wt . The cycle time C of the line is defined by the maximum total execution time max w\u2208W D w . In assembly line balancing, a problem of type 1 aims to reduce the number of stations for a given cycle time. Since in SWDs the goal is to include all workers, our problem is of type 2, and aims to minimize the cycle time for ap tw t 1 t 2 t 3 t 4 t 5 t 6 w 1 4 4 3 1 1 6 w 2 \u221e 5 6 5 2 4 w 3 3 4 2 \u221e 3 \u221e : Example of an ALWABP instance and an assignment of tasks to workers (in grey).Upper part: precedence constraints among the tasks. Lower part: task execution times.given number of stations and the same number of workers. A valid solution is an assignment of workers to stations together with an assignment of tasks to workers that satisfies the precedence constraints.  shows an example of an ALWABP-2 instance. For the assignment given in the figure, we have D w 1 = 5, D w 2 = 6, D w 3 = 5, and a cycle time of Structure of the paperIn Section 2 we introduce a new MIP model for the ALWABP-2. In Section 3 we present several lower bounds for the problem. A new heuristic for ALWABP-2 is proposed in Section 4. In Section 5 we present a task-oriented branch-and-bound method for solving the problem exactly. Computational results are presented and analyzed in Section 6. We conclude in Section 7. Sset of stations; W set of workers; T set of tasks; G  transitively reduced precedence graph of tasks;transitive closure of graph G(T, E); p tw execution time of task t by worker w; A w \u2286 T set of tasks feasible for worker w; A t \u2286 W set of workers able to execute task t; P t and F t set of direct predecessors and successors of task t in G; P * t and F * t set of all predecessors and successors of task t in G * ; C \u2208 R cycle time of a solution. Mathematical formulationIn this section we will present a new mixed-integer model for the ALWABP-2. Currently, the only model used in the literature, called M 1 here, is the one proposed by . It has O(|T | |W | |S|) variables, and O(|T | + |E| + |W | |S|) constraints. In the following we will use the notation defined in . Formulation with two-index variablesOur formulation is based on the observation that it is sufficient to assign tasks to workers and to guarantee that the directed graph over the workers, induced by the precedences between the tasks, is acyclic. Therefore our model uses variables x wt such that x wt = 1 if task t \u2208 T has been assigned to worker w \u2208 W , and d vw such that d vw = 1 if worker v \u2208 W must precede worker w \u2208 W . In this way, we obtain a model M 2 as follows:Constraint (2) defines the cycle time C of the problem. Constraint (3) ensures that every task is executed by exactly one worker. The dependencies between workers are defined by constraint (4): when a task t is assigned to worker v and precedes another task t \u2032 assigned to a different worker w, worker v must precede worker w. Constraints (5) and (6) enforce transitivity and anti-symmetry of the worker dependencies. As a consequence of these constraints, the workers of a valid solution can always be ordered linearly. Continuity constraintsWe can strengthen the above model by the following observation: if two tasks i and k are assigned to the same worker w, then all tasks j that are simultaneously successors of i and predecessors of k should also be assigned to w. These continuity constraints generalize constraints proposed by  for single station loads in the SALBP to several stations:Similarly, if task i is assigned to worker w, but some successor (predecessor) j of i is unfeasible for w, then no successor (predecessor) of j can be assigned to w. This justifies the constraintsLet model M 3 be model M 2 with additional constraints (10) and (11). Model M 3 has O(|W |(|T |+ |W |)) variables, and O(|E * ||T ||W | + |W | 3 + |E||W | 2 ) constraints, i.e. it has less variables but more constraints than M 1 . As will be seen in Section 6 model M 3 gives significantly better bounds than M 1 . Lower boundsLower bounds for ALWABP-2 can be obtained by different relaxations of the problem. In this section we discuss relaxations of the mixed-integer model presented above, as well as relaxations to SALBP-2 and R || C max . Relaxation to SALBP-2If we relax the task processing times to their minimum p \u2212 t = min {p tw | w \u2208 W }, ALWABP-2 reduces to SALBP-2. Therefore, all valid lower bounds for SALBP-2 apply to this relaxation. In particular, we use the lower bounds(The bound LC 2 supposes that the tasks are ordered such thatWe further use the SALBP-2 bounds on the earliest and latest possible station of task t for a given cycle time Cto obtain the lower bound LC 3 , defined as the smallest cycle time C such that E t (C) \u2264 L t (C) for all t \u2208 T . For more details on these bounds we refer the reader to the survey of . Relaxation to R || C maxBy removing the precedence constraints the ALWABP-2 reduces to the problem of minimizing the makespan of the tasks on unrelated parallel machines (R || C max ), which itself is an NP-hard problem. Several effective lower bounds for R || C max have been proposed by . Their lower bounds L 1 and L 2 are obtained by Lagrangian relaxation of the cycle time constraints (2) and the assignment constraints (3), respectively.  further propose an additive improvement that can be applied to L 1 to obtain a bound L a 1 \u2265 L 1 , as well as an improvement by cuts on disjunctions, that may be applied to L a 1 and Linear relaxation of ALWABP-2 modelsBounds obtained from linear relaxations of integer models for the SALBP-2 are usually weaker than the SALBP-2 bounds of Section 3.1. However, the relaxation to minimum task execution times weakens the SALBP-2 bounds considerably. Therefore, the linear relaxations of model M 3 provides a useful lower bound for the ALWABP-2 . Heuristic search procedureIn this section, we describe a heuristic algorithm IPBS for the ALWABP-2. It systematically searches for a small cycle time by trying to solve the feasibility problem ALWABP-F for different candidate cycle times from an interval ending at the current best upper bound. For each candidate cycle time C, a probabilistic beam search tries to find a feasible allocation. Probabilistic beam search for the ALWABP-FThe basis for the probabilistic beam search is a station-based assignment procedure, which assigns tasks in a forward manner station by station. For each station it repeatedly selects an available task, until no such task has an execution time less than the idle time of the current station. A task is available if all its predecessors have been assigned already. If there are several available tasks the highest priority task as defined by a prioritization rule is assigned next. The procedure succeeds if an assignment using at most the available number of stations is found. Station-based procedures can be also applied in a backward manner, assigning tasks whose successors have been assigned already. For this it is sufficient to apply a forward procedure to an instance with reversed dependencies. For the ALWABP we additionally have to decide which worker to assign to the current station. This is accomplished by applying the task assignment procedure to all workers which are not yet assigned, and then choosing the best worker for the current station by a worker prioritization rule. The probabilistic beam search extends the station-oriented assignment procedure in two aspects. First, when assigning tasks to the current station, it randomly chooses one of the available tasks with a probability proportional to its priority. Second, it applies beam search to find the best assignment of workers and their corresponding tasks.Beam search is a truncated breadth-first tree search procedure . When applied to the ALWABP-F, it maintains a set of partial solutions called the beam during the station-based assignment. The number of solutions in the beam is called its width \u03b3. Beam search extends a partial solution by assigning each available worker to the next station, and for each worker, chooses the tasks to execute according to the above probabilistic rule. For each worker this is repeated several times, to select different subsets of tasks. The number of repetitions is the beam's branching factor f . Among all new partial solutions the algorithm selects those of highest worker priority to form the new beam. The number of solutions selected is at most the beam width.Task and worker prioritization rules are important for the efficacy of station-oriented assignment procedure.  compared the performance of 16 task priority rules and three worker prioritization rules for the ALWABP-2. We have chosen the task priority rule MaxPW \u2212 and the worker priority rule MinRLB, which have been found to produce the best results in average for the problem. The task prioritization rule MaxPW \u2212 gives preference to tasks with larger minimum positional weightThe worker prioritization rule MinRLB gives preference to workers with smaller restricted lower boundwith the set W u \u2286 W corresponding to the unassigned workers and T u \u2286 T to the set of unassigned tasks of a partial assignment. Before computing MinRLB we apply to each partial solution the logic of the continuity constraints (10) and (11) to strengthen the bound. If tasks i and k have been assigned already to some worker w, we also assign all tasks succeeding i and preceding k to w. Similarly, if i has been assigned to w and some successor (predecessor) j of i is infeasible for w we set p kw = \u221e for all successors (predecessors) k of j. The probabilistic beam search is shown in Algorithm 1. The interval search method IPBSAn upper bound search starts from a known feasible cycle time and tries to reduce it iteratively. A common strategy is to reduce it successively by one and to try to find a better feasible solution by some heuristic algorithm. However, it is well known that heuristic assignment procedures are not monotone, i.e., they may find a feasible solution for some cycle time but not for larger cycle times. To overcome this, we propose to modify the upper bound search to examine an interval of cycle times ending at the current best upper bound. If the current lower and upper Algorithm 1: Probabilistic beam search input : A set of stations S, a candidate cycle time C, a beam width \u03b3 and a beam factor f . output: A valid assignment or \"failed\" if no valid assignment could be found. bounds on the cycle time are C and C, the upper bound search will try to find a feasible solution for all cycle times between max{C, pC } and C \u2212 1 for a given factor p \u2208 (0, 1) and update C to the best cycle time found, if any. Otherwise, the upper bound search continues with the same interval. Since the beam search is probabilistic this may produce a feasible solution in a later trial. The interval search depends on three parameters: the minimum search time t min , the maximum search time t max and the maximum number of repetitions r. The search terminates if the cycle time found equals the lower bound, or if the maximum time or the maximum number of repetitions are exceeded, but not before the minimum search time has passed. Initially, the value of C is set to the best of all lower bounds presented in Section 3. The initial upper bound C is determined by an single run of the beam search with a beam factor of one. Improvement by local searchA local search is applied to the results found by interval search method. It focuses on critical stations whose load equals the cycle time of the current assignment. It tries to remove tasks from a critical station in order to reduce the cycle time. Since there can be multiple critical stations, a move is considered successful if it reduces the number of critical stations. The local search applies the following four types of moves, until the assignment cannot be improved any more.1. A shift of a task from a critical station to another station.2. A swap of two tasks. At least one of the tasks must be on a critical station.3. A sequence of two shift moves. Here the first shift move is allowed to produce a worse result than the initial assignment.4. A swap of workers between two stations without reassigning the tasks.5 Task-oriented branch-and-bound algorithmIn this section we propose a branch-and-bound algorithm for ALWABP-2 using the bounds and the heuristic presented in the previous sections.The algorithm first computes a heuristic solution by running the probabilistic beam search. It also applies the lower bounds, L 2 at the root node to obtain an initial lower bound.If the solution cannot be proven optimal at the root node, the algorithm proceeds with a depth-first search. In branch-and-bound algorithms for assembly line balancing two branching strategies are common. The station-oriented method proceeds by stations and branches on all feasible maximal loads for the current station, while the task-oriented method, proceeds by tasks and branches on all possible stations for the current task. The most effective methods for SALBP use station-oriented branching. However, for the ALWABP the additional worker selection substantially increases the branching factor of the station-oriented approach. A workeroriented strategy, on the other hand, has to consider much more station loads, since all subsets of unassigned tasks which satisfy the continuity constraints (10) are candidates to be assigned to a worker. Therefore, we use a task-oriented branching strategy.The proposed task-oriented method executes the recursive procedure shown in Algorithm 2. At each new node it applies the lower bounds LC 1 , LC 2 , LC 3 , L a 1 (line 7), since the lower bounds M 3 and L 2 are too slow to be applied during the search, although they obtain the best bounds. When a complete solution has been found, the algorithm updates the incumbent (line 2). Otherwise, it selects an unassigned task t (line 4) and assigns it to all feasible workers (loop in lines 5-11). Algorithm 2: branchTasks(llb, A)Input : An upper bound gub, a set A \u2286 T of assigned tasks, and a local lower bound llb. For branching, the task with the largest number of infeasible workers is chosen in line 4. A worker is considered infeasible, if the allocation of the task to the worker creates an immediate cyclic worker dependency or the lower bound LC 1 after the assignment is at least the value of the incumbent. In case of ties, the task with the largest lower bound is chosen, where the lower bound of a task is the smallest lower bound LC 1 over its feasible workers. This rule gives preference to tasks that tighten the lower bound early. Any remaining tie is broken by the task index. After the task has been chosen, a branch is created for each valid worker. The branches are visited in order of non-decreasing lower bounds. Again, ties are broken by the worker index. Valid assignmentsThe algorithm maintains a directed graph H on the set of workers to verify efficiently if the precedence constraints are satisfied. It contains an edge (w, w \u2032 ) if there is some task t assigned to w and another task t \u2032 assigned to w \u2032 , such that (t,t \u2032 ) \u2208 E * . The graph H also contains all resulting transitive edges. For a valid assignment of tasks, H must be acylic. If this is the case, any topological sorting defines a valid assignment of workers to stations. The procedure assignmentIsValid(t, w) verifies in time O(|T ||W |) if the assignment of task t to worker w would insert an arc into H whose inverse arc exists already. Before branching to a new node, the procedure setAssignment(t, w) inserts such arcs into H and computes the new transitive closure in time O(|T ||W |). This is undone by unsetAssignment(t, w) when backtracking. To speed up the selection of a task for branching, we do not consider the violation of transitive dependencies in H in line 4, but only the creation of an immediate cyclic worker dependency, which results from inserting an edge (w, w \u2032 ) for which (w \u2032 , w) is already present. This can be tested in time O(|P t | + |F t |). Reduction rulesAfter a task t has been assigned to a worker w, and before branching, we apply several more costly reduction rules to strengthen the lower bounds (line 6). First, we can set p tw \u2032 = \u221e for any w \u2032 = w. Second, we can enforce the continuity constraints (10) and (11). An application of (10) may assign further tasks to w, and the application of (11) may exclude some tasks from being assigned to w (whose execution time is set to p t \u2032 w = \u221e). Finally, we can exclude a taskworker assignment (t \u2032 , w) if the total execution time p tw + p t \u2032 w +\u2211 u\u2208i(t,t \u2032 ) p uw of the tasks i(t,t \u2032 ) = (P * t \u2229 F * t \u2032 ) \u222a (F * t \u2229 P * t \u2032 ) between t and t \u2032 is more than or equal to the current upper bound. These rules are repeatedly applied until no more tasks can be assigned or excluded. Computational resultsAll algorithms were implemented in C++ and compiled with the GNU C compiler 4.6.3 with maximum optimization. The MIP models and their linear relaxations were solved using the commercial solver CPLEX 12.3. The experiments were done on a PC with a 2.8 GHz Core i7 930 processor and 12 GB of main memory, running a 64-bit Ubuntu Linux. All tests used only one core. Details of the results reported in this section are available online 1 . Test instancesA set of 320 test instances has been proposed by . They are characterized by five experimental factors: the number of tasks, the order strength (OS) 2 , the number of workers  (|W |), the task time variability (Var), and the percentage of infeasible task-worker pairs (Inf). All factors take two levels, as shown in Comparison of lower boundsWe first compare the strength of the lower bounds proposed in Section 3. To compute the lower bound L 1 we use the ascent direction method of van de Velde (1993). This bound was improved to L a 1 , as proposed by . Their method applies a binary search for the best improved bound, which is obtained by solving |S| knapsack problems of capacity C for each trial cycle time C. Different from  we solve the all-capacities knapsack problem by dynamic programming only once and use the resulting table during the binary search. The knapsack problems that arise when computing L 2 and L 2 by subgradient optimization are solved similarly.  shows the average relative deviation in percent from the best known value and the average computation time over all 320 instances. Looking at the models, the lower bound of M 2 is significantly better than M 1 , and the addition of the continuity constraints improves the relative deviation by another 10%, yielding the best lower bound overall. The computation time of the three models is comparable, with M 3 being slower than the other two models. The linear relaxation of R || C max is slightly worse that M 2 , but two orders of magnitude faster. The bounds L 1 and L a 1 achieve about the same quality an order of magnitude faster than R || C max . The lower bounds from the relaxation to SALBP are weaker than most of the other lower bounds, except LC 1 , but another order of magnitude faster.For the branch-and-bound we chose to use the lower bounds from the relaxation to SALBP and L a 1 , since the other bounds are too costly to be applied at every node of the branch-and-bound tree. We include all of the faster bounds, since they yield complementary results. In particular LC 1 obtains the best bound in average at the root node, but is less effective during the search. Comparison of MIP modelsWe next compare the performance of the new MIP models with that of model M 1 .  shows the average number of nodes and the average computation time needed to solve the instances to optimality for the 16 groups with a low number of tasks. The instance groups Tonge and Wee-Mag with a high number of tasks are not shown, since none of them could be solved to optimality within an hour.Overall model M 2 needs significantly more nodes than M 1 , and is a factor of about two slower. It executes more nodes per second, and has a better lower bound, but CPLEX is able to apply more cuts for model M 1 at the root, such that in average model M 2 has no advantage on the tested instances. However, when the continuity constraints are applied, model M 3 needs significantly less nodes and time compared to model M 1 (confirmed by a Wilcoxon signed rank test with p < 0.01). The results show that the continuity constraints are very effective, in particular for a high order strength and for high numbers of workers. Results for the IPBS heuristicWe compare IPBS with three state of the art heuristic methods for the ALWABP-2, namely the hybrid genetic algorithm (HGA) of , the iterated beam search (IBS) of , and the iterative genetic algorithm (IGA) of . In   preliminary experiments we determined reasonable parameters for the probabilistic beam search as shown in . For the HGA and the IBS we compare in  the relative deviation from the current best known value (Gap) and the computation time (t), in average for each group of instances and over 20 replications per instance. We further report the average computation time to find the best value (t b ), and the average relative deviation of the best solution of the 20 replications (Gap b ). The total computation time of Blum and Miralles (2011) is always 120s more than the time to find the best value, and has been omitted from the table.We can see that the problem can be considered well solved for a low number of tasks, since all three methods find the optimal solution with a few exceptions in less than ten seconds. In six instance groups the IPBS terminates in less than the minimum search time, since the solution was provably optimal. For instances with a high number of tasks, IBS produces better solutions for more workers, while the HGA is better on less workers. IPBS always achieves better results than both methods (confirmed by a Wilcoxon signed rank test with p < 0.01). This holds for the : Comparison of the proposed heuristic with a hybrid genetic algorithm  and an iterated beam search .  10% 136.9 56.9 6.6 2.3 104.9 13.9 9.9 25.1 9.4 5.8 3.8 20% 158.8 60.1 7.6 3.3 84.9 12.0 7.8 25.1 7.7 4.6 2.9 H 10% 248.5 115.8 8.9 3.9 160.3 12.6 9.3 37.1 12.4 5.6 3.7 20% 245.9 112.7 7.8 2.9 143.3 13.8 9.6 36.1 14.1 4.5 2.6 19 L 10% 213.9 61.4 12.5 6.2 57.1 8.1 4.1 39.9 11.6 1.4 0.0 20% 225.6 66.1 14.0 9.2 60.3 9.8 5.4 39.0 10.4 2.6 0.8 H 10% 283.7 97.9 15.3 9.1 71.4 11.1 6.5 38.3 11.0 4.6 3.7 20% 288.1 108.9 13.0 7.7 90.0 10.3 5.8 41.6 11.8 3.8 3.5 Total averages 143.7 40.1 4.9 2.7 54.7 4.7 3.1 31.0 8.8 1.8 1.2 averages as well as the best found solutions (except the first instance group of Wee-Mag, where the best solution of IPBS is slightly worse than that of the HGA). IPBS is also very robust in the sense that the difference between average and best relative deviations is the smallest of the three methods. In average over all instances, its solutions are 1.8% over the best known values. Since the best known values are known to be optimal for 307 of the 320 instances, the gap is close to optimal.To compare execution times, we have to consider that the results have been obtained on different machines (for IBS a PC with a 2.2 GHz AMD64X2 4400 processor and 4GB of main memory, for HGA a PC with a Core 2 Duo 2.2 GHz processor in 3 GB of main memory). A conservative assumption is that their performance is within a factor of two of each other. Taking this into account, over all instances HGA and IBS have comparable computation times, and the IPBS is about a factor two faster. This holds for finding the best solution and also for the total computation time. (Remember that the total computation time of IBS is 120 s longer than the : Comparison of the proposed heuristic with an iterated genetic algorithm . time to find the best solution.) The faster average computation times are mainly due to the instances with a high number of tasks, for which IPBS scales better. The best solutions are almost always found in less than 30 seconds. For all heuristics, the computation time is significantly less for a low number of tasks, a low number of workers, and a low order strength. Similarly, the relative deviations are smaller for a low number of tasks and low order strength. However, the relative deviation does not depend significantly on the number of workers, except for the HGA, which produces better solution for a low number of workers. (These findings are confirmed by a Wilcoxon signed rank test at significance level p < 0.01.) For IBS and IBPS there is an interaction between the number of workers and the order strength: both produce better solutions for a low number of workers and a high order strength or vice versa.Since for the IGA no detailed results are available, we compare in  with the summarized values reported by : the average cycle time (C), the average cycle time of the best found solution (C b ), and the average computation time to find the best value (t b ). The values are again averages for all groups of instances, but over only 10 replications for the IGA. The results for our method are the same as in  but in absolute values. Note that this evaluation may mask large deviations in instances with low cycle times and overestimate small deviations for high cycle times.As the other methods, the IGA solves the small instances optimally, but not the larger ones. Compared to our method, its average performance is worse except for three groups of wee-mag with a low number of workers, where the average cycle time is about 0.2 lower. The comparison is similar for the best found values, where the IGA is better by 0.4 in a single group. In average over all large instances our method produces a cycle time of about 1 unit less.The execution times of the two methods are comparable. The results of  have been obtained on a Intel Core 2 Duo T5750 processor running at 2.0 GHz, whose performance is within a factor of three from our machine. Taking this into account, our methods find the best value about 50% faster.In summary, the results show that IPBS can compete with and often outperforms the other methods in solution quality as well as computation time. The difference to the other methods is smallest for the large instances with a low order strength and a low number of workers. IPBS in general is very robust over the entire set of instances. Results for the branch-and-bound algorithmWe evaluated the branch-and-bound algorithm on the same 320 test instances. For the tests, IPBS was used to produce an initial heuristic solution. It was made deterministic by fixing a random seed of 42 and configured with a minimum search time of 0 s and a maximum search time of |T ||W |/10 s. During the search the number of iterations of the ascent direction method to compute L 1 has been limited to 50, and the number of iterations for the subgradient optimization to compute L 2 to 20.The only other branch-and-bound algorithm in the literature proposed by  for the ALWABP-2 has been found inferior to model M 1 by  in tests with CPLEX (version 10.1). We therefore limit our comparison to the MIP models. We first compare our approach to CPLEX on the best model M 3 on the instances with a low number of tasks in . In  we then present the results of the branch-and-bound algorithm with a time limit of one hour on the larger instances. CPLEX is not able to solve any of the models on the instances with a high number of tasks within this time limit.  shows the average solving time and the average number of nodes in the branch-andbound tree for all instance groups with a low number of workers. On these instances both methods have a similar performance, solving all instances in a few seconds, and are even competitive with the heuristic methods. In most cases the branch-and-bound algorithm needs fewer nodes than CPLEX, except for five groups with a low number of workers. Computation times are also comparable, although the time of the branch-and-bound algorithm is dominated by the initial heuristic.  shows the results of the branch-and-bound algorithm on the larger instances. We report the number of optimal solutions found (Opt) and the number of solutions proven to be optimal (Prov), the average computation time (t), the average relative deviation from the best  known value (Gap), and the average cycle time for each group of instances (C).In about 70% of the instances the optimal solution was found, and about 60% of the solutions could be proven to be optimal within the time limit. All except four instances with a high order strength were solved. The average relative deviation over all 320 instances is 0.60%, about one third of the average case of the best heuristic.As expected, the solution times are higher than those of the heuristic methods but for the instances with a high order strength only about an order of magnitude, in average. The solving time depends mainly on the number of tasks, the number of workers, and the order strength (as confirmed by a Kruskal-Wallis test followed by Wilcoxon signed rank post hoc tests at significance level p < 0.01). The instances with a high order strength or a low number of workers are easier to solve, because the reduction rules are more effective. ConclusionWe have presented a new MIP model, a heuristic search procedure and an exact algorithm for solving the Assembly Line Worker Assignment and Balancing Problem of type 2. The new MIP model shows the importance of including continuity constraints in this type of problem, and its linear relaxation gives the current best lower bound for the problem. The proposed heuristic IPBS is competetive with the current best methods, often outperforms them in computation time and solution quality, and shows a robust performance over the complete set of 320 test instances. Finally, the branch-and-bound method can solve instances with a low number of tasks in a few seconds, and was able to optimally solve 95 of the 160 instances with a high number of tasks for the first time.With respect to the problem, constraints that enforce continuity have shown to be the most effective way of strengthening the lower bounds in the models as well as the heuristic and exact algorithm. Besides the size of the instance, the number of workers and the order strength has the strongest influence on the problem difficulty. All methods are able to solve instances with a high order strength better. This also holds for the branch-and-bound algorithm on instances with a low number of workers.Our results show that assembly lines with heterogeneous workers can be balanced robustly and close to optimal for problems of sizes of about 75 tasks and 20 workers. Problems of this size arise, for example, in Sheltered Work Centers for Disabled, and we hope that these methods will contribute to a better integration of persons with disabilities in the labour market. A very interesting future line of research in this context may be the integration of persons with disabilities into larger assembly lines with regular workers.\n###\n"}
